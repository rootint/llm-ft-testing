 Hey, this is Craig Cannon, and you're listening to Y Combinators podcast. Today's episode is with Doug Eck. Doug's a research scientist at Google, and he's working on Magenta, which is a project making music and art through machine learning. Their goal is to basically create open source tools and models that help creative people be even more creative. So if you want to learn more about Magenta or get started using it, you can check out magenta.tensorflow.org. All right, here we go. I wanted to start with the quote that you ended your IO talk with, because I feel like that might be helpful for some folks. So it's a Brian Eno quote, and I will have the slightly longer version. Yeah, good. Yeah. So yeah, it goes like this. Whatever you now find weird, ugly, uncomfortable, and nasty about a new medium will surely become its signature. CD distortion, the jitteriness of digital video, the crap sound So that's how you ended your I.O. talk. Correct. breaking apart. So that's how you ended your IO talk. Correct. And what it kind of opened up for me was like, what, when you're thinking about creating Magenta and all the projects therein as new mediums, how are you thinking about like, how are you thinking about what's going to be broken and what's going to be created? The reason that I put that quote there, I think is to, to be honest with the, uh, the division between engineering and research and artistry and to not think that what I'm doing is being a machine learning artist, but we're trying to build interesting ways to make new kinds of art. And I think, you know, it occurred to me, I read that quote and I thought, you know, that's it, right? No matter how hard Eastman or whomever invented the film camera, I'm sorry if that's the wrong person, right? Like they clearly weren't thinking of breakage or they're trying to avoid certain kinds of breakage. I mean, you know, guitar amplifiers aren't supposed to distort, you know. And, you know, I thought, well, what if we do that with machine learning? Like models that are, like the first thing you're going to do if you think, if someone comes to you and says, here's this really smart model that you can make art with, what are you going to do? You're going to try to show the world that it's a stupid model, right? But maybe the way that, maybe it's smart enough that it's kind of hard to make it stupid. So you get to have a lot of fun making it stupid, right? with my girlfriend and um what she was trying to do was make the most accurate picture that the computer wouldn't recognize like immediately out of the gate because she works in art and like yeah doesn't want to believe that's right it's a good it's a good intuition i mean you know yeah um so so maybe the best ways to start is then talk about like what are you working on right now what are you guys making so right now we're we're working on, let me think. It's a good question. We have this project called NSynth, which is trying to get deep learning models to generate new sounds. And we're working on a number of ways to make that better. I think one way to think about it is we have this latent space. So to make that a little bit less a buzzword we have a kind of compressed space a space that doesn't have the ability to memorize the original audio but it's set up in such a way that we can try to regenerate some of that audio and in regenerating it we don't get back exactly what we started with but hopefully we get something close mm-hmm and that space is set up so that we can move around in that space in and come into new points in that space and actually listen to what's there. Right now, it's quite slow to listen, so to speak. We're not able to do things in real time. And we also would love to be at kind of a meta level, building models that can generate those embeddings, having trained on other data, so that you're able to move around in that space in different ways. And so we're kind of moving. We're continuing to work with sound generation for music. And we also are spending quite a bit of time on rethinking the music sequence generation work that we're doing. We put out some models that were, by any reasonable account, primitive. I mean, kind of very simple recurrent neural networks that generate MIDI from MIDI and that maybe use attention, that maybe have a little bit smarter ways to sample when doing inference, when generating. And now we're actually taking seriously, wait a minute, what if we really look at large data sets of performed music? What if we actually start to care about expressive timing and dynamics, care deeply about polyphony, and really care about not putting out kind of what you would consider a simple reference model, but actually what we think is super good? And I think those are the things we're focusing on. I think we're trying to actually make things really pull up quality and make things that are better and more usable for for people and so with all that supervised learning are you like are you going to create a web app that people will evaluate how good the music is because i i heard a couple interviews with you before where that was the issue right like how do you know what's good yeah uh the so that is like i'm pausing because that's the big question I think in my mind is how do we evaluate these models? At least for Magenta, I haven't felt like the quality of what we've been generating has been good enough to bother, so to speak. Like you find it, you cherry pick, you find some good things, you're like, okay, this model trains and it's interesting. And now we kind of understand that the API, the input output of what we're trying to do. I would love, yeah, I don't know how to solve this. Conceptually what we do, here's what we do, right? We build a mobile app and we make it go viral. That's what we do, right? And then once it's viral, we just keep feeding all of this great art and music in. And I used to do music recommendation. We just build a collaborative filter and which is a kind of way to make, you know, recommend items to people based upon what they like. And we'd start giving people what they like and we pay attention to what they like and we make the models better. So all we need to do is make that app go viral. One simple thing. In fact, maybe someone in the Y Combinator world can help us do that. It's like sliding between cow and trombone. What's the best sound? Exactly. Yeah. Right. Maybe that particular web app is not the right answer. Now, I mean, I'm saying that as a joke, but I think, look at it this way. If we can find a way, or the community in general can find a way for machine-generated media to be sort of out there for a large group of interested users to play with, I think we can learn from that signal. And I think we can learn to improve. And if we do, we'll make quite a nice contribution to machine learning. We will learn to improve based upon human feedback to generate something of interest. So that's a great goal. improve, based upon human feedback to generate something of interest. So that's a great goal. Yeah. But I'm totally like today in this room, you know, I wish I could tell you we had a secret, we had like a secret plan, you know, like, we're, you know, he's figured it out, the app's gonna launch tomorrow. It's like, it's really hard work. Like bleep cut. Yeah, yeah, exactly. Sorry. Interesting. Okay, because I was wondering what kind of data you were getting back from artists. You know, do people just use all of your projects, you know, all of the repos to create things of their own interest? Or are they pushing back valuable data to you? So we're getting some valuable data back. And I think what we're getting back, some of the signals that we're getting back are giving us such an obvious direction for improvement. Like, why would I want to run a Python command to generate 1000 MIDI files? That's not what we do. You know, like, like, like, you get that kind of feedback, you're like, okay, we wanted this command line version, because, you know, we needed to be able to test some things. But if musicians are really going to use the music part of what we're doing, we have to provide them with more fluid and more useful tools. And there, I think we're, we're still sitting with so many obvious hard problems to solve, like integration with something like Ableton or like really solid, you know, real-time IO and things like that, that we know what to work on. But I think we'll get to the point pretty quickly where we'll have something that's kind of solves the obvious problems, plugs in reasonably well to your workflow, and you can start to generate some things and you can play with sound. And then we need to be much more careful about the questions we ask and how good we are at listening to how people use what we're doing. And so what are artists using it for at this point? Right now, most of what we've done so far has had to do with music. If we look for a second away from music and look at SketchRNN, which is a model that learned to draw, we've actually seen quite a bit of... So first, at a higher level, SketchRNN is a recurrent neural network trained on sketches to make sketches. And the sketches came from a game that Google released called QuickDraw, where people had 20 seconds to draw something to try to win at Pictionary with a computer, a classifier counterport. And so we trained a model that can generate new cats or dogs or whatever. There's some really cool classes in there. A cruise ship is nice. Yeah. The one that always threw me was Camouflage. It calls out Camouflage all the time. I'm never... As if by definition definition you can't draw it right yeah um nothing pause for 20 seconds right yeah i i actually won a fictionary round with the word white and i just pointed at the paper i'm like no way and she said white i'm like you gotta be kidding okay so anyway it's kind of like a corollary to camouflage. So we've seen artists start to sample from the model. We've seen artists using the model as a distance measure to look for weird examples, because the model has an idea of what's probable in the space. We've also seen artists just playing around with the raw data. And so there's been a nice explosion there. I think I'm not expecting that artists really do a huge amount with this quick draw data, because as cool as it is, these things were drawn in 20 seconds. There's kind of a limit to how much we can do with them. On the music side, we've had a number of people playing with NSynth with just dumps of samples from NSynth, so basically like a rudimentary synthesizer. And there I've been surprised at the kind of, I would expect that if you're really good at this, so like you're Aphex Twin, or how about this, you want to be Aphex Twin, right? That you look at this and go, yeah, whatever. There are 50 other tools that I have that I can use. But those are the people that we found have been the most interested because I think, I think we are generating some sounds that are, that are new. I mean, so first you can test someone pointed out on, on hacker news, you can take a few, a few oscillators and a noise generator and make something new. But I think these are new in a way when you start sampling the space between, you know, I think these are new in a way when you start sampling the space between, you know, a trombone and a flute or something like that, that these are new in a way that, that capture some very nice harmonic properties, capture some of the essence of the Brian Eno quote are kind of, kind of broken and glitchy and edgy in a way. But that glitchiness is not the same as you would get from like a digital clipping. The glitchiness sounds really harmonic. And so, for example, Jesse on our team, Jesse Engel, he built some Ableton plug-in where you're listening to these notes, but you're able to erase the beginnings of the notes. So you erase the onsets, which is usually where most of the information is. Most of the information in a piano note is kind of that first percussive onset, but it's the onsets that the model is doing such a great job of reproducing because it gradually kind of moves away from in time. It's a temporal embedding and the noise kind of adds up as we move through the embedding in time. So it's the tails of these notes that start to get ringy and like they'll detune and you'll hear these rushes of noise come in or there'll be this little weird whoop at the end and so like we found that that musicians who've actually played with sound a lot find these particular sounds immensely fascinating i think they're the kinds of sounds that are that sound interesting in a way that's hard to describe unless you've played with them. I think they're interesting because the model has been forced to capture some of the important sources of variance in real music audio. And even when it fails to reproduce all of them, when it fills in with confusion, so to speak, even that confusion is somehow driven by musical sound. And which you see by the corollary, if you look at something like Deep Dream and you see what models are doing when they're sort of showing you what they've learned, it may not be what you expect from the world, but there's something kind of interesting about them, right? Anyway, it's a long answer, but the short version of the answer is we found that working with very talented musicians has been really fruitful. And our challenge is now to be good enough at what we do and make it easy enough and make it clean enough that even someone who's not an Aphex Twin, and I'm not saying we worked with Aphex Twin. We didn't work with Aphex Twin. But that kind of artist, yeah, that we can also be saying, hey, this is really genuinely musically engaging for a much, much larger audience. That's surprising. So it's not necessarily generating melodies for people so much as it is generating interesting sounds. That's what's brought them in? That's what's brought them in, though the parallel has existed for the sequence generation stuff. And what I noticed, even with AI Duet, which is this web-based, it's a simple RNN. I can lay claim, it's technology that was simple RNN. It's like, I can lay claim, it's technology that was published in 2002. It's really a very simple, really simple. But like, so this model, if you haven't, if your viewers haven't seen it, you play a little melody and then the model thinks for a minute and the AI genius, which is an LSTM network, comes back and plays something back to you, right? If you play for release, you know, right? And you wait, you're expecting maybe, you know, that it'll continue the tune. It's not going to, right? It's going to go, right? So like this idea of like expecting the model to like carry these long arcs of melody along is not really understanding the model. What we saw was like, like, especially jazz musicians, but like musicians who listen, the game they play is to follow the model. And so they'll like, I'd see guys or people, women too, sit down and go like, you know, dum, dum, dum, dum, and just wait. And it's almost like pinging the model with an impulse response. Like, what's this thing going to do? And then instead of trying to drive it, comes back and goes dumb dumb dumb dumb right yeah and then and then the musician says oh i see let's go up to the fifth and and then you get this really it's almost like follow the leader yeah but you're following the model and then it's super fun and like it's basically a challenge for the musician to to try to understand how to play along with something that's so primitive right right but if don't have the musical, so basically it's the musician bringing all the skill to the table, right? So even with the primitive sequence generation stuff, it's still been interesting to see that it's musicians with a lot of musical talent and particularly the ability to improvise and listen that have managed to actually get what I would consider interesting results out of that. So it's become more of like a call and response game than a tool. Yeah, I think so. And that's partially because the model is pretty primitive. I think that if we can get the data pipelines in order so that we know what we're training on and we can actually do some more modern sequence learning, you know, having like generative adversarial feedback and things like that, we can do much better. And even we have some stuff that we haven't released yet that I think is better. But yeah, I think as we make it better, it'll be more of a, this model is going to give me some more ideas from what I've done. Right now it's more of a, this model is kind of weird, but I'm going to try to understand what it's doing. Okay. Both are fun modes, by the way. Yeah. They're both cool modes, right? Yeah. I mean, I've enjoyed it. I'm definitely not a pianist. I mean, I've played guitar before, and I tried to get a song going, but I had trouble with it. We're sorry. I think it's mostly I love the YouTube video. Blame the user, right? Yeah, yeah. The video where that guy played a song with it, that was amazing. Yeah, that was cool. It was very cool. Have you seen a lot of that stuff as well? Yeah, we've seen, we saw, like, we haven't, I haven't, well, we haven't pushed the sequence generation stuff much because we really wanted to focus on Tamper. But when we have released things things and kind of try to show people we're there. Yeah, we've gotten, if you look on them, there's a Magenta mailing list that's just like it's linked in g.co Magenta. And if you look around, there's like a discussion list, which is like as flamey and spammy as some discussion lists, but a little bit less so. It's pretty, you know, every couple of weeks, someone will put up some stuff they composed with Magenta. And usually they're more effective if they've layered their own stuff on top of it or they've you know taken time offline rather than in performance to to generate but some stuff's actually quite good i think it's fun it's a start yeah i mean i think it's great and so how like you compared it to you know the work you did in 2002 um where is lstm gone since then like you you talk about like, you ended up doing this project. I saw in your talk that because you kind of like failed at it a while ago. Failure is good. Yeah. Yeah. So there was a point in time, I was at a lab called IDSEA, the Dalai Moli Institute for Artificial Intelligence. And I was working for Juergen Schmidhuber, who's one of the co-authors. He was the advisor to Sepp Hockritter, who did LSTM. And there was a point in time where there were three of us in a room in Mano, Switzerland, which is a suburb of Lugano, Switzerland, who were the only people in the world using LSTM. It was myself, Felix Gehrs, and Alex Graves. Among the three of us, by far far Alex Graves has done the most with LSTM. So he continued after he finished his PhD and he continued doggedly to try to understand how recurrent neural networks worked, how to train them, and how to make them useful for sequence learning. I think more than more than anybody else in the world including Sep, the the person who created LSTM, you know Alex just stuck with it and and finally started to get wins in speech and language. And I more or less put down LSTMs. I started working with audio stuff and other more cognitively driven music stuff at University of Montreal. But it worked finally. And there's this thing in music, a 20-year overnight success. Somebody was like, this worked because he stuck with it. And now, of course, it's, you know, become like sort of the touchstone for, you know, recurrent models in time series analysis. It forms, some version of it forms the core of what we're doing with translation. I mean, these models have changed, right? They've evolved over time. But basically, you know, Recurrent Neural Networks as a family of models is around because of that effort of like, it's interesting, right? It's really, there really were three of us. And Felix went on with his life. And I went on with my life. And Alex stuck with us. Kind of really one person carrying forward. But you may get letters from people saying, hey you forgot about me you forgot about me this is this is a little bit reductionist obviously there were more but it felt that way at the time right what was the breakthrough breakthrough then that like got people interested i think it was the same breakthrough that got people interested in deep neural networks and convolutional neural networks it's that these models don't work that well with small training sets and small models. So like ImageNet? Yeah, that they're data absorptive, meaning that they can absorb lots of data if they have it. And neural networks as a class are really good with high dimensional data. And so as machines got faster and memory got bigger, they started to work. So we were working with really small machines and working with LSTM networks that maybe had like 50 to 100 hidden units and then a couple of gates to control them and trying things that had to do with the dynamics of how these things can count and how they can follow time series. So you try to scale that to speech or you try to scale that to speech recognition was one try to scale that to, you know, speech recognition was one of the first things. This is really hard to do. So I think a lot of this is just due to having faster machines and more memory. It's kind of weird, right? Yeah, it's surprising that that would be it. Yeah, I think it surprises everybody a little bit. Now the running joke, like having coffee here at Brain is sort of like, what other technology from the 80s should we rescue. Oh, yeah. It's just like, I mean, exactly right. AI's back. Exactly right. How far have you pushed LSTM? Obviously, there's some amount of text generation that people are trying out. Have you let it create an entire song? No, we haven't, because we haven't got the conditional part of it right yet. So I think LSTM in its most vanilla form, I think everybody's pretty convinced that it's not going to handle really long time scale hierarchical patterning. And I'd love it if someone comes along and says, no, you don't need anything but vanilla LSTM to do this. But I think what makes music interesting over, even after like five seconds or 10 seconds, is this idea that you're getting repetition, you're getting harmonic shifts like chord changes. There's a there there, right? And one way to talk about that there there is that you have some lower level pattern generation going on, but there's some conditioning happening. Oh, now continue generating, but the condition shifted. We just shifted chords, for example. And so I think if we start talking about conditional models, if we talk about models that are explicitly hierarchical, if we talk about models that we can sample from in different ways, we can start to get somewhere. But I think only a recurrent neural network is... It would be reductionist to say that it's the whole answer, and it know it's it's it's not it's it would be reductionist to say that it's the whole answer and it's in fact true it's not the whole answer i i was thinking about how you were um was it the tensorflow or the io talk where you're talking about bach um oh probably um oh that we did stuff that was like more bach than bach yeah we nailed it yeah that's like you start making things that like are you know more palatable it's like i'll make the best picasso painting for you but it's not necessarily a picasso painting because it's not necessarily saying anything precisely right so i think i think by analogy so first in case it's not clear i don't believe that we made something that was better than bach but when we put these tunes out for likerained listeners to listen to, they sometimes voted them as sounding more Bach-y. And I think it's, imagine what these models are learning. They're learning the principal axes of variance. They're learning what's most important. They have to, because they have a limited memory. They're compressed. So if you sample from Sketch RNN with very low temperature, meaning without a lot of noise in the system, you actually get what, if you want to squint your eyes and break philosophy, is like the platonic cat. You get a cat that looks more like a cat than you want to draw, sort of the average cat. And I think that's sort of what we're getting from these time series models as well. They're kind of giving you something that's more a caricature than, than a sample, right? So, so then in the creation of art, like what are you, what are you predicting is going to happen? Like as, as magenta progresses? I think in, can I make predictions that are on the timeframe of like 28 to 40 years when no one will ever test in a thousand years, Magenta is going to be the only... Joking aside, I do believe that the machine learning and AI will continue, will become part of the toolkit for communicating and for expression, including art. I think that in the same way, I think that it's healthy for me to admit that those of us who are doing this engineering won't almost by definition know where it's going to go. We can't and we shouldn't know where it's going to go. I think our job is to build smart, AI smart tools. At the same time, I want to point out, some people find that answer boring, like's a um it's hedging but i do think there are directions i can imagine directions we could go on that'd be really cool for example um thinking of literature right um i think i think plot is really interesting in stories and that that you can imagine that that we have a particular way as humans like the kind of cognitive constraints that we have of like kind of limitations and how how we would how we would draw plots out you know as an as an author and you're not going to do it in one pass left to right like in the recurrent neural network it's going to be like sketching out the plot and do we kill this character off but i kind of can imagine that generative models might be able to generate plots that are really, really difficult for people to generate, but still make sense to us as readers, right? Like, you know, think of it, if you flip it around, like, I think jokes are hard, because it's really hard to generate the surprising turns and the re, like kind of like you go in one direction and you land over here, but it still makes sense. And I can imagine that the right kind of language model might be able to generate jokes that are super, super funny to us. And that actually might have a flavor to them of being like, yeah, I know this joke must've been machine generated because it fits in so many different ways. Right. Right. It like this high, it like fits in so many different ways. Right. Right. It like this high dimension, it like fits in so many different ways in a Matthew way, like in a high dimensional space, but it's super funny to us. Like, like, I don't know how to do that, but I can totally imagine that we would be in a world where we get that. I thought about it in the complete opposite way, but that makes sense. I was thinking about it, um, you know, training it to create pulp fiction. like that'd be so simple in my mind like you know just create these like airport novels they can just like bang out the plots um i mean that's probably where we'll start i mean i would love it if we could write so so everybody understands that's listening or watching you know we we can we can't generate a coherent paragraph right so i don't mean we magenta i mean kind of we humanity i can't write at all um it's really hard like and it all hits it all i think it all hits at structure at some level like you know nested structure whether it's music or i think there's like like art plays with geometry or color or something else um you know it's it's it's it's meaning it's you know it's nested structure somewhere and it and has the art world or you know uh i guess any kind of artist, any kind of creator, have people pushed back in the way that they're scared? You know, I imagine when photography came out, everyone was pushing back saying like this, this might end painting because it's about, you know, photography captures the essence. But then it ended up changing because people realized that that painting wasn't just about capturing something, capturing an exact moment. Certainly, the generative art world, and we've seen lots of that. Another researcher in London, someone posted on his Facebook something like, he posted to us a tweet that was like, what you're doing is bad for humanity. It's like, really? He's making new folk songs, right? He's generating folk songs with an LSDM. This is Bob's. And I was like, really? Like he's making like new folk songs, right? He's like generating folk songs with an LSDM. This is Bob's term. I'm like, it's probably not bad for humanity. So yeah, of course. But like what I love about that is, you know, it's okay if a bunch of people don't like it. And in fact, if it's interesting, what art does everybody like? it's really boring right so you have this idea that like if you want to really engage with people you're probably going to find an audience that audience is going to be some slice frankly it's probably going to be some slice coming up from the next generation of people that have experienced technology that are taking some things for granted that are still novel to someone like myself right you know and but it's you know it's okay if a bunch of people don't like it. Yeah. Well, when we were talking before, I was, I was surprised that you hadn't gotten more pushback. It seems to be like most people in our world are just like, oh, okay, whatever. It's like, do your thing. It's kind of opening up new territory rather than it is like, you know, challenging. I think that I've gotten pushback in terms of questions. I think we have, and I think this is as a community in Google and outside of Google and outside of Magenta, I think people are really clear that what's interesting about a project like this is that it be a tool, not a replacement. And I think if we presented this as push this button and you'll get your finished music, it would be a very different story. But it's kind of boring. I think it's super. Yeah, I mean, it's funny you mentioned Hacker News because I was talking with one of the moderators. We love you, Hacker News. Be nice to us. No, they're great. It's just impersonal. It's so easy to critique people. But I was talking with Scott, one of the moderators, and he was wondering if you guys were concerned with the actual cathartic feeling of creating music or if that's just something you don't even consider right now. I mean, as people, we have chorus. You have to. Yeah, and I think there's a couple of levels there. I think you lose that if what you're just doing is pushing a button. And so I think this is everywhere. The drum machine is such a great thing to fall back on. It is just not fun to just push the button and make the drum machine do its canned patterns. And I think that was the goal. My sense of the reading that I've done is this will make it really easy. But what makes the drum machine interesting is people working with it writing their own you know writing their own loops or their own patterns changing it working against it working with it and so um you know I think this project loses its interest if if we don't have people getting that cathartic release which I believe me I understand what you mean that's thing one the other thing I would mention is um I if if there's anything that we're not getting that I wish we were getting more of as creative people, people coding creatively, right. We talk about creative coding and it like this kind of hand wavy sense, but like, like, like I would love to have the right kind of mix of models in Magenta and in open source linking to other projects that you as a coder could come in and actually say, I'm going to code for the evening and add some things. I'm going to maybe retrain. I'm going to, maybe I'm going to hack data and I'm going to get the effect that I want. And that part of what you're doing is being an artist by coding. Sure. And I think we haven't hit that yet in Magenta. And I'd love to like get feedback from whomever like in terms of ways to get there the point is there's a certain catharsis for those of us that train the model you get the model to train it's like you know it's like it's fine you won't you'll be bored if you just push the button but feels good for me to push that button because i'm the one that made that button work you know so there's that right you know it's a creative act in its own right and have people been like uh creatively breaking the code like oh it would be funny if it did this or interesting if it did that um a few though i think our code is so easy like most open source projects need to be rewritten a couple times and i think you know we've gone through we're on our second rewrite um is that if the code is brittle enough that it's easy to break uncreatively then it's hard to also break it creatively. And listen, I'm being pretty critical. I'm actually, I'm really proud of the quality of the Magenta open source effort. I actually think we have, you know, well-tested, well-thought-out code. I think it's just a really hard problem to do coding for art and music. And that, you know, if you get it wrong a little bit, it's just wrong enough that you have to fix it. And so we still have a lot of work to do. So then where does that creative coder world go? I've seen a lot of people that are concerned with even just preserving. I think Rhizome is doing the preserving digital art project. What direction do you think that's going to go in? Presumably a number of cool directions in parallel. The one that interests me personally the most is reinforcement learning. And this idea that models train... So there's a long story or a short story? Which one do you want? Long, for sure. Okay, yeah, okay. Well, not long. It's not that bad. So generative models 101, you start generating from a model trained just to be able to regenerate the data it's trained on. You tend to get output that's blurry, right? Or is just kind of wandery. And that's because all the model learns to do is kind of sit somewhere on the big. Imagine the distribution as a mountain range and it just sits on the high mountain top. Kind of plays it safe. Yeah, kind of plays it safe. All T-shirts are gray if you're colorizing because that's safe. You're not going to get punished. And, you know, one revolution that came along, thanks to Ian Goodfellow, is this idea of a generative adversarial network that is a different, it's a different cost for the model to minimize where the model is actually trying to create counterfeits. And it's forced to not just play it safe. I don't know how. This is too technical. It's very interesting to me. Yeah, this was part of the talk, right, where you cut out the square. Yeah, exactly. Yeah, I saw that part. So another way to do this is to use reinforcement learning. And it's slower to train, because all you have is a single number, scale or reward, instead of this whole gradient flowing, than GANs, but it also is more flexible. Okay, so my story here is that the GANs are part of a larger family of models that are at some level critical. Everybody needs a critic, and they're pushing back, and they're pushing you out of your safe spot, whatever that safe spot is, and that's helping you be able to do a better job of generating. We have a particular idea that you can use reinforcement learning to provide reward for following a certain set of rules or a certain set of heuristics. And this is normally like, if you mention rules at a machine learning dinner party, everybody looks at you funny, right? Like you're stepping backwards. Yeah, you're not supposed to use rules. Come on, we don't use rules. But instead of building the rules into the model, the AI is not rules. The machine learning is not rules. It's that the rules are out there in the world, and you get rewarded for following them. And we had, I thought, some very nice generated samples of music that were pretty boring with the LSTM network. But then the LSTM network trained additionally using a kind of reinforcement learning called deep Q learning to follow some of these rules. The generation got way different and way better and specifically got catchier. What were the rules? The rules were like rules of composition for counterpoint from the 1800s. They were super simple. Now we don't care about those rules, but there's a really nice creative coding aspect which is think of it this way i have a ton of data i have a model that's trained i have a generative model whatever it may be it may be one trained to draw it may be one trained for music and that model has kind of tried to disentangle all the sources of variants that are you know that are sitting in this data. And so it's smartly generated. It can generate new things. But now think like, as long as I can write a bit of code that takes a sample from the model and evaluates it, providing scale or reward, anything I stuff in that evaluator, then I can get the generator to try to do a better job of generating stuff that makes that evaluator happy it doesn't have to be 18th century rules of counterpoint right so you could imagine like taking something like sketch rnn and adding a reinforcement learning model that says I really hate straight lines and suddenly the model is going to try to learn to draw cats but without straight lines the data is telling it to to learn to draw cats, but without straight lines. The data is telling it to draw cats. Sometimes the cats have triangular ears with straight lines, but the model is going to get rewarded for trying to draw those cats that it can without drawing straight lines. And straight lines was just one constraint that I picked off the top of my head. It has to be a constraint that you can measure in the output of the model. But like musically speaking, if I could come up with an evaluator that described what I meant in my mind by shimmery, really fast changing, small local changes, I should be able to get a kind of music that sounds shimmery by adding that reward to an existing model. And furthermore, the model still retains the kind of nice realness that it gets from being trained on data i'm not trying to come up with a rule to generate shimmery i'm trying to come up with a rule that rewards a model for generating something yeah it's very different right yeah so i think that's one really interesting direction to go in is like opening up the ability if you can generate scalar reward and drop it in this box over here and we'll take a model that's already trained on data and we'll tilt it to do what you want it to do that kind of underlies a fear that people have right which is like what happens when you can create the best pop song and what do people do and do you have thoughts on like is a is that possible and b what would the world look like if that world comes to be i think that it is i had an algorithm for, which is, which is the best pop song for me, which is when we used to sell used CDs, it was usually like a two to one. Okay. So every time if you have a thousand CDs and you trade them in and you have 500 that you like better and you just kind of keep going, right. You finally get that one hill climbing in that space. Yeah. I think that what I... So I'm not sure. Part of me wants to say people love the kind of rawness and the variety of things that aren't kind of predictable pop. But let's face it, people love pop music. There's a kind of pop music that you'll catch on the radio sometimes that isn't like, most of your listeners are probably in the same camp, or viewers. There's pop that we love. I love the poppiest of Frank Ocean's music. I can listen to it forever. But then there's the gutter of pop. You can't even distinguish who the artist is, but they play at the big festivals so i guess that kind of unasks the perfect pop i mean pop is such a broad thing yeah but yeah i think like i can imagine that with with machine learning and ai at the table we'll we will here's another way to look at it like some things that used to be hard will be easy right and so we'll offload all of. And if people are happy just listening to the stuff that's now easy, then yeah, it's a problem solved and we'll be able to generate lots of it. But then what people tend to do is go look for something else hard, right? It's like the drum machine argument. So you've solved the metronomic beat problem. And then what you actually find is that artists who are really good at this, they play off of it and they're allowed, like when they sing, to do many more like rhythmical things than they could do before. Because now they have this, this scaffolding they didn't have to work with before. They just constantly break it, right? As soon as you distorted the electric guitar. But I hope that's an honest answer to your question. I different flavor it's like hey are we really moving towards a world where we're going to generate the perfect pop song um yeah i don't know i don't think so i mean i don't feel like that's gonna happen um but you know maybe it happens so quickly and then as soon as we realize like okay this is how we're gonna break it this is how we're gonna retrain ourselves yeah it can learn so fast that it's like okay now i can do that too yeah that's yeah that's nice other than i can do that too um so then uh what like what i was wondering is there like in in you know the next like handful of years is there like a holy grail that you're working toward uh for magenta like okay now we've hit it like this is the benchmark that we're going for there are a couple of um there are a couple of things I'd love to do. I think, I think composing, creating long form pieces, whether they're, whether they're music or art, I think is, is something we want to do. And this hints at this idea of like, not just having these things that make sense at like 20 seconds of music time, but actually say something more. That direction is really interesting because I think that not only, so let's face it, that would be at least more interesting if you push the button and listen to it, but also this leads to tools where composers can offload very interesting things. Some people, I'm one of these people, I'm really obsessed with expressive timing. I'm really obsessed with musical texture. Okay, I don't know what that is oh no I just mean like um let's say you're playing the piano oh I know like in the gray art space or the gray talk you were contrasting the piano yeah you did your homework yeah exactly I just watched a bunch of YouTube videos so you know you know if you listen to someone play waltz it'll have a little little it. Or like some of my favorite musicians like Thelonious Monk, if you're familiar, if you're not familiar with Thelonious Monk homework, go listen to him. He played piano with a very, very specific style that almost sounded lurching sometimes. It was very, he really cared about time in a way. And so like, if the way that you're thinking about music and composition is really, really caring about kind of local stuff right it'd be very very interesting if you had a model that would handle for you some of the decisions that you would make for longer time scale things like like when do chord changes happen right so like usually it's the other way around you have these ai you know machine learning models can handle local texture but you have to have to decide that so um yeah my point is if we get to models that can handle longer structure and nested structure, we'll have a lot more ways in which we can decide what we want to work on versus what we have the machine help us with, right? And has this now, has it affected your creative work? Or do you still do creative composition? Yeah, so I'm working here at Google. And this is like a coal mine of work to do this Project Magenta every day. No, joking aside. Yeah, as we're here. Yeah, plus two kids. Plus two kids. Yeah, no, basically, I've been using music as more of a catharsis relaxation thing. I don't feel like personally I've done anything recently that I would consider creative of a level that I want to share with someone else. It's been more like jamming with friends or like, you know, just like throwaway compositions, jamming. I like, here's 10 chords that sound good. Let's jam over it for the evening. And then like, don't even remember it the next day. And really trying hard to understand this creative coding thing. Like that's something else I've worked on. A lot of it's just like, I'll start and then I'll get distracted. But yeah, so that's sort of the level of my creative output, I'm afraid. Well, the creative coding thing, it's seemingly, I don't know, so many people are looking for it in every venue. And it's so difficult to find people. There's kind of like one-offs now. Yeah, I think that's right. It's so hard to have the right. I think maybe we need the GarageBand of this. Like we need the garage band of this. We need to have something that's so well put together that it makes it easy for a whole generation of people to jump in and try this, even if they haven't had four or five years of Python experience or something like that. I didn't know if that's what you were alluding to when you were saying that command line, obviously not the way to do it, where it dumps MIDI files, but now it's an API, right? What is the next step that's very obvious? Try to make it more usable and more expressive. Expressivity is hard in an API, right? It's so hard to get it right. And I think it's almost always multiple passes. So we've got, I think, the API, the core API that allows us to move music around in real time in MIDI and actually have a meaningful conversation between an AI model and multiple musicians is there. And there's just a bunch more thinking that needs to happen to get it right. Cool. So if someone wants to become a creative coder or wants to learn more about you guys, what would you advise them to check out? So I would say the call to action for us is to visit our site. The shortest URL is g.co slash magenta. Okay. It's also magenta.tensorflow.org. We can link it all up. g.co. And have a look at, we have some open issues. We have a bunch of code that you can install on your computer and hope you can make work, and maybe you will be able to. And we want feedback. We have a pretty active, and we certainly follow our discussion list closely, and our game for philosophical discussions, and our game for technical discussions. And beyond that, we're just kind of keeping rolling. We're just going to try to keep doing research, and keep trying to build this community. Great. Thanks, man. Sure. It was fun.