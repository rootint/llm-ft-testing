 We ought to start with a little bit of your background. So what did you start researching? And then what are you researching now? Okay. So I started out my research in mathematics in Austria, in Vienna, where I actually didn't look at image processing or imaging at all. I started out with so-called partial differential equations, which are equations of a function and its derivatives. So they can express change over time or space. And they are models for various natural phenomena in physics and biology. Lots of things are explained by these differential equations. And my first paper, again, had nothing to do with image processing. It was on the khan-hillad equation which is an equation that describes phase separation and coarsening in alloys in metallic alloys for instance so when you cool them down to a certain temperature you have you have a mixture of two and if you cool them down to a certain temperature they are starting to separate from each other and coarsen out and build these larger areas. And so there is an equation that models this kind of phenomenon, which is the Cahill equation. Okay. And my first paper was on the stability analysis of a certain type of solutions to this Cahill equation. Stability analysis meaning that if you perturb your initial condition a little bit how much is your stationary solution that is when you let time evolve infinitely okay how you know when a stationary state is a state of where the system is in no change okay how much do these stationary states differ from each other when you just perturb the initial condition a little bit and this is in the context of creating alloys or building structure alloys for structures or what was there any particular purpose? Well the purpose is a lot with these differential equations to simulate certain phenomena and so if you understand how stable these stationary states are so if you are at a stationary state and then you perturb the stationary state a little bit is it going back to the same stationary state okay or is it going somewhere completely different so you kind of understand how the system how these systems uh react to perturbations that are naturally occurring because we are in real life and things happen gotcha okay yeah uh so it's more an understanding of the physical processes involved in you know mixture of alloys for instance or things like that and were you at a technical university where you would be like focusing on alloys or this was a personal interest actually you know a lot of applied mathematics on the continent, which is everything else in the UK, basically here in Europe, is applied mathematics very much means that what you're doing is inspired by applications, but eventually you end up with a mathematical problem. So it was really the driving factor was, well, we were interested in analyzing this equation and there were techniques coming up that are kind of cool. Yeah, so it was just a kind of intellectual interest in this equation that was the driving factor for this particular paper. But then during writing this paper, researchers at UCLA, in particular, the group of Andrea Bertozzi, used this same equation to do image restoration. And image restoration, meaning you have a digital image, and there are parts of this image, which are damaged, for some reason, or which are damaged for some reason or which way which uh which are where you have objects which are occluding some other object of interest that you want to get rid of the occlusion or something like this so you have one part in the image that you somehow want to replace by something that is suggested by the surrounding area of this of this region so is is this similar to content-aware fill in Photoshop? Exactly. Okay. Exactly. Okay. But this predates the Photoshop development, I assume. It actually does. And I mean, also the content-aware fill is actually very much based on some of the things that have been initiated by people like Andrea Batozzi. So, I mean, the technique is different in what Photoshop is using, but it's still based on research in mathematics. In fact, it's a differential equation. Maybe if you want, if you wish that is more, it's not the cannelia equation, but it's a different type of differential equation that is non-local is taking patches in images and kind of copy and pasting them into the region that you want to replace yep um but anyway so she used the canelian equation to do that and um that was a kind of eye-opening moment and then i i i moved into image processing still sticking to differential equations at the time and actually looking at image restoration so at this photoshop content aware fill uh type problem um and yeah and that and that was basically my phd my phd was about image uh restoration okay and during my postdoc then I moved more and more into what is called inverse imaging problems, where what you are observing or what you're measuring in the first place is not an image. Like when you take a photo, you know, the digital image is an image. there are certain applications like in biomedical imaging where what you're observing is not an image directly, but there's some transform of this image, like an image tomography, for instance. Okay. Think about CT, for instance, computer tomography, what you are, what the CT Tom, what the tomograph is measuring are projections of your three-dimensional object, which is whatever you have in your body. And from that, you want to reconstruct the object. So projections meaning in the CT sense, a particular sense, which is that you send x-rays through the body. So you're sending them through. What you're measuring at the other end is the attenuation that they feel when they travel through the body, depending on which type of tissues they hit. And so that's what you're measuring on the other hand. And you can model that by saying what you're measuring is an integral along the line that the x-ray takes through your body where you're integrating over the attenuation that it feels yep um and so from that and that is a very old problem it goes back to radon it's called the radon transform what you're measuring is not an image but it's the radon transform of your image uh which are line integrals over the image density that you want to reconstruct right that consists and where the density is different in different parts of your consists and where the density is different in different parts of your body and then you can see organs in your body and stuff like that right and so the likelihood of there to be some amount of it missing that you need to fill or recreate or denoise is very much higher than an image yeah that's obvious that's quite obvious because well first of all um we are in a finite dimensional world. So, you know, you don't have all possible infinitely many line integrals of your body measured. Yeah. And then it's not even, you know, it's not even, that would be still okay if you're measuring as many line integrals as you're corresponding to the resolution of the image that you then want to compute from these line integrals. But're corresponding to the resolution of the image that you then want to compute from these line integrals. But then very often it's not like that because you want a very high resolution image because you want to look at all the details in the body. Right. But you don't want to measure so many line integrals because you don't want to radiate the patient so much you don't want to send tons of x-rays through through the patient so you have a lack of data you you don't have as much data as you want the you know for a high resolution image to reconstruct and then there is noise because these are measurements right and there is always noise and measurements and so were you doing denoising work as well at the same time it's it's uh it's it's integrated in the reconstruction approach so in in the in the mathematical algorithm that reconstructs an image or you know the three-dimensional yeah inside of body, from these line measurements, the denoising is integrated into this reconstruction step coming from these line integrals reconstructing a 3D object. Gotcha. I know about denoising mostly through audio, like a Fourier transform and that kind of thing. So how are you doing it with an image? How are you denoising in the algorithm? So with images, it depends on what you think is important in the image. That will determine how you're going to denoise it, let's say. A very successful assumption that has been made for designing image denoising approaches is and has has been and still is that the most important information that visually guides you of what this image is showing you but also that helps you if you later want to quantify something in the image are the edges in the image this is the most important thing where are boundaries between different objects okay when you think about it what really what really makes an impression on you of what this image shows are colors you know and the and the boundary between these colors where are the colors changing and these are the edges in the image. And to preserve those and not make them blurry, blurred out, is something that a lot of research in image denoising has gone into. So image denoising methods which can preserve edges in an image. So image denoising methods which can preserve edges in an image. And so the Fourier type techniques are good. They can smoothen out the noise by taking away the high frequencies. But they will take away the high frequencies everywhere, which means they will also take away the high frequencies that correspond to edges where the image function is changing rapidly. So you're looking for the delta. This is a very high frequency component of your image, but this is a component you would like to keep. And so you want to differentiate between the high frequency components in the image, which are just noise, and the high frequency components, which correspond to these very characteristic features that you want to keep. And so, you know, there are various techniques, but one very successful one is total variation regularization, for instance, which is a technique that has been used a lot by people in image denoising to, you know, that models this assumption that you have sharp discontinuities. Median filtering is a maybe simpler thing to understand or that people might have heard about, which is not exactly total variation denoising, but it's related. MARK MANDELMANN- Got you. FRANCESC CAMPOY- So median filtering instead of Gaussian filtering, maybe, where Gaussian filtering corresponds to your Fourier taking away the high frequencies type of thing. Oh, okay. Got you. You know, it's so funny. When I was doing Photoshop of the onion, we were always actually interested in blurring edges because one of the most obvious things to spot a Photoshop is a sharp edge and a soft edge in the same photo. Okay. So, for instance, like if I were to cut you out and then put you in front of the White House, if the photo has a slight blur, so like the depth of field in the photo is like, say like a 1.4 aperture, which creates a very, very like shallow depth of field. So there's a lot of blur. But if you're crispy, someone can immediately spot that you were dropped into the photo. So it was all about blurring the edges to trick someone into thinking that it was in the same photo. Yeah, okay. to trick someone into thinking that it was in the same photo. Yeah, okay. So in your context, these algorithms that will handle the edge sharpness, are they hand-coded or are you using machine learning to create them? How does that work? So they are classically hand-coded. And this is maybe something that is now more and more being replaced by other things where image denoising nowadays, I think the best image denoising approaches are actually coming from deep neural networks. Okay. So, you know, these handcrafted methods get more and more beaten in terms of performance by some of these neural network approaches. They get beaten in certain scenarios though. Because they get beaten on the type of examples they have seen already or similar type of images that they have seen already. If you present them with something completely different, if you only train them on photographs of animals or whatever right uh and then you present them with a ct image or with a ct scan they will they will not be able to handle that so that is one of the things i think we're still handcrafted models have a certain justification of existence in a sense because there is still, although we can do GPU programming and everything, there is still not enough computational power to train a machine to know everything, to learn everything about the world. Right. And so I think a lot, so while in certain scenarios, if you know what you want to apply your image denoising approach well it's like the image net thing from like almost 10 years ago exactly yeah if you know that then it's fine and um that's good but if you uh if you want you know think about for instance one big thing in CT, let's say, or in different types of biomedical imaging, let's say MRI, magnetic resonance tomography. The type of image that you get, the resolution, the contrast and everything very much depends on how you do the acquisition. How many, let's say in the CT case, how many x-rays you have have been shooting through the patient um but also also and that is actually um connected to what i just said also the type of scanner you're using are you using a g or siemens or toshiba or whatever they they have different settings and they have different ways of going from the measurements to an image and so you know if you train an algorithm for instance a neural network on one of these scanners it doesn't mean that it works on the on images of another scanner really so they're producing entirely different data i thought they were just like basically the same tools inside with a different logo well it's so so this is the other interesting thing it's not entirely different right you might not spot also visually what the difference is but uh this is one of the things that also people start you know more and more hopefully start to you know uh do some research and understanding this, that even small perturbances that are consistent in small differences that are consistent between the different scanners might contribute to your algorithm then failing. I don't know if you have seen these adversarial errors where you do a little perturbation and then all of a sudden it classifies the image into something completely different. Right. So, yeah. So I think the, the, the really very exciting and, uh, for mathematicians in particular, the exciting opportunity that neural networks are now offering in contrast to these handcrafted models are that they can go beyond just saying, yeah, I want an algorithm that preserves edges, which is a very simplistic view of the world. But on the other hand hand that there are lots of unknowns in these algorithms on the one hand that mathematicians i think should be exploring and try to bring some of the analysis and some of the methodologies that help us to understand why these handcrafted models work because we we can prove um properties about the denoising um abilities of these methods of how stable they are for instance to perturbations in the images we know we know how that works so we can prove things about that we have error estimates and things like this and to bring those over to neural networks, I think, is very exciting. But for that, bringing some structure into these neural networks is also important. And that might, on the other hand, when you think about these neural networks having these 100 millions of parameters that are adapting themselves to the data, maybe in some cases it would be better to not have a million parameters, but have an intelligent structural way of reducing the search space. Right. And as such, bring some structure into the problem, which helps you make statements about stability and things like that. And also statements about what the algorithm is actually doing. also statements about what the algorithm is actually doing. And statements about what the algorithm is doing. Yeah. Because, so that is another thing, right? Because since when you look at these handcrafted models, you have started with hypotheses, right? You have started with hypotheses of edges are important in images. And then you come up with a hypothesis of edges are important in images. And then you come up with a mathematical algorithm that is exactly doing what you want it to do. Or then you have to make sure that it is actually doing what you want it to do. And if it doesn't, then that code is bad. The code is bad or your model is bad. Maybe you have to change your model in a certain way. Okay. But you understand why things are happening. If you have millions of parameters and then you train this algorithm to do something and then you get a parameterization that is a one million different parameters, how are you ever going to interpret that there are ways you know where machine learning people are are trying to interpret uh classification results for instance yeah you have these salient features that you can detect in the image what was important for the classification to do this or this yeah but it's still limited and i think uh yeah there are lots of very very cool opportunities and so are you guys working on hand stitching the two together at this point like what's the status of the current research yeah so there are different people are trying to do different things so um i can i can first tell you what i've been doing over the last couple of years. So the last couple of years, what I've been doing is I've been trying to, starting with these more handcrafted models, nothing to do yet with neural networks. I started with the handcrafted models. And then for certain parts in these models where I wasn't quite sure about, are edges really the only thing I'm looking for for instance I've tried to parametrize them in a certain way okay but not with a million parameters but maybe with 10 parameters or something like this and then learn these parameters from actual examples that I would like my handcrafted model to spit out. And this is what we call bi-level optimization or parameter estimation. I mean, people have been doing this for a long time, but now I think the motivation comes more from, you know, there is a certain interpretation in terms of machine learning that is kind of exciting where people are more interested in. So this is one way and levels of parameterization vary in this context. But the good thing is you have a handcrafted model in the end that you still understand. Right. And that you can still prove things about. You still have guarantees on your solution. And that you can still prove things about. You still have guarantees on your solution. You have guarantees that you don't have these adversarial errors that if you perturb a little bit, you get a completely different result. This is really something you don't want. The other thing is, and this is more blue sky, and this actually goes a little bit against what i said before um which is we have been starting to use deep neural networks for problems in computer tomography for instance um and there at the moment we cannot prove a lot of things um but we can see some ways of how to combine these more handcrafted models with neural networks in the sense of what you feed them with for instance the prior information you feed them with the data maybe not just the measurements but maybe also the information that the measurements are actually line integrals okay of the 3d object that you want to reconstruct yep um and doing this in a kind of iterated fashion where you always go back to the fact that ah actually remember neural network these are line measurements that i'm feeding you with remember this and then you you do another sweep through a neural network but then how does that work in the context of building out a model around say like i mean i don't even know know in an MRI how many images are created or lines are monitored. But, like, say you have 10,000 images, but you want to create a combination of a hand-coded algorithm and a machine learning system. How do you go about tagging all that stuff? What do you mean exactly? How are you go about tagging all that stuff? What do you mean exactly? How are you going to... So what I understand you're saying is like you're giving it more data than just like the original source material. Yes. And so how do you do that with a more like at larger scale? Huh, computationally you mean? Yeah. Okay. So computationally we are doing this in a sequential manner. Okay. So you can do it in different ways, but in a sequential manner means that you're not feeding it to 10,000 images at the same time, but you're doing it bit by bit and you're adapting your objective towards this. Another thing about computational performance is also, of course, that the optimization that is underlying, but this is not just a problem that we have. This is a problem that neural networks have in general is that you do not necessarily need to solve your optimization problem, your training exactly. And maybe sometimes, or most of the time, you actually don't want it, want to save it exactly because you only have a finite amount of training examples. And so when you think about what these neural networks are doing, they are trying to minimize a loss over the training examples that you have. But this loss is only an approximation of many, many, many, many more images that you want your neural network to work for. And so very often you do not want to solve that exactly. You don't want to minimize your loss exactly for this training set. Okay. And so there are different types of optimization methods that people are using, but the main thing in machine learning is stochastic optimization. So you don't minimize exactly for all the variables that you have, but you randomly pick a certain amount in every sweep through the network that you're optimizing for. And then you randomly change which ones you're optimizing the next sweep and so on. And just so I understand, minimizing lost, why don't you want to do that? Why don't you want to do that? So what you're minimizing, so the loss, let's say, could be the least squares error. Yeah. Let's go back to denoising. Let's say you want to train your neural network to optimally denoise images by saying for this training set where I have both noisy and clean images, I want that if I sum over the difference between the denoised image, so you feed your neural network with a noisy image, it gives you a denoised image. You want that this denoised image is closest in the least square sense to the clean image that you know in this case because you have a training set you have a label you have a true label for this noisy image which the label in this case is your ground truth image okay um and you want your denoising method which is neural this neural network to produce denoised images such that all of them are in the least square sense closest to the original label to the ground label which is the clean image okay and you want that to work over all the images in the training set. Got you. Okay. Okay. So, but let's say you have 10,000 of these images that you both know the clean and noisy image. If you would perfectly fit to this training set, if you would perfectly minimize this loss function, you could think. And again, you know, people are not really understanding this. And I also don't really understand this, but conceptually, the idea is what you actually want to minimize is not the loss just over the training set, but it's the loss over an infinite amount of images, which you then want to denoise. But you don't have all these infinite amount of images. to denoise, right? Okay. But you don't have all these infinite amount of images. So why would you want to very accurately minimize the loss over this finite amount of images? Maybe you don't. Maybe you only approximately want such that you still have freedom. Right. Such that it could be optimal also for more images that you don't have. In other words, you could train it on the wrong thing, and it could only work for, you know, like denoising photos of apple trees. Exactly. And then you're in the same place that you were in the beginning. Yeah, exactly. With a hand-coded. Okay, gotcha. So the idea is if you only do it approximately, you might be able to generalize it more. But all of this is not really so i'm hand waving here because i can't really say anything mathematically about that but are have you pushed your research into practical practical applications at this point like are you working with you know companies or student groups or or anyone else so my main collaborations are actually with people in academia, but from other disciplines. So we have been collaborating a lot in recent years with people in the hospital, in the university hospital in Cambridge. So with clinicians and medical physicists, different types of applications. You know, one of the things I said before is that I got more and more interested in these problems where you don't measure an image directly, but only indirectly via these x-rays, for instance. amount of data, the most out in terms of very high resolution images, is something we have been collaborating a lot with people in magnetic resonance tomography, in particular in the Addenbrookes Hospital, which is the local Cambridge Hospital here, but also with people in chemical engineering, where one of the driving factors for people in chemical engineering is, for instance, there is a group here, which is the Magnetic Resonance Research Center, where they look in particular at processes which are dynamic. So they have these tubes filled with water, and then they pump certain things through, and they want to understand what the dynamics of this process are. So now if you think about not just having a static 3D object, but having something that changes over time as well and now thinking back about how many x-rays not in magnetic resonance tomography these are not x-rays but just going back to an example what we had before so um not sending through as so many x-rays means you don't have a lot of data to reconstruct. Right. Which now, if you want to track something dynamically also means you're not measuring a lot per timestamp. If you want to have a very high resolution over time, it means per timestamp, you can't acquire as much data as if you would have, you know, if you just have one second for reconstructing your organ inside the body at this particular time stamp and then you then the organ is moving again and you need to go to the next time stamp and so on uh you have less data for reconstructing each time stamp as if you would have a static object and you would have 10 seconds to acquire this instead of one second. You can measure much more, right? And then you reconstruct just one image. But now we have maybe one to reconstruct not just one image in 10 seconds, but 10 images because we want to see something evolving over time. So here also the challenges are along these lines of getting high resolution out of limited data another thing which is not connected to um indirect measurements so much than uh than these applications in magnetic resonance tomography is that we have collaborations with people in plant sciences for instance so they are interested in monitoring forest health um or constituencies, let's say, from airborne imaging data. So they fly mostly in my collaboration, they fly. So not so much satellite, but more flying. They fly over forest regions and then they acquire different types of imaging data. And then they acquire different types of imaging data. They acquire just photographs, aerial photographs, hyperspectral imaging data or multispectral imaging data, which means you do not only have RGB, but you have a broader range. You cover a broader range over the light spectrum. So also the invisible light. So you don't have just three channels, but you have 200 channels or something like this in your image yeah and hyperspectral imaging is interesting so the spectral component that you get from these measurements gives you uh an idea of what the material properties are of these trees so it it tells you something about what really yeah so this is so so so the spectral component tells you something about the material that you that you are looking at so in other words like the different materials have a different signature in the light spectrum of how they reflect light back they have a different signature in the light spectrum okay um and so the intent would be to figure out you know say for instance like an invasive tree that was taking over an area. They could figure that out by just by flying right over it. Gotcha. Okay. and acquiring are LIDAR measurements. Yep. Where you do not just get kind of a planar picture of the trees, but you actually get a 3D model of the trees. Yeah. So this is also nice. I was just watching a documentary about that, about searching for Mayan runes with LIDAR. Okay. Like flying over the Yucatan Peninsula or something. Essentially like saying like, hey, we could take 20 years for an archaeologist to like dig around in the dirt or we could just fly over it and look for the hard stuff and like see what happens yeah yeah very interesting and and are people also looking to this in the context of you know for instance like denoising um like camera footage from anything you know like security on one hand yeah i haven't done so much work in that myself but there are of course you know the i mean cctv cameras are everywhere yeah i mean it's like it's kind of the terrifying output of figuring out this research right like being tracked everywhere like in the uk in particular like i imagine people are looking to do this right you know um it's quite funny because uh when you think about uh these um crime tv shows csi whatever miami or whatever they're always these these funny things right so you don't you have a very pixelated image and uh you press a magic button and then you can zoom in and you can see everything. So when you think, so this is ridiculous. Of course, you can't do that, but you can't do it now. Maybe, you know, if you have all these machine learning methods, which have learned to look at just pixels and then know what is a very probable match in terms of high resolution maybe at some point you can do it but then you don't know if you're right or wrong right just just by chance i was reading a new yorker article from i think 2010 about this guy in montreal uh allegedly finding 500 year old fingerprints using different kinds of like spectral photography okay cool i haven't heard about that okay tell me more about it uh so i don't want to give away the whole thing but and then there was an ensuing lawsuit actually from him uh the New Yorker saying they like, it was libel. But the, basically what happens is like he was accused of faking these fingerprints that may or may not have existed. And like copying them from a real one, duplicating them onto the back, using like proprietary methods to find them out. But you are interested in doing it whether whether or not it's legit like you want to you want to work i hope so i mean i'm going to tell people that it's fake yes okay yeah that's the whole idea yeah is it like yeah what direction are you going with uh with art so it kind of uh in cambridge it's, well, okay, let me say a bit more. So when I, again, during my PhD, in Vienna, there was a collaboration that we had with physical conservators, so with conservators, who were looking at particular wall frescoes, at frescoes in an old apartment in the city center of Vienna, which are called the Night Hut frescoes. I'm not going more into detail, but they were in the process of restaurating these frescoes. And so that was my first hand experience there. And there, the idea was that, you know, it takes them a long time to physically restore these wall paintings and once you have restored it there is no way back right you need to decide what to do yeah because then it's it sticks and so our idea was to uh help them uh by creating a virtual template of how the restoration could look if they do this or this. Right. Yeah. So because the important part is a fresco is actually part of the wall chemically. It's not paint. Exactly. Yeah, exactly. But even with paintings, you know, if you do something, if you do, if you manually really, you know, physically restore them. Yeah. You've done it. I mean, you can still maybe, you know physically restore them yeah you've done it i mean you can still maybe you know try to do but i mean you you're you're you're you're you're treat you're you're you're you're just well you're changing a historical piece right of the world right so mean, this is, yeah. Anyway, so coming here to Cambridge, I got to know people in the Fitzwilliam Museum, which is a museum here in Cambridge. And they're interested in illuminated manuscripts. So I met a very good colleague of mine who is the who is the keeper of manuscripts in the fitzwilliam museum um got interested in this idea of virtual restoration because illuminated manuscripts are so fragile that you that the the culture is you never physically restore them you never physically restore them they you know if they get damaged or altered over time you leave it wow okay you leave them like this and so there the idea was couldn't we create a virtual restoration and you know kind of exhibit the original manuscript and the virtual restoration next to each other and so last last year, there was an exhibition in the Fitzwilliam Museum, which was called Color. And in this exhibition, we had one piece, which was a page of an illuminated manuscript, which had been altered over time, actually manually overpainted. Okay. And what we did was that we exhibited the manuscript and next to it the virtual restoration where we took off the overpaint. And yeah, and that has led to other things. But I mean, this is, so this is kind of the idea that you don't physically change something, but you virtually do it, which is, you know, nothing damaged. You just virtually create a digital copy of this manuscript and you play around with it so you're you're not only going like back in time to see maybe like restoring it to its original you know vitality like its original color but you're actually like going deeper into the layers like this has been painted over yeah so you can go further in with imaging and then you kind of of apply everything you might already. Wow, that's super cool. So if someone's really excited about this kind of research, if they want to get into it, what would you point them to? Where should they get started? Depends what their background is. Yeah, yeah, yeah. So they have a CS degree. They're interested in imaging. So they're technical, you know, they have a CS degree. They're interested in imaging. So they're like technical, but they haven't done anything in particular, like in this field. Okay, so what I would advise is to look. So I think in particular, when you think about the US, So I think in particular when you think about the US, I think some of the cool things that came out of image processing in the last couple of years were from UCLA. So if you look at some of the applied math faculty there and some of the online lecture material or YouTube videos of some of their talks, I think that would be a good source to look at. So, I mean, very classical names are Stan Osher, Andrea Bertozia mentioned, Malik, Perona, Stefano Soato. There are lots of people. There is, now the name escapes me. I can tell you a few more things afterwards but i think just to look for mathematical approaches to image processing i think it would be the first thing i would do there are very good introductory books to look at that explain a bit of the basics. Great. But yeah, I would first start reading a little bit in these more general foundational books. And then I think just starting from that, you immediately come go to the, you know, more modern recent years research. I think that would be a, that would be a good way to start. I can catch up to you maybe. Yeah, apply here um awesome well thank you so much thanks for making time yeah thanks you