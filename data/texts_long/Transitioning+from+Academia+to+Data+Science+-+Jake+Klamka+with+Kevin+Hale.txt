 So Kevin, for those of our listeners that don't know who you are, what's your deal? I'm a partner here at Y Combinator. I actually was in the second ever batch. I was in winter 2006 and I founded a company called Wufoo. Ran that for five years and then we were acquired by SurveyMonkey and that moved us from Florida to California and that's when PG asked if I'd be interested in helping out at YC. And I've been there pretty much ever since. Yeah, and you suggested Jake as a guest for this episode. So Jake, what do you do? So I'm the founder and CEO of Insight. So Insight is an education company. We run fellows programs that help scientists and engineers transition to careers in data science and AI. And it's a pretty unique model because they're completely free of these fellowships. They're full-time. The companies sort of fund the process. Engineer scientists build projects for seven weeks. They meet top data teams, and then they get hired on those teams. We've got over 2,000 Insight alumni working as data scientists now across the US and Canada. Nice. And you always haven't been working on this. So you applied to YC for the Winter 2011 batch. That's right, yeah. And what was your idea then? So I was back in, so I started my career, and this is relevant to why I started Insight, because I basically started, I wish it had existed. When I was around, I was a physicist at the University of Toronto. I thought I was going to be a scientist for the rest of my life. And then partway through my PhD, I realized I want to go into technology. I think to myself, I'm writing code, I'm building machine learning models, this is great, I've got what I need. It frankly took me a long time to transition. Eventually got into Y Combinator, came down here from the Winner 11 batch. I was building a bunch of time mobile productivity apps that were machine learning enabled. And didn't quite get the up and to the right graph that you would hope for after YC. But it was an incredible experience. And in that sort of late 2011 after, call it six, 12 months after YC, was searching for a new idea. And actually went, spoke with Paul Graham and a few other advisors. And the recommendation was work on a problem you yourself have. You're kind of building these apps that you're trying to use these machine learning advisors. And the recommendation was work on a problem you yourself have. You're kind of building these apps that, you know, you're trying to use these machine learning models. And hopefully somebody's got that as a problem, but flip it around. Start with a problem you've had, then figure out what the solution is. And when I reflected on it, it took me a few years to really make this transition. I've been so close all along, but I didn't know product. I wasn't really connected in the Valley. There's a bunch of, you know, technically I had the fundamentals, but a lot of the tool sets were different in the industry. So I didn't know what I didn't know. And when I got down here and I started talking to people, that's when I finally started figuring it out. And I was seeing a lot of my friends having that same struggle. So brilliant mathematicians, neuroscientists, biologists, also engineers later, we found the same thing, kind of getting stuck. And like, they're like, I want to go into data science. I want to go into AI. I want to go into these cutting edge fields. But, you know, it doesn't say the right thing on my resume, or I'm kind of like, you know, just getting that last mile is really hard to cross. And I thought, okay, well, this is a problem I want to solve because these are some of the most brilliant people I'd ever worked with. A lot of them were my former colleagues from physics. And I thought, what does the solution for this look like? And at first I was focused on, it's going to be an app again, right? It's some machine learning enabled app. And then I realized now it actually probably looks more like an in-person program where folks are getting together, building cool projects and then getting started from there. And so did you just go ahead and teach a class? Yeah. So I basically started talking. First, I talked to companies. And I said, listen, I've got these brilliant friends coming out of academia who I think you should be hiring. Why aren't you hiring them? And basically, what they told me is, I know they're brilliant. I know they've got all these great skills. But they're probably like one to two months away from where I need them to be. If I had full days to mentor them for a month or two, they'd be an incredible data scientist. But they're like, I don't have a month or two to mentor them. So I say no in the interview. And so I'm like, well, I have a month or two. So maybe what Insight is going to be is that month or two where folks are filling in those last pieces of the puzzle, learning the sort of cutting edge techniques and sort of tool sets and other things. And then let's bring those data scientists in the room and have them hire. And then we just jump right in. And I ran the first sessions. First session was just me. First students. So the focus were PhDs. So that first group was in 2012. Were they like your friends? No, no. I mean, I had to go beyond my friends. So at first I started talking to my friends in academia. So I got confirmation from my friends in academia that, yeah, I mean, I already knew that they were looking for jobs and they were excited about transitioning. I got confirmation from the hiring managers to say, listen, we're hiring. We can't find folks with the full skill set. If you bring them into a room, we'll go look at them. And then the rest was kind of getting the word out and getting applications. How did you know what to teach them? Because you mentioned that you didn't know what you didn't know. Yeah. I mean, by that time, I had spent like three years figuring it out, including doing YC and meeting a bunch of data scientists and building a bunch of data products. So, you know, by that point, I kind of knew what the pieces were. But also, really, the program was focused not on me teaching the fellows. It was focused on me bringing in the sort of leading data scientists at the time and having them directly tell them. So we had, you know, Facebook, LinkedIn, Twitter, Square, all these early data teams in 2012, their heads of data science come in. So they're willing to do like one day. They just couldn't commit like a month. Yeah, that's exactly it. That's exactly it. They're like, I'll come in for a few hours, but I don't have two months. And I'm like, well, if I have a bunch of you come in for a few hours, plus really have these folks kind of working away for a few months, other learning from these mentors once we had alumni too is incredible we had all these alumni coming in to help and it's like how big was that first class it was eight fellows and then how many of them did you get them jobs all of them pretty much yeah all of them yeah 100 yeah yeah one went to facebook one to square one went to linkedin one went to twitter i mean at the time these were like they still are the top data teams but i mean it was it was it was a clear success it super stressful. I didn't have the model. I hadn't figured it out. It was crazy. I mean- What mistakes did you make? Like, was that first class kind of a shit show? Oh, of course. Of course. So what was the problem? In the sense that it was the first time I was doing it. And a lot of it, a career transition is always stressful. Whenever people are doing Ins insight they're stressed but at least there's a track record there and now we have things baked pretty well at that time the overall idea was there but the a lot of the details weren't there right and so and frankly the track records wasn't there so a lot of these folks are like what have i done i'm in a room with this guy who's never done this before like so there there's a lot of stress just around is this even going to work, this weird model? What I'm trying to understand is like, what is like, what did those eight students believe? Were they desperate? Or were you great at sales? No, no. I think they were genuinely excited. Part of the application process, I got way more applications than I expected when I started it. There was a real demand to get into the field. I didn't have a track record, but I basically went around to these universities and said, I'm going to have the head of data science from Facebook, LinkedIn, Twitter, all these companies coming in, and you're going to meet them. So the roster made them feel a lot more comfortable. Yeah. And my interview process really centered around how excited you are about this. So the folks who are like, I really don't want to do this, but I need a plan B, no thank you. It was the people who said to me, I love my work as a scientist, but I really want to have more of an applied impact in the world. I'm excited about what I'm seeing here. Here's what I think I can do. I mean, that's kind of folks I would take into the program. Totally makes sense. Starting off with like qualifying the lead. It's such a more common technique. You're seeing a lot of startups do now, like Superhuman, for example. Yeah. Heavily qualifying a lead before they'll even let them access to the product. So that way you're trying to someone that's going to have a spectacular experience. Yeah, that's why these higher managers wanted to come in. How did you figure out what students were going to be the most excited about this? What do you ask them? Yeah, so I mean, I had some opinions, but really what I did is I went to these early heads of data science teams and said, what do you look for? And what they said is, they list off some technical skills, but it's kind of like laser, they need to know SQL, they need to know Python, they need to know, you know, and it's like, I'm like, okay, but what, what really like would like clinch it for you? Like they, they, like you want this person. And there's two things always, there's like, they have a side project and their eyes would light up. They'd go, oh, if they had a side project, if you send me a URL, oh my God, then I know they're excited. Then I know they're. And so that's where the idea came around for, hey, this isn't about, you know, these folks have been through enough classes. It's about actually building. It's about actually creating something and proving that I've got all these great background, but now I'm going to do this last piece of the puzzle to show you I can do something relevant in this area. And the second thing that they wanted, and I think this is where the project really shows this, but they wanted overall is just curiosity. So folks, and I thought that they weren't being serious to be honest with you. Cause I was like, yeah, you say you want curiosity, but really you just want somebody who's good at like SQL or something, right. Or good at like machine learning. And it proved to be true. The people they were hire would be the ones who were the people who, hey, I studied astrophysics, but in my spare time I was like kind of dabbling with genomics. And then I got into machine learning on the side. And then I built this cool for fun project that like, I don't know, predicts like where I should go to, you know, camping or something because I'm a big camper or something. And then you take a person like that and that's the kind of folks that these teams want wanted and still want because these problems are so open-ended well it's like curious people don't get blocked as much right they're willing to try other things and you know it's such a new field the the the roles are our fellows are getting hired into most of the companies it's not like we know what we need you to do just do it yeah it's what can we even do here right what can we what kind of impact we have with data again how did you ask how did you test for that like curious like the project seems like okay that's something we have to shoot for but again it's like how did you know that these were the right eight people well so you know a lot of it was trial and error i would do like 12 plus interviews a day and kind of you kind of get to know folks and kind of get to know it but i think the main, the signal that I saw was, it's kind of that example I gave, is that almost people would be almost apologetic. They'd be like, listen, what I'm about to tell you is not part of my usual work, but it's on the side. And it's like, no, no, I want to hear about that. I remember I had this, one of the fellows came, well, she became a fellow, but she was in an early session. She was a mathematician at Berkeley and she had done all this incredible analysis. I can't quite remember what, like this really cool data analysis project. I think on like maybe flight times or something in sports. I can't quite remember. And partway through the interview, I'm like, but you're a mathematician. Like don't you do pencil and paper math? And she's just, I had done she's like oh yeah like i can't remember what the field was and she's like oh yeah this is this is not even part of my and she almost felt kind of apologetic but i was like this is who i want as a fellow right brilliant mathematician doing incredible work uh and able to on the side on the weekend quickly pick up python this that the other make something useful and she went on to what she worked at is, continues to work at Facebook. She went to Facebook after the program, been super successful ever since. So it's people like that, that you're like, I want you. So this is related to one of these like overarching questions we had for you. So basically it's like, how can people get into data science? And then what are the pitfalls for people who say have a PhD, you know, they know Python, they're at a higher level than a coding boot camp person? Yeah. What are the pitfalls they make when they're trying to bridge that gap and get into a data science role, provided that they didn't do your program? Yeah, absolutely. And we see it because I started with scientists, and now we also have programs for engineers who are transitioning to machine learning, engineering, deep learning research. who are transitioning to machine learning, engineering, deep learning research. And you sort of see a very similar problem on both sides, which is folks are extremely focused on the sort of technical, let me get the algorithmic knowledge down, let me know every last algorithm, which of course you need, and you need those foundations. But when you're already dealing with someone who has been doing a bunch of work for years in a PhD or in engineering in these areas, what you actually want to see and what these teams want to see is communication ability, its ability to understand the underlying business and product problem. Because what they want to do is hire someone who's going to first think about what are we trying to accomplish here? How can we help our users? How can we help our company succeed? And then figure out, how do I use my toolset of machine learning or analysis to do it? And what often happens, and this is the pitfall, is part of why you get into it is because you're excited about that analysis, excited about the machine learning. And so you start always putting that first. And you're always like, let me tell you the algorithms I can build. And it's like, what folks who are trying to transition into it need to start thinking about product, need to start thinking about business, need to ask entrepreneurs what are they actually trying to achieve. Like the skills there are making them a better salesperson. And what's interesting about the advice that we give to a lot of people about sales is it's not about selling your own thing, it's about understanding their problem and then feeding whatever you have to them and so it seems like for the data science the same thing needs to happen it's not to say right here's all the things i have right it is like try to figure out what it is that you fit into for them exactly right and it's like understand the underlying forget data forget machine learning elders what are we trying to accomplish here what's our mission what are we trying to do for our users and then like making yourself look like the solution not trying to be like oh i have a bunch of stuff which one of these things are you interested in exactly i have a hammer and a screwdriver like which like i can use all of it's like what are we trying to build here yeah and sometimes that's actually a separate role so for instance say like facebook might list a data science job, whereas some smaller startup would say, we have an engineering role open. And you might classify yourself as a data scientist. So if you have to pitch data science to a startup, how do you do that as an engineer? MARK MANDELMANN This is a great question. So first of all, data science, machine learning, engineering, these are all super broad umbrella terms. It's such a new field. Yeah. Maybe you should define data science. So yeah, maybe what I'll do is define data science. And I think, I think this is, this is going to, this is essentially answering your question, which is, so what we see in the industry, kind of broadly speaking, broad terms, details, let's not worry about the details. It's sort of, I see kind of three big pieces of how sort of data science is used. So some data science roles are what I kind of call product analytics or business analytics roles. The idea there is you're looking for a better understanding. You're analyzing data about users or a company and trying to understand how to improve it, help users succeed, help the business succeed. The second types of roles that we see are data product roles. So these are roles where you're actually using machine learning and predictive models to actually change the user experience and give them something they want right there and then as part of the product. And the third one is usually what you hear termed as AI, which is AI roles, machine learning engineering roles, where it's not just the feature in the product, that's the prediction. It's the product of machine learning engineering roles, where it's not just the feature in the product, like that's the prediction. It's like the product is machine learning, right? Like it's like a self-driving car. Like if the machine learning doesn't work, like the whole product doesn't work. And so you'll have an example of a lot of teams misunderstanding which one they need. Is there not a category in between where it's like, oh, machine learning supplements a feature or augments? Yeah. Yeah. So usually that's where folks talk about data products. So when they talk about data products, it's often like a feature. So like the Netflix recommendation engine. That's a situation where honestly, if they didn't have machine learning, they could still just say like, here are the top movies, go watch them. But with that predictive model, you're really getting a much better experience. And so we have probably 30 plus fellows working at Netflix. A lot of them work on that stuff, but some of them work on analytics, which is how are people even using this product? What can we add a more product level do to improve it? And there the output isn't a feature that the user sees, like an actual algorithm serving recommendations. There it's like they have to go and communicate with the product team to say, hey, users seem to want us to be building this sort of product for them. Let's over the next six to 12 months take the product in that direction. It's like a very different role. Here's an interesting question. So I know what the dream scenario for a lot of data scientists are. I want to get a job working on these interesting problems. What should they look out for that they should avoid in a company? Like what is a company who says, because I think everyone's kind of thinking or more than should is like looking for a data scientist. And what should a data scientist be worried about? It's like, oh, they're not ready to actually hire me. And if I go here, this will be a bad experience. So you remember how I said the data scientist needs to know what the actual problem is? The company needs to know what the actual problem is. And so the companies you need to be wary about are the ones where it's like, hey, you know what? Just like, I want deep learning. And it's like, what does that mean? What do you want us to do here? And why do you need it? And the company you want to see them succeed, you want to have whatever solution they're bringing to the market, you know, thrive in the world. And then they have a clear sense of if we add some data analysis to this, if we add machine learning to this, it's going to be better. And then you can help them get there. So someone from Twitter asked this, Chuck Grimmett asked, when do you know, or when do you know, or when do you know you need to bring in seasoned data scientists? So like, is there any kind of benchmark you can offer? Yeah, I think, so first of all, I think you have to start as a founder, start with the idea and you can do this. I recommend this before you have a data scientist, understand is data sort of critical to my, to building my product, or is it something that I'll just add on once it's already working and I need to kind of optimize the experience? So, you know, an example for sort of something critical is like Amazon Alexa, right? Like if you're building Alexa, like those algorithms, voice recognition algorithms better work from day one versus a scenario where like, say, you're on the analytics team at Airbnb and you already have a lot of users and you're just trying to optimize that experience. And so for a startup, figure that out first. And then if you need one from day one, hire one from day one. If you get a machine learning engineer in the door who really, that's their forte, you're going to be better set up for success instead of trying to hack it and then have to catch up later. Because often you don't know what you don't know. And you might not be tracking the right data. Or you're not setting things up, your infrastructure, in a way that's going to help you scale later. And then those, especially in products where machine learning is critical, that becomes challenging. One thing I recommend to startups actually is just talk to folks in the industry and frankly get an advisor. If you're not ready to hire a data scientist yet, at least maybe think about getting a data science advisor because they're going to be able to sit down. Where do you find those? Yeah, good question. I'm trying to understand, who gives that information for free? Email me. No, I mean, So I'm trying to understand who gives that information for free. Email me. Yeah. No, I mean, you'd be surprised a lot of, I mean, maybe some of the top folks who started the data science team at like LinkedIn, you know, that's, that's, that's, that's hard to get an advisor. But, you know, I think even any sort of data scientists who've been in the field who knows what they're doing will be able to sit with a founder and say, listen, you're probably gonna wanna instrument these features to collect this data because you're gonna wanna analyze this later. Or here's the type of work you want done probably down the road. Like you want someone to help you understand how to lay the groundwork to actually do that higher. That's right. You guys started off with eight students in that first class. Can you talk about where it is right now? How many students are you processing now? And then also, what is different about the curriculum and program? Yeah. Yeah. So it's definitely scaled up a bit since then. We're now in five cities. So San Francisco, New York, Boston, Seattle, Toronto. My hometown just launched it this year, which is fun. And we've got a bunch of different specializations now. So data science is one, data engineering, health data, AI. We're even sort of doing product management now, helping product managers transition to AI. So overall, we do three sessions a year. It's like almost like you have different classes depending on where you're starting on. On the specializations, yeah. Because the field's specialized, right? It used to be like you just hire a data scientist who you hope will take care of everything. And now you want folks who are building infrastructure, the data engineers. You want the data scientists who are building the early prototypes and figuring out what to build. And then more often than not now, you need machine learning engineers to really put that into production now. And so you see these different specializations, and we essentially have a program for each. So the data science program is for PhDs, because that sort of scientific experience is critical. The AI program, for instance, is for PhDs, but predominantly for engineers, actually, who are going into machine learning engineering roles. Then how big are these classes? So overall, across all the cities and programs, we're at about 300, just over 300 fellows per session now. But each program is small so we keep it sort of maximum 20 to 30 35 fellows and because the idea is each one of those sub programs that's right each programs in each location because you want that the collaboration is critical you want that group to sort of gel everybody's working on a project you want you want people kind of uh tapping each other on the shoulder asking for help. You want that alumni who's coming in to be able to kind of sit with the fellows. How long is the class? And the small groups really are critical for that. How long is the class? Seven weeks. Super fast. So what gets done in seven weeks? Yeah, so it's pretty incredible how fast people learn and what they build. So literally, they'll go from, in week one, they'll go from in week one trying to come up with the idea or partnering with a startup. So often fellows work with startups. We have a partnership with YC. So right from the get-go, they start with a project. Well, week one is figure out what project. So your first week is like, should I come up with something on my own and build it based on advice I'm getting from our alumni, from our mentors, our team? Or should I go partner with a YC startup who's got a data challenge that they want solved? And so that's step one, is figure out what you're building. And again, figure out what problem you're building. In the next couple of weeks, you better build it fast. So folks have to go from literally nothing to like an MVP in a week or two. And then they're out presenting those projects in a few weeks time. They're working individually because they're trying to show that they're able to kind of execute end to end on a real world problem, but it's incredibly collaborative. So if you come to Insight, it's like, it's, it's doesn't look like a classroom. It looks like kind of like a startup office and everybody's just kind of at desks sitting together and people are on whiteboards. They're talking to each other, helping each other, because you encounter the same problems, technical, otherwise, and it's that collaborative aspect that allows people to move super fast and learn a ton. And if you're in the program, or you're just checking out the program, maybe applying for jobs like this, what are the types of projects that you recommend avoiding? Things that people have seen a hundred times before. Yeah, I recommend, like, are people happy on Twitter is like, that's maybe done. That's a bad example. I'll give a general example because there's people have been doing like this. The more kind of, I think, useful example is make something useful. Right. So I think it's really easy to just be like, I took this algorithm that used to operate at 99.1 percent and now accuracy and now i'm going to make it 92.3 like you know and i don't know why but like it's better now right yeah or uh what you see scientists sometimes do is this very generic like i studied um you know here i'll give you an example of a project i love that i felt come up but you know so here's the bad version here's the version someone did at Insight, but you can do this at home. So the bad version, so let's say the topic is solar panels. You want to understand solar panel usage and really enable people to adopt solar panels. Bad project is I analyzed general trends about solar panel usage in California, right? It's like, look at this interesting fact I found about, it's like, okay, whatever, right? Maybe for an analyst report, that's interesting, but not for actually getting anything done. To me, it's like, it has no call to action. Exactly. Like you want it to be almost opinionated because that way a business knows it's like, oh, I can look at this and I don't know what to do. That's's right. Oh, here's some information you might want to consider. It's actually the problem I have with a lot of like analytics startup. It's like all you do is like just tell me that I don't know anything. Yeah. But now I still don't know what to do. So I've paid to be told to figure stuff out or it feels dumb. Exactly. And so the good version of this project is i'm a homeowner should i buy solar or not will solar be profitable on my roof okay that's a hard problem what's the weather like what's i mean a ton of different factors press there's some predict some predictive aspects of all that all this the fellow took all this data synthesized it build a predictive model i come in i type in my address it tells me whether i should buy solar oh they basically build a product. Yeah. That's great. So all these projects are very product-focused. They're so product-focused that sometimes companies are like, why are you showing us products when we just want data scientists? And the answer is because that demonstrates that people can think product-wise. And they end up loving it because they sort of abstractly don't understand why they're showing us products but but people gravitate to real solutions and then well they hire the fellows so well so this is related to something we talked about the other day which is like in the future are more data scientists going to become founders or is that like personality that mentality like is that best suited within a big company the thing i absolutely i think oh really yeah so this is not going to be a case like designers who've like, for some reason, designers don't tend to become founders. You know, we'll see how it shapes up in terms of like, is it going to be on mass data scientists? But certainly I would say probably about a quarter of every fellows program I see like raise their hand when they say they want to start a company in the next five years. Oh, shit. I see like raise their hand when they say they want to start a company in the next five years um oh shit i'm gonna get my ass out there yeah yeah and so and so i think that's going to be a big thing we've already seen some of our uh alumni start companies although again it's early and in the early days we have very few fellows so but uh uh diana wu was in uh one of the early sessions started trace genomics a genomics company which uh uses genomic data to tell farmers when to plant when not to plant super interesting not an alumni uh but like an early mentor uh ben caymans uh used to be the kind of founding engineer at um uh con academy and he hired uh uh one of our fellows lauren who is a a physicist. She went there, helped them sort of, you know, help impact a bunch of, hopefully impact a bunch of kids' lives by like helping them learn faster because they really have millions of data points of data on how people learn. And she was there for a few years with Ben helping with education. And now Ben went off. I mentioned Ben because he's very much kind of a data scientist at heart. And now Ben went off. I mentioned Ben because he's very much kind of a data scientist at heart. And although his title is officially CTO, he went off and founded Spring Discovery now. They're doing sort of helping aging-related diseases using machine learning to do that. Lauren went over there with him, sort of part of the founding team. And so, again, TBD in terms of what the stats are going to be in terms of founders. But that founder spirit is there. And the skill set is so useful. I mean, that's the thing. Like, regardless, having an understanding of product is like the pinnacle. Absolutely. That's what you can go for. Absolutely. Because whether you're an employee, whether you're a founder or employee 10 or 100 or, frankly, 1,000, you better know what users really want. And so do you teach that as well? Oh, yeah. It's one of the biggest things. I mean, how do you teach it? You know, I found the only way to teach is by doing. Yeah. So you say, like, build a product. And then they don't. They give you a graph that shows you interesting things. And you say, no, no, like, no. And you iterate. You just iterate. I mean, that's the learning experience. You do it wrong and then you iterate and you fix it and get better. And so the model at Insight is really just continual feedback. So if at the end of the program I tell you, like, no, that's wrong, then that's a bad learning experience. But at Insight, you'll be told like half a day in that that's like not the way to go. And by the next half day, you'll be closer there. And by the first week, you'll hopefully be on a good path to building a cool product. It's that fast iteration feedback. I think one of the things that ends up being a problem for a lot of startups or for even people getting into the data science field is they're encountering very dirty data. And so now a lot of time actually is like, this is not like, oh, I'm solving cool problems and making products. It's like, oh, I just sitting sitting here cleaning up yeah this day just so i can get to this point yeah and so i'm trying to figure out is that is this something that data scientists need to be aware of that you're just going to walk into this or something like startups need to start thinking about and like what can they do to like prevent that both but i think you can never avoid it so it really is the data scientist's job to be prepared for that to be to do well at that and that's what is that what's the ratio of the job of like cleaning versus like there's this joke that like 90 of the job is data cleaning i don't know if it's 90 but it's a lot and it's not just data cleaning because data cleaning sounds kind of lame like you're just kind of cleaning things up yeah it's it's i think more interesting that. It's like literally like what data even makes sense to get here? It's not obvious. In advance, you think it's obvious. You're like, oh, just throw some data in. What data? Of what? And how can you combine that data? And what does it mean to have clean, relevant data? And that's a skill set. Well, you know, I have an example around the founder side, right? So I think founders often make the the sort of assumption that they're tracking all the right things and then we've had many uh experiences where you know we'll talk to a founder if I was going to work with like a founder and they'll say yeah we got all the data we got everything big data big data yeah it's all it's always the best data um and then uh you know and then you you it up, and it's like, oh, shit. They didn't track user logins, like which user was logging in. They're tracking all the movements on the site, but not which movement, which user was doing that, and at what time stamp. And again, it's like, oh my god. All this data is borderline unusable because we can't kind of peg it to specific behavior and model that behavior. And you know, when you're looking at it from the data perspective, it sounds like hilarious. Like, why don't you track users? But you know what? I'm a founder. I know when you're a founder, you're thinking about a million different things. You have a million different trade-offs. And honestly, like, yeah, the logging trade-offs and honestly like yeah the logins turn on like let's go right let's build let's and then a year later you're regretting that so again uh i think a lesson learned for sure that's why it's like hey have a coffee with a data scientist like maybe all you'll get from it is like log your user uh logins but that might be enough and then and then a year later you can get started with a data scientist what's the best tools that people should do for tracking data? Is there a product that startups should use just right out of the gate that you know that if they do this, they're just going to start on a good site? Honestly, I saw some of the questions on Twitter and folks always ask about tools. I was actually asking around with some of my team, like, hey, what's the latest on this? There are great tools, I think, for just sort of like basic analytic tracking of like websites. But if you're really building products, like it's still to this day, we see the teams roll around because there's so much, there's so much. Such a disappointing. It is a disappointing answer. And I think, you know, listen, there are companies working on it, some YC companies, and they're slowly progressing up to more sophisticated sort of data products. But at the end of the day, if your lifeblood is a very specific product that does something very specific, like there's like nothing beats just having somebody very thoughtfully say, what do we actually care about tracking here? How do we track it? So stepping back then, yeah, like assuming there is no easy answer then, you're a founder, you just started your thing. Can you give me like five or 10 things that I should be tracking? Well, I mean, it really depends on the company, right? Sure, okay, fine. So I think the number one thing you have to think about as a founder is actually not even what you're tracking. Because honestly, if you think about this first thing right, I think that'll become more obvious. The first you think about this first thing right i think that'll become more obvious yeah the first thing you got to think about and think about it right is what are you actually trying to optimize what's the one or two metrics you actually care about what if you're thinking about machine learning and building predictive models like say you had a magic machine learning model that like did whatever you want but you only had one or two which problem in your company would you would you apply it to because i think what what i see folks do is, oh, I know my business in and out. And so I know my metric is this, this, this, this, this, this, and this. And then, oh, machine learning, we'll build this, this, this, this, this. And you know what? You might at some point down the road. But initially, you're going to have to focus. And if you don't have that focus that's where you get into this habit of i'll just track everything or nothing whereas if you know what you're trying to optimize is um let's say let's say i'm netflix yeah yeah what am i going to start tracking oh i mean you're you're you obviously want to see how how uh long people are watching the video how far they get in that video uh one of the teams they there less obvious is people are using different devices on different bandwidths so they track i mean they test this stuff and track it on all sorts of different different machines so again like if in a generic in a generic tool would you have a situation where you're testing like a stream on a hundred different devices and no you wouldn't because like if that's not core to your business why would you ever do that right but if you're netflix you better be doing that and because you know that user experience is the key right con academy it's something different right for con academy it's like you know maybe it's the amount of time kids are spending on a question and that's telling you something about whether they're learning where on another site it's like you don't really care about the timing you just care about the flow can i just simplify that so for us like for any startup and most companies it's like always like my goal is growth yeah and for us at yc we've actually pretty much simplified it where it's just like look for the most part your kpi that your company is actually interested in driving it's either going to be revenue, and that's like 99% of the companies. And for some consumer, a very difficult play is like, I'm going after engagement. Daily active users is the ideal. Sometimes it's weekly active users. That's just the nature of the product. And so to me, it's just like, okay, what drives those two things are really just like only two numbers. It's like conversion and then churn. And are really just like only two numbers. It's like conversion and then like churn. And so I imagine like most questions fall into those two categories. Like what increases conversion for revenue and what reduces churn for revenue and the same thing for like engagement. So maybe I'll speak directly to those because now you're kind of zeroing in on certain types of companies. And so for churn, we often have fellows build churn prediction models for startups. So again, they're customized because there's churn for what's happening. But when we're talking about churn, it's a customer deciding to stop using the product. If we can predict that ahead of time, then they're able to intervene, maybe offer a discount, maybe engage that user, get feedback. Those are top of the list. For conversion, experimentation is the key. It's like these experimentation frameworks I always feel like a lot of times startups especially early ones they neglect that whole churn question because I always tell them it's like look you're obsessed about conversion because you're in sales mode trying to bring them in but I always feel like it's very expensive and I feel like improving churn, like improving churn by the same percentage is exactly the same thing for coverage, but it's way easier, cheaper, et cetera. And so is that usually what the first projects that startups and companies should be looking at if they haven't at all? Absolutely agree. And you know, one thing I'll add about churn is it's often more reflective of what is actually working or not working, right? It of what is actually working or not working, right? It's like make something people want. It's like if you improve churn, that means you're truly understanding what the user wants. Maybe you can get them to sign up or convert just by sort of having a flashy sales pitch, but churn really you understand it. And then that's where the exploratory data analysis comes in. Do you really understand what your user is doing? That's where the A-B testing and often what's called multi-armed bandit testing where you're trying various different experiments at once. That's where you're predicting churn and then trying to intervene to help the customer. But you see what I'm saying? It's like a number of different things, all of which are grounded in do I understand what my user wants and am I building to what they really care about. I think the other big trend that you're having people sort of obsessed with metrics-wise is cohorts and retention curves over time. And so what do you see as the best things people should do? Yes, just understanding and knowing it. That's sometimes really difficult. But in terms of improving that, where does data science usually help? Right. I think it's coming back to churn, right? Because if you're seeing folks drop off at month three in your early cohorts, I mean, that's a churn problem right there. So yeah, I think it goes back to churn. A lot of those sort of dashboards, there are great tools for those. So certainly when I started, people would hand code cohort analyses. Now there's a bunch of tools for that so i'm not saying certainly i think in the metrics sort of dashboard domain there's a lot of solutions when i was saying that there isn't really a ready-made solution it's more it's more that stuff that's that's kind of uh where it's you're actually building models to improve the product in a very sort of uh deep way do you guys have a favorite for like because you said like good startups have good problems yeah are you waiting for a sponsorship i'm trying. What do you mean? For like some tool to pay you money to say what's great? So honestly, at Insight, almost everybody just uses open source, right? So everybody's building Python. Yeah, it's all the open source. And because that's actually what we're seeing reflected in the industry. So if you go to a top data science team, by far and away, the vast majority of what they're using and building on is open source what what are the what are those projects um like i think python has definitely it used to be like python and they're still building it themselves yeah yeah absolutely yeah right and then they just use like what jupiter notebooks stuff like that for prototyping um and then and then you gotta then start building then you roll your own and then you roll your own and frankly at that point as soon as you get away as soon as you get past the prototyping stage, you're really just building product. It's the same thing an engineering team does as a startup. It's like, what tools are they using to build the fundamental product? And that's where you're living. Those data scientists are often embedded with the team building directly. Who makes the best data science? From what field have you noticed, it's much better that they like from what field have you noticed we're like oh it's much better that they come from this field uh what's kind of been shocking i want to know who your favorite children what what's been i was early days i was accused of of you know i'm from physics so but now you know there's uh we have fellows from all the different backgrounds so so they all succeed no i mean i think that's been the's been the shocking thing is like how different the backgrounds are. We have a fellow in this session who's in as an archaeology PhD. We had a fellow a session ago. It was like an engineer at like SpaceX, right? Like we had. Imagine each of them. So we have certain kinds of problems. Like you have like a mathematician. That's exactly right. Going to get the the math understand the parting soul but selling themselves and understanding problems probably exactly so so often you'll find like a mathematician is great for instance i've made great data engineers because they think about large-scale systems and how can they fail i mean in math is logic systems but then they kind of transfer that that that sort of mode of thinking to to data infrastructure but someone like for instance uh psychology was one that like in the early days i didn't really have a network in kind of psychology and or or uh neuroscience so we did a lot of work to try to kind of put the word out there we found social scientists are incredible uh data scientists quite often because they know how to ask the right questions yeah and they know how to think about people and ultimately you, obviously, data is branching out. But most of the time, when you're talking about users, you're talking about customers, it's people, right? And so fantastic data scientists from those fields. But it's just one of my favorite parts of my job, actually, is the fact that I'll sit at lunch or a happy hour or just hang out at the office. And it's like an astrophysicist with a psychologist with a software engineer with a you know electrical engineer and they're all kind of working collaborating and uh it's it's just incredible incredible kind of environment to be around all these different people you have all these companies coming in talking with all your students during the program and they're usually coming like with a problem or are they just talking about here's the kind of problems we work on and solve because they're kind of doing a little bit of recruiting in addition to giving an understanding oh absolutely yeah yeah and and who's been really great at that uh we we have i mean what do they do that's really there's a bunch of teams um you know listen i i think so the way the program works is fellows will often work with a startup company on on project. But most of the interactions the fellows have with companies is actually companies coming in who try to hire them, right? And when I say companies, I mean like the actual technical data team coming in, talking about what they work on. And trying to hire them. And so the teams that do really well, listen, obviously the ones with great brands, the Airbnbs, the Lyfts, the Ubers, the Facebooks. I want to know what the little guys have to do to compete with them. But this is what I found is when startups come in, what often happens is fellows come in, what's this startup? I've never heard of it. Why do I have to go to this? And they come out, they're like, this is my dream job. I want to work at this company. And I started trying to do that and what it really boils down to is impact the startups that do well recruiting data scientists make the pitch you are critical to our success if we if oh they make it look like they're going to be all stars and and they're telling the truth because a lot of companies these days frankly if the machine learning or if the analytics doesn't work like the company will fail like that's what they're pitching well also when there's one of you versus 300 of you right well that's a personality thing right some people are excited about i'm gonna be the first data scientist and some people are like i want i want i want some mentorship i want i need a little motor on me yeah yeah but when it comes to like i've never heard of this company before and then an hour later like oh my god i want to work for them it's always the impact piece uh it's always the like if you come here what you do will matter in a big way uh and and obviously there's the technical piece that you're going to work on cool stuff yeah but i thought the technical piece would be the biggest one but the biggest one actually is the impact for sure huh so one thing we haven't talked about and i actually don't know if you have an opinion on this, is contracting. So for an average startup, say they're like a couple of years and they're like, I don't know if we really have a need for this. But we have all this data. Maybe we could put it to use. Do you see people like doing two month contracts and getting a system up and then just letting it go? What happens? Yeah, I think contracting is good for prototyping. So we see a lot of, like when I'm saying YC startups work with our fellows, that's essentially, it's a pro bono consulting, but they're working with them for the program and helping to deliver some results. And where that works really well is, you know, often it is integrated, but it's at this sort of prototyping stage. Will this even work? Or I've got a model, will this one work better if we try this? So let me give you an example of one I really like recently, a fellow work with Isono Health, YC startup, really amazing product as like sort of in-home breast cancer screening. So it's a device instead of going once a year to get screened for breast cancer, if you're in high risk, you can do it at home. Second leading cause of cancer death in women. So huge impact potentially, life-saving technology. risk you can do it at home um second leading leading cause of cancer death in women so huge impact potentially life-saving technology and obviously a big part of that is can we do we have the right algorithms to detect and notify a user that hey you need to go speak to your doctor or notify a doctor obviously a doctor does the the the final thing but is there something abnormal here that we need to be taking a closer look at they had algorithms that were working great and doing well for them especially at that stage of hey let's just bring it to a need to be taking a closer look at. They had algorithms that were working great and doing well for them, especially at that stage of, hey, let's just bring it to a doctor to be safe. But they were curious about, hey, are some of these newest sort of deep learning algorithms that are just coming out in the papers, are they going to do better for us? So Othello did that. They took the data and essentially used some brand new sort of convolutional neural network techniques that had just kind of been published and got better results for them that were almost on par with sort of expert radiologists. And so, I mean, that's awesome, right? And so, of course, that team then has to do some more work to implement it. But that's an example where I think consulting works is like, Is this going to work? Is this feasible? Is this a prototype? Anytime that becomes a part of your product, you need a team. Because it's never static. Something's going to evolve and change, and you need to be able to evolve it. It's just like asking, can a startup just have contract software engineers overseas? It's like, well, maybe to prototype something, but in general, probably the answer is no, because that product's going to keep evolving every month, every year, and you need folks on the staff to do it. Makes sense to me. Great. Cool. So I think one last thing I wanted to talk about was just areas you're excited about in particular. Yeah. We mentioned health the other day. Yeah. But yeah, what's exciting to you right now in the field? No, I mean, there's a bunch of stuff that's exciting me, but health is the top one that I'm pumped about because it, I mean, the impacts there, right? Like the example I just shared with you, I mean, early detection, disease monitoring. I mean, you're literally saving people's lives as this stuff works. And what's interesting is people have been talking about the impact in data science, machine learning, and health for years. Because you start thinking about this stuff and pretty quickly you're like, oh, this could make an impact. But actually getting it to work is tough. And only, I think, in the last few years we've been seeing a lot of teams actually making really amazing progress there. I'll give you an example I love of the impact here. Memorial Sloan Kettering Cancer Hospital in New York has hired out a team of data science data engineers from us over the last few years. And what they do is they build essentially data products that are used internally by their doctors. So these are cancer doctors um really tough situations uh and they're faced with a situation of what clinical trial do i recommend to my to my patient and there's thousands of clinical trials and there's new ones coming online every day which one do you suggest and so they're building these kind of data products where the doctor gets based on the specific personalized you know whether it's genomic or clinical factors hey you should at least think about these new clinical trials that are coming online and again the doctor makes the final decision but it's but it's hey maybe one of those trials they hadn't heard about now can save that that patient's life right and it's it's um fascinating it's a hospital stuff it's a hospital. And then soon thereafter, New York Presbyterian hired a fellow. And then Mount Sinai hired a fellow. And now pharma companies are hiring fellows. And it's really fascinating to see data broaden out when companies realize that they can just take it beyond, oh, I want to optimize this business inefficiency and really think, what can I create that's going to add incredible value. And so health is what I'm excited about. There's a ton more out there. Yeah. That's super cool. All right. Well, thanks for coming in. Thanks so much. you