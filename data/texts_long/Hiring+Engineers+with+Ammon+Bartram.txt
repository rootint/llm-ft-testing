 Hey guys, today we have Amon Bartram, co-founder of SocialCam, TripleByte, and he is here to talk to us about hiring. So could you just give us a quick intro about what you've worked on? Cool. So I joined Justin.TV fresh out of school in 2009 when it was just 25 folks and kind of went through the roller coaster of the early days of JustinTV. And there I worked mostly on the video system. And I think that was where I got my first sort of taste of hiring. At the end of that, we were hiring pretty aggressively. And that was when I first realized how much noise there is in the hiring process. And then I was part of the spin-off of Socialchem, and that was a video-sharing app. I did that for about three and a half years, and we were acquired by Autodesk in 2012, and I worked there until 2014, and then took a bit of time off and started Triplebyte. Cool. And so Triplebyte, just for context, for people, can you explain? Sure. So yeah, so we're a recruiting startup. So we help startups hire engineers. Okay. And so engineers apply to us. And then we do sort of a full interview with them, pass the engineers who are good and match them with companies where they have a high probability of doing well. Cool. So people ask us a million questions about hiring, recruiting, all of it. In general, let's assume that you're an early stage startup. What should companies be looking for in an engineer? There's not a crisp answer to that. there's not a crisp answer to that. I think the pithiest answer I can give to that is you have to decide what it is you want to look for, and then you have to effectively look for that. So sort of the status quo, actually, I think there's sort of an important truth here. Most companies think that they are trying to hire good engineers. That's what they say to themselves. And what they don't realize is that company A's definition of a good engineer is significantly different from company B's. And so what you have is a situation where everyone has this definition in mind, and they're all different. And this is a big source of noise. So for example, if one company thinks that engineers need to be very fast and productive and be able to show that in an interview and a different company thinks that engineers need to be very intellectual and be able to deeply understand computer science topics and talk crisply about that, what happens is all of the awesomely productive engineers, very practical, not necessarily an astrong academic, who applied to the second company, fail. And all of the very academic, could totally solve all your hard problems, but maybe aren't quite as flashy in a web environment, engineers who applied to the first company, also fail. And so it's, for companies that are a bit of a larger stage, I think the obvious answer is you want to hire both those people. And so it's about building a process that can identify more broadly different types of skill. For smaller companies, I think you're more in a situation where you may well actually only want one of those folks. You may well, you know, the thing that's holding your company back may well be productivity. And we need someone who's going to come in and be productive and sort of bang out code. And if that's the case, you know, you need to realize that it's not important that everyone you hire be strong in computer science. And if you are, you know, a company where you're facing security issues and you know really clean code really precise code and solving hard problems is important to you then you know at an early stage and it probably makes sense to have a process that skews more in that direction and so do you have general advice for people that come to you guys at triple byte or just you as a friend advisor um for diagnosing like what kind of engineer is a good engineer for your company? What do you tell people? It's funny. We're rarely in that situation, actually. I think most people have strong preconceptions. We're more often in the situation of sort of broadening people's vision of what a skill engineer can be. But I think the obvious... Yeah, let me circle back to the question. I think what happens a lot, this is a mistake that's easy to fall into, is when people are interviewing an engineer, they tend to ask about the things that they are the best at. There's this overlap between the things that you're the best at and the things that you think are the most important. Every engineer thinks the things that they know are kind of the core of the discipline. And so you ask everyone who you interview those questions, you bias yourself toward hiring engineers who have those skills, they join your team, they ask people, they interview the same type of questions. And so the whole organization can grow in a direction that might not make sense. It's very complicated because there are plenty of examples of companies with defined engineering cultures that have worked out very well. So for example, Google intentionally or unintentionally grown very much in a computer science direction and that's obviously worked out very well for them. There are other companies in the YC that, I just don't want to say names here and there, but there are companies that take a complete opposite, take a very human productivity friendly approach and are also extremely successful, excellent companies. And so answering that question is not... Basically there are success cases on both sides, but I think when you're hiring your first employers, I think you need to just basically try to decide what's holding us back. Yeah. And so say I'm trying to vet this pool of engineers and they all like fit the certain rubric that I've created, right? Yep. But maybe one of them did a boot camp and has some projects. And then one of them, you know, went to a great school, has a CS degree. So how do you how should I think about, you know, credentials and experience? Boot camps versus CS degree, I don't think are all that different. Now, that's obviously a forceful statement. So I think experience matters more than where you got your education. So someone fresh out of a CS program who doesn't have internships is still essentially a junior engineer. And perhaps they may have a more academic slant to what they've studied than is still essentially a junior engineer. And perhaps they may have a more academic slant to what they've studied than someone out of a boot camp. But both those people lack real-world experience. I think categorizing that differently than someone who has worked in the industry for five years and can own projects. So the skill that you can most easily measure in an interview is ability to think through rather small problems quickly. That's really what interviews can evaluate. The skill that you need in an employee is quite different. The ability to solve large, sprawling problems well, but over a long period of time. And there's obviously a correlation there. We use interviews as a proxy of evaluating actual skill because there is a correlation there. But the correlation is not perfect. And an interesting observation is that people who are fresh out of university and boot camps, actually in many cases, because they've been practicing, are better at the kind of problem that gets asked in an interview than your very senior 10 years of experience at that large company engineer. What the senior engineer has, typically, is experience making architecture decisions, owning projects, gathering requirements, carrying that whole process through. And so, okay, how do we evaluate for that? It's super hard. Basically, what ends up happening, and this is honestly unfair, is that experience is used as a proxy for that. This is something that we're focusing a lot at Trillbyte. But what, you know, this is very strange. Actually, if you have five years of experience, it's just flat out easier to pass an interview. You will get a job offer after a worse performance. People think maybe that senior engineers, you expect their senior, they should perform better in the interview. And that's actually not generally true. The bar to getting a job offer goes down as you have sort of a present-looking resume. And it's not irrational on the part of the companies. It's just the reality. Sure. Sure. Okay. And so then when you guys are pre-screening these people for TripleByte, for example, what are you looking at? What are you having them do? So the approach that we take is to evaluate as much as we can in isolation and be aware what we're evaluating. So we explicitly evaluate programming productivity given given a relatively specked out problem. So for example, describing an algorithm to solve a problem, it's not super mathy, it's just here's the set of steps that you have to do. Can the candidate take that and render it into well-working, well-structured code? Interestingly, junior folks actually often do better than senior folks at that sort of problem. We then separately do an evaluation of academic computer science skills. Is the engineer knowledgeable about computer science and about that approach to problem solving? We then separately, one thing that we took from Stripe, actually, is we do a debugging section. And so we give the candidate a large code base that has some bugs, and we ask them to take some time, dig into the code base, and then try to find and fix these bugs. And I think this does a great job of solving some of those problems, basically, because this is a skill that comes from experience that is often missed by more and more traditional interviews. And then finally, we do a system design section. So here's some requirements. Design a production web system to satisfy these requirements. OK, now we're going to change the requirements. How do you adapt your design? How do you talk about trade-offs? MARK MANDELBAUM CHEUNG. And all of that's done remotely because the person's at home, right? DAVID EASTMAN. MARK MANDELBAUM CHEUNG. DAVID EASTMAN. DAVID EASTMAN. DAVID EASTMAN. DAVID EASTMAN. DAVID EASTMAN. Are they all widely different? To go through our exercises? Our interviews are about two hours in length. And so we spend about 30 minutes on each section. Okay, cool. And you find that's like a very strong data set in terms of correlating how successful they are? Yeah. Well, we've done about 2,000 of these interviews over the last year and a half. And so we've been able to sort of drill in on the parts that are most predictive, and cut time off and shorten it. I think if you're starting from scratch, you would probably need about twice the amount of time to get through all that stuff. Okay. And so having gone through all these interviews at this point, was there anything that you thought was really important in the beginning or something that's very common in the Valley that many people think is important that isn't really important? Too much reliance on a single question. So the classic interview format in tech companies is a number of 45-minute to one-hour sessions. And often, engineers pick the questions themselves. And they're usually little nuggets of... They're sometimes pejoratively called brain teasers. I don't think... Almost no one actually asks brain teasers. They're more like legitimate programming problems. But they are little nuggets of difficulty. Like how do you, given a string of parentheses, how do you determine if they're well matched? Okay, given multiple types, how do you determine if they're well matched? If you don't, that's a classic interview question. And it ends up there's just a huge amount of noise. If you just take a bunch of candidates and in a controlled setting have them all answer three or four of these questions, you'll see there's some correlation, but there's way less correlation than you would think. I believe this in my previous, you know, when I was, you know, like you, you, you, you have this question, you ask them the question and you see this variation and you assume, oh, the people who answer this question well must be smart and good programmers. And people who get totally tripped up must not be. And then when you actually like inspect that, you see that there's this huge amount of noise and like we have this pretty incredible lens on this this because we evaluate engineers pretty rigorously and then send out to multiple companies and see what happens get detailed feedback and so yeah is that feedback from the actual interview process or then once they're placed you actually know as well how they're doing both okay but i'm kind of talking about about the interview process okay so we so we screen engineers we send them to companies and then they do the interview there, and we get feedback. And it's just pretty incredible how much disagreement there is. So a candidate who does really well at one company, and we get users reviewing movies online. And the inter-rater agreement was equivalent. So basically, knowing that an engineer did well at one company gives you about as much information about whether that engineer is skilled as knowing whether the New York Times film critic rated 12 Years a Slave as excellent or terrible. So, okay, maybe you don't have an answer to this, but say I'm really good at brain teasers. Where should I interview? Larger companies. Probably larger companies. So, and this makes a certain amount of, okay, this is all very complicated. It makes a certain amount of sense. So, bigger companies, so brain teasers always introduce noise, but we've found that bigger companies rely more on that type of interview. And they do that partly for some rational reasons. So bigger companies care more about measuring your innate ability and less about measuring whether you can jump into their particular code base and be productive on day one. But it's way more likely that a smaller company is going to say, we're using Ruby on Rails. We need a very productive Rails developer. Come take this interview and we're going to measure how well you can work on our actual code base. Whereas the big companies, Facebook, Dropbox, Apple, Google, are more likely to say, we care about smart people and you know within the confines of the noise of the interview process they're trying to identify intelligence um rather than specific experience i mean they also have like capacity time to train people precisely whereas like a small company in no way okay and then like prior um one of the questions what about um skills that people don't think is correlated that are strongly correlated to a successful engineer? Relatively easy problem solving. So we've found that asking pretty easy interview questions is often more generally predictive than asking harder interview questions. So to break this down there are two sources of signal from asking a question. You can get signal on whether the candidate comes up with the right answer. And you can get signal on whether they struggle, how easy or hard it is for them to solve the problem. And so we score both these things for a bunch of questions. And we've done this for, again, thousands of candidates. And what we found is that if you go in and look at how much the individual score on one question we're asking correlates with how the candidate does on the job. And what we've found is that, as you would expect, getting a question right is correlated with being a good engineer. And as you would expect, being able to answer a question easily is correlated with being a good engineer. But there's also, of course, false negatives. So there are great engineers who fail at individual questions, and there are great engineers who struggle with questions. And if you look at the actual predictive ability, rather than just the correlation of getting on the right side, right? The sweet spot is actually far lower on the scale than most people intuitively think. And so, yeah, can you give me like a couple examples of what those easy-er questions might be? Yeah, sure. Just like saying like, you know, we want you to create a checkers game. Absolutely no, you know, no logic, nothing complicated, just a class that has a board, has a grid, it's got pieces, pieces move around. This is really a pretty mundane, straightforward task. That actually ends up being, how well candidates do that ends up being a more stable predictor of engineering skill than sort of, here's a, you know, here's the, I'm going to give you a sentence that consists of a list of words all glammed together and you define the optimal way to, you know, give a dictionary to break this apart into words. Okay. That second problem, you know, ends up being a graph search problem that can be, you know, optimized with, you know, memorization or dynamic programming. The second, getting the second problem right carries more information than getting the first problem right. Um, but with a really high false negative rate. Yeah. And so the first problem ends up actually being a better general predictor of engineering skill. And so is there a way if I'm like getting ready for a new, uh, you know, I'm going to go to another company, I'm going to get ready to interview. Um, do you recommend people train in any particular way or is because like you're going for that sweet spot of easy questions? You just have to be smart enough to do it. What do you tell people? Sort of in general? If I'm going to prep to do some interviews, what would I do? I guess there's two questions there. One is for companies that I think are doing a good job of interviewing, and then maybe for where I think the status quo is. In general, it depends where coming from so very different advice for for new grads and for experienced folks okay so let's let's break them apart yeah new grads okay for new grads i would say um you know make sure you're solid with sort of the classic the classic sort of you know stuff that so so you know breadth first search search, hash tables, heaps. I mean, it's important. Classic core computer science. Yeah. A surprising percentage of a new question ends up being slightly obscured applications of those, especially hash tables and breadth-first search. Those two things by themselves represent probably 40% of the questions that are asked by most companies. And so you need to know those. But actually many new grads are already pretty solid on that because they've been being drilled for that throughout school. The second thing is practicing writing code under stress. So working out a big problem over time is very different than you have 10 minutes or 30 minutes, here's a marker, write in a whiteboard, or even here's a laptop program on it. And so just the things correlate, the skills correlate, but you can improve your performance by practicing. So totally put in 30 minutes a day, finding some questions online and giving yourself a time limit and sort of trying to solve some of the stressful situations. And are there good resources that people can look for? Like anything in particular? Yeah, I mean, the classic ones, right? So Cracking the Coding Interview has a pretty good list of questions. The other advice in that book I don't think really applies to startups very much, but the questions are good. Then there are a bunch of sites online that have lists. Interview Cake is one that I've seen that I think is high quality. An interesting aside to this, though, is that most companies actually want you to do these things, right? It's not, companies would prefer that all their candidates, we prefer, we totally prefer, now we try to design our interview in such a way that there's no impact, right? We don't really want to be measuring if you've been cramming on algorithms. But what interview companies want to measure is like max scale, max potential. So they would actually much rather see you in a state where you're well prepared in the material rather than in a state when you have the potential to understand it but forgot about it. And an interesting trend that's happening in the industry is companies being more upfront about what they're asking. So Facebook, for example, has started providing a sort of interview prep class to everyone who applies so that they're sort of going over the material. It's, part of me finds it encouraging because it is moving in a better direction, but that's also discouraging because it's like really sucky that you have to like take a class to figure out how to. Right, I just wonder if it's filtering for those types of people right who are just like looking for like I don't know What to do? I don't know what to do It's like well if you apply here you can take the class and then like take it like let me hold your hand the whole Way through your life Yeah Okay, so say I am going to interview at a bigger company. Is there a way to prep to do well with the brain teaser stuff? Yeah, practice. So again, there are some words that are thrown around describing any of your questions that aren't. Brain teasers are pretty rare. Some companies probably have asked things like, you know, the golf balls in a 747, but that's really very rare. companies probably have asked things like the golf balls in a 747, but that's really very rare. Much more common is application of a computer science idea to a practical problem. And there still is this leap of insight required. In many cases, I think those are bad interview questions. Companies should try very hard to ask questions where it's not like there's one thing that has to be grasped until that's grasped, the problem feels impossible. But practical application of a computer science topic represents the significant majority of questions at big companies. And so when I was in college, I interviewed at one of those big management consulting firms, and they did have all those questions. And so we spent like two months like prepping for it and i didn't get the job uh and uh i did okay on like the stupid you know ping pong ball questions you don't have to feel bad one thing one number we have that's interesting is that um the engineers who do the best at companies go on to pass uh about 80 percent of their interviews but not 100 no almost no one passes pass about 80% of their interviews, but not 100. Almost no one passes more than 80% of their interviews at companies. And yeah, so one big advice to everyone is just don't feel bad if you fail the interview. It is really not a referendum on your skill. I'm very happy to have not gone in that direction. So what about the role of like, you know, projects, someone's portfolio of like side projects, are there certain types of side projects that across the board are attractive to companies? Or is it like, you know, say, say I'm applying for a job at Stripe, and like I did, you know, x payment type project, and that would be more attractive to them. So across the board, are there things that are interesting? Let me answer that question, and then we'll talk about what I think the right thing for companies to do is. So companies don't actually pay very much attention to side projects, except for at the screening stage. So resume screen, candidate applies to the company, the company decides if they're going to interview the person at all. And there's some adverse selection bias in who applies to companies, and there's this big stream of candidates. And so at any company, there's this huge stream coming in, and they have to decide somehow. And so they do that based on resume screens, and it comes down to pretty dumb credential stuff. If you've worked at a top company, you've gone to a top school, or in some cases, if you have a project that catches by, that's impressive. And so side projects can help a lot there. But they are very rarely given away in the actual interview. And I think it's actually probably the right decision. So people who have side projects sometimes feel bad about this. But the reason it's the right decision is that most engineers don't have side projects. Most engineers have been working at a company, and it's all proprietary code, and there's very little they can show. That's 8 out of 10 in that situation. And having a consistent process, consistent, fair, like consistency is the first goal of a process. The big problem is that the process is not usually consistent. So if you make it consistent, then you can optimize it. And having this sort of other process where you look at projects introduces noise. And it's also just really hard to do. So you can't tell if someone spent a weekend on the project or if they've been working on it for the last 10 years. We literally see both pretty regularly when we talk to people about their projects. Is it something I did over a weekend, or has this been my abiding passion for the last 10 years? Let alone who actually contributed. Yeah, and things like coding quality. It's startlingly hard to look at a big bit of code and decide if you think the programmer who wrote it is skilled. Just again, there's so much context, you can't tell what bugs they spent hours over. Finding bugs, none of us are good enough to look at a code and immediately find the bugs. For that reason, for all those reasons, side projects are useful. So if your problem as an engineer, as an engineer, if you're applying for jobs and you're being screened out a lot at the resume stage, doing projects probably helps. Doing projects is a great way to obviously increase your skills, and that will be reflected in better performance on interviews. But I don't think projects have a very big role in the actual interview. And so what other things should I think about if I am being screened out? Say I'm getting a callback from one out of ten. What should I do? Flight of Trilby? That's the short answer. Otherwise, yeah, flight projects help. It just sucks. It's not malice on the part of the companies, you know, like they're overwhelmed by applicants. And so they use these sort of crude filters. And that's, I think, the big thing that we're focused on is trying to figure out how to directly measure the skill. And so that we don't have to rely on, you know, filters like where someone's worked or what school they went to. And what about things like, for example, location? So say I live in Salt Lake City and I'm interested in getting a job possibly at Facebook. Should I put San Francisco on my resume and just fly out for an interview? Do you have general advice in that area? Big companies don't care at all where you're based. They fly people in by the hundreds every week. Smaller companies do show a slight preference to local candidates. And so if your goal is to work at a small, let's say, sub-20-person startup, you're probably at a, I don't know, 10% to 20% advantage if you're based in the Bay Area. Right there. Okay, cool. So from the company side, there's like a million different interview methods that people go for. Say we are, you know, they go through TripleByte, they get screened, they're going to do an interview. Whiteboarding, pair programming, all that stuff. How do you feel about it? All the methods can work. let me sort of give a bit of a bit of an overview here um so again as i mentioned earlier the core problem is there's this tension between the skills that can be measured in an interview solving small problems quickly and the skill that matters as a programmer solving big projects over a long period of time and so the first you know the first approach you can take to interviewing is to say, okay, we're going to not do it. We're going to do trial employment or something like that. And that totally works. If you've worked with someone for a week, you have a far better read of their skill than I think anyone can get during a three to four hour interview. during a three to four hour interview. The problem is that there's a pretty strong bias in who's willing to do trial employment. And it's actually, it's adverse bias. So that, so some of the, many of the best programmers have lots of options. And if your company requires that everyone do this trial employment period, most of them are not gonna do, just gonna say no. And obviously anyone who currently has a job. They can't leave for a week. Can't do it, can't leave for a week, yeah. And of course, there's also, you're committing a week of time. And so obviously, you need some filter before the trial employment. And so I think in the end, we're left with a thing kind of like the famous democracy is the worst form of government except for all the other options. And so you have to do it. It's fundamentally inaccurate, but you still have to do it. The goal is to make it as accurate as possible. So once you're on that page, we see two sources of noise. We see noise that comes from the companies being inconsistent. So I talked about that a bit earlier. You know, just it is still too often the process that engineers are responsible for coming up with their own questions. So if you're asking every candidate different questions and coming to a gut call, there's just this far larger than anyone really realizes source of noise. And so like if you asked, you know, pick any company that has that process, if you ask them to somehow re-interview their colleagues in a blind fashion, they would likely have a 50 to 60% pass rate. So 40% of their colleagues would be screened out. And so the solution there is just to be really consistent. So make sure that you're asking everyone the same question and make sure that you're evaluating them the same question and make sure that you're evaluating them in the same way. And I think that's more important than what you're actually asking. Right? So like, the first step is be consistent. Second step is tweak that over time based on the results you see. Okay. You know, once you're doing that, I think the other source of noise we see is companies looking for different things. As an example, if earlier you have a company that's looking for super academic engineers, you have to look for very practical engineers, you have companies that think that all skilled engineers know about how operating systems work, you have companies who think that they only want to talk to people who have experience in compiled languages, you have companies who hate compiled languages and think they're old and stodgy, you have companies who want people to use enterprise languages, You have companies. It's this mess. And so I think the important thing is to untangle which of those are conscious decisions you're making about who you want to hire. So you're a banking company. You want a big focus on QA process and safe code. It probably makes sense to reject someone for being too much of a cowboy. You're a social media company. Your goal is to move really fast. Maybe you decide to have a culture where you want to move fast and break things, and you want to hire cowboys. Those are logical decisions. But very often, companies are making similar kinds of decisions almost by accident. And so introspection, deciding, OK, we want to hire those people, and then designing the process to look for it. And so in your examples, whiteboard coding tends to skew toward the academic. It tends to give preference to people who are really good at breaking their thoughts down into this sort of structured academic way and writing with a small amount of code. So you often have people who are actually really productive, excellent programmers who look really stupid and bad on a whiteboard interview. And so if you're not looking for academic skills, it probably makes more sense to put people on an actual computer and see how they actually work in their environment. Okay. And so what does the, I get the underlying question for me is like, could you engineer a perfect interview? But I wonder like, what does the interview for a job at Triplebyte look like? I mean, I imagine you made it, right? Yep. Well, so first of all, everyone that goes through, all our candidates go through a regular process. So we hire people out of our regular stream, and then we compete head to head with the companies. So we just enter them to us as well as other companies, which is kind of fun. So they first go through a regular process um and so we already you know have a pretty strong sense of how they are um uh in those areas and then just like you know my advice generally is to decide what the skills that that you preference um and so i think we we preference uh a couple things um we prefer you know data and data analysis is pretty key to our business. And so we preference people being comfortable and familiar, you know, talking and thinking about data. That skews a bit more academic, I think, than what many companies hire for. And then we, because we're in the business of evaluating knowledge really broadly, we then preference breadth of knowledge, I think, to a greater degree than most companies need to. And so what does that mean in practice? What questions would I be looking at? Again, so everyone goes through first our standard process. And so that gives us a pretty good read on just productive programming output, general knowledge of computer science, general knowledge of system design. And then we then, we do an additional follow-up onsite with the candidates, and that goes much more into depth, into data. Or if we're hiring them for a different role, we sometimes hire folks who are not working in your data. So if we're hiring to be sort of a front-end developer, that would be sort of into depth, into front-end development. And so here's a spec for a front-end developer, that would be sort of into-depth, into front-end development. And so here's a spec for a front-end project. You have two hours, build that. Or if there can be a back-end specialist, here's a back-end spec. Gotcha. Okay. And so as an engineer, should I be paying attention to like every new thing that's coming out? Is that going to be of importance when I'm doing an interview or should I be paying attention to whatever, a medium amount of it? Well, there's an interesting long-term answer. A class of people we see, interestingly, we see people who thought about that same thing 10 years ago, made the decision to not keep up. Now the industry has changed and now these folks are maybe still using, let's say, CGI and don't understand modern web stack and are indeed in a weak situation in interviews so I think the answer to your question is day one is not so important very few companies especially only generally only smaller ones are like directly evaluating flashy new tech however if you if you if you make the decision too too forcefully today and you know don't give up today and you end up being you know totally behind 10 years from now then you probably are gonna gonna pay a price yeah i mean especially if you're actually interested in starting your own thing at some point like being on the edge really really matters um okay i uh i mean maybe uh this is kind of difficult to answer but i wonder about like employee retention engineer retention are there any qualities that you've found like you can vet someone and say like okay i think the average is like 18 months or something for someone to stay around um are there qualities that correlate to longer term employment i'm not having looked at it on this recently so there's gonna be a little bit sort of off off the cuff yeah i mean mean, just the obvious things. If candidates who are excited about the mission and the actual company have a higher probability of staying than candidates who are chasing the highest paycheck. Coastal Arcana examples. Sometimes there are awesome engineers who are looking for a place to really commit, but also want to make a fair wage. So these things are all really complicated. But yeah, I think the number one thing I would just say is looking for engineers who are excited about the company and the job. Okay, cool. So kind of just wrapping up, I wonder, are there any books or things that if I'm kind of like an engineering manager, I'm going to be running a bunch of interviews that I should really dig into and I can get a lot out of? I have not actually found any books that I think are very useful. I think, this is going to sound arrogant maybe, but I think engineering is this field where it's so easy to say things that sound profound that are not true. I truly believe that 80% of what's written out there about interviewing just doesn't actually hold up. So for example, a really really idea a lot of engineers love right this is the statement that interviews don't make any sense at all and you just look at work someone has done in the past and um you know we've we've we've tested this a bunch we tried we tried scoring engineers and having talked about past projects yeah and scoring them and trying to even go even like like a full hour like going to depth in the project, doing technical details, scoring it. Just talking skill, ability to spin a tail ended up dominating the actual engineering rigor. And this was far less predictive of job performance than giving them a relatively simple programming assignment. I mean, and that kind of sucks. Like, I don't really like that's the case. And you can find so many articles out there about sort of this, it's stupid that we're asking engineers to do these interviews, why don't we just have them talk about their past experience? And if you test it, it doesn't hold up. And just for the sake of keeping things standard, right, what do you tell people do in that way when they're conducting an interview? Well, yeah. I mean, just standardize. So you'd be really careful about helping people, interestingly. So certain candidates are a lot better at eliciting help without necessarily realizing that you're helping them. It's something you've had to battle with a bunch, actually. And so we have, it helps because, again, we're doing thousands of interviews, and so it's easier for us to do this. But sort of being, we have kind of a decision tree of all different ways any of you can go and like what help we're allowed to give and what help we're not allowed to give. It's a big source of noise. Outside of doing a thousand interviews and standardizing it, I'm not sure I have a really good fix for it, but be aware that some really compassionate candidates will be like, okay, well, so, okay. So what's a common area where I might ask for help without you even realizing that I'm getting help? One is just being brave enough to ask. So saying like, being really friendly. Yeah. And then saying something with confidence that's sort of right. And then getting, there's this natural instinct to add on to and correct the error. And as an interviewer, it's really easy to do that and not realize that you're steering the person through the problem. So if you're going out for interviews, you should do exactly that. No, not realize that you're like steering the person through the problem so if you're going out for interviews you should do exactly that i have a blog post on how to fare for interviews and i do like i recommend trying to do that actually really oh that's awesome um yeah i want to add a sort of a side to that though yeah which is like actually the negative side of that which is that interviewing can turn into hazing interview Interviewing is not just evaluation. It's also like this right of the shared right of entry into a company. And some companies develop this culture around the interviews being hard and unpleasant. And as the interviewer, it's really easy to forget how much harder it is if you're the one answering the question. It's so much easier to feel smart when you're asking the question. And sometimes candidates get really flustered and can't answer a question, and it seems like it's going to be really frustrating as the interviewer if they're like, this thing is obvious in front of them and they're just missing it and they're wasting your time and you can get a little bit angry inside. And it's just really important to stay away from like the hazing but like taking out anger on them by like like you know you know i'm generally against cutting interviews short actually i think it's i think i think i think except for in the case with the interview where the candidate is in pain i think it's not worth doing i think you save some time but but you damage reputation you know they they really dislike it it's embarrassing okay um but definitely staying away from the hazing, staying away from the... So does that just mean crazy brain teasers? Does that mean cutting them off in conversation? What does that mean? Yeah. Well, it means all those things, right? So it means crazy brain teasers. Being mean, getting slightly angry and aggressive in how you answer their questions okay because you're frustrated by by how poorly they're doing okay um and a trick that we use that i think helps in that case is to sort of in the case where a candidate is totally failing the interview like flipping a switch in your brain and going from like evaluation mode into teaching mode and you're like you're like full goal now is just to explain and as friendly as possible like i mean generally you already know the person failed right this happens when the person has already essentially failed at least the problem if not the interview sure and so you're like you already have in their brain found okay this person is not passing yeah so i'm going to spend the remaining 15 minutes being friendly and explaining the answer to this question sure rather than continuing to try to elicit responses from them and what about the um just the dynamic like do you advise one-on-one interviews or how many people per interviewee? Yeah, I take an interview panel that definitely increases the stress. So we max out at two to one. So training is important, right? So if you're trying to keep it consistent, you need to have continual cross. People need to watch each other's interviews. But one interviewer, one shadower is enough to do that. Um, uh, you know, going beyond that increases the stress and I don't think it really helps. Cool. Um, so if people want to follow up with you and ask you questions, how can they reach out to you? Sure. Uh, my email is, uh, amen at drill bite.com. That's a M M O N. Cool. Thanks, man. Thank you, Craig.