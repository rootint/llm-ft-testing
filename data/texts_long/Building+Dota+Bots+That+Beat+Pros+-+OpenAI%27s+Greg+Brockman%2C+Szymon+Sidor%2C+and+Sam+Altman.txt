 Now, if you look forward to what's going to happen over upcoming years, is the hardware for these applications for running neural nets really, really quickly are going to get fast, faster than people expect. And I think that what that's going to unlock is you're going to be able to scale up these models and you're going to see qualitatively different behaviors from what you've seen so far. At OpenAI, we see this sometimes. For example, we had a paper on this unsupervised learning where you train a language model to predict the next character in Amazon reviews. And just by learning to predict the next character in Amazon reviews, somehow it learned a state-of-the-art sentiment analysis classifier. And so it's kind of crazy if you think about it, right? You just were told, hey, predict the next character. And if you were told to do this, well, the first thing you'd do is you'd learn the spelling of words and you'd learn punctuation. The next thing you do is you start to learn semantics, right? If you have extra capacity there. And that this effect goes away if you use a slightly smaller model. And what happens if you have a slightly larger model? Well, we but in upcoming years we'll be able to what do you guys think are the most promising under-explored areas in in ai if we're trying to make it come faster uh what should people be working on that they're not yeah so uh there are many areas of ai that we already developed by quite a bit there is uh there's some some basic research in um just classification deep learning and reinforcement learning. What people do is people kind of try to invent problems, such as solving some complicated games of hierarchical structure, and they try to add kind of extra features to their models to combat those problems. But I think there's very little research happening on actually understanding the existing methods and their limits. So for example, it was a long-held belief in deep learning that kind of to parallelize your computation you need to cram as small batches as possible on every device. And in fact Baidu did this impressive engineering feat where they took recurrent neural networks and they implemented the kind of GPU assembly code to make sure that you can fit like batch size one RNNs on every GPU. And you you know, despite like all those smart people working on those problems, like only very recently did Facebook kind of took a cold, quiet look at just like very basic problem of classification. And in their great paper called ImageNet in one hour, they showed that if you actually take a code that does image classification and if you fix all the bugs, you actually can get away with much larger batch size and therefore finish the classification probably much faster. And it's not the kind of sexy research that people want to see where you have some hierarchy of big RNN, but, this kind of research, I think, at this point, will advance field the most. So, Greg, you mentioned hardware in your initial answer. In the near term, what are the actual innovations that you foresee happening? So the big change is that the kinds of computers that we've been trying to make really fast are general-purpose computers that are built on trying to make really fast are general purpose computers that are built on the von Neumann architecture. You basically have a processor, you have a big memory, and you have some bottleneck between the two. With the applications that we're starting to do now, suddenly you can start making use of massively parallel compute. The architectures that these models can run on the fastest are going to look kind of like the brain, that the architectures that these models can run on sort of the fastest are going to look kind of like the brain, where the brain is basically you have a bunch of neurons that all have their own memory right near to them, and that they all talk to their neighbors, and maybe there's some kind of longer range skip connections. And that just no one's really had incentive to develop hardware like this. And so what we've seen is that, well, you move your neural networks from running on a CPU to a GPU, and now suddenly you have a thousand CUDA cores running in parallel, and that you can get massive performance boosts there. Now, if you move to specialized hardware that is sort of much more brain-like and that runs a bunch of, you know, that sort of runs in parallel with a bunch of tiny little cores that you're going to be able to run these models sort of insanely faster. Okay. So I think one of the most common questions or threads of question that were asked on Twitter and Facebook were generally how to get into AI. Could you guys give us just a primer of where someone should start if they're just a CS major in college? give us just a primer of where someone should start if they're you know just a cs major in college um yeah absolutely so it really depends on the nature of the project uh that you would like to do um i can tell you a bit about our project which is um essentially developing large-scale reinforcement learning for Dota 2. And their majority of the work is actually engineering. And essentially taking the algorithms that we have already implemented and trying to them up is usually the fastest way to get improvement in our experiments. So essentially, becoming a good engineer for our team is much more valuable than, for example, people spending months upon months implementing exotic models in TensorFlow. So just to echo this, because I hear this come up all the time, people say it's like my dream to work at OpenAI, but I got to go get an AI PhD, so I'll see you in like five or seven years. If people are just a really solid engineer, but no experience at all with AI, how long does it take someone like that to become productive for the kind of work at OpenAI that we're looking for? So someone like that can actually become productive from day one. And with different engineers who show up at OpenAI, there's a spectrum of where they end up specializing. There are some people who focus on building out infrastructure and that actually this infrastructure can range from, well, we have a big Kubernetes deployment that we run on top of a cloud platform and building tooling and monitoring and sort of managing this underlying layer. And it actually looks quite a bit like running a startup and that a lot of the people who are most successful at that have quite a bit of running at large scale in a startup or production environment. There's kind of a next level of getting closer to the actual machine learning, where if you think of how machine learning systems look, that they tend to be this like magical black box of machine learning, this core. And you actually try to make that core be as small as possible because machine learning is really hard, eats a lot of compute. It's really hard to tell what's going on there. And so you want it to to be as simple as possible but then you surround it by as much engineering as you possibly can so what percent of the work on the dota 2 project would you guys say was what people would really think of as like machine learning science versus engineering essentially like as as far as day-to-day work goes that this kind of work was almost non-existent. There was maybe a few personal weeks spent on that compared to personal spent engineering. And I think maybe placing some good bets was one of it. Good bets on the machine learning side? On the machine learning side, yeah. And they're often more about what not to do rather than what to do um so at the very beginning of the project we knew we wanted to solve a game a hard game we didn't know exactly which one we wanted to do because these are you know great test beds for pushing the limits of our algorithms and that one of the great things about it and just to be clear you guys are two of the key people the entire team was like 10 people about 10 people that's right and you know these things good test beds for algorithms see what the limits are to really push the limit of what's possible and you know for sure that when you've done it that you've done it it's a very binary testable and uh so actually the way that we selected the game uh was we went on on twitch and just looked down the list of most popular games in the world and starting, you know, number one is League of Legends. The thing about League of Legends is it doesn't run on Linux and it doesn't have a game API. And little things like that actually are the biggest barrier to making AI progress in a lot of ways. And so looking down the list, Dota actually was the first one that kind of had all the right properties. Runs on Linux, that it has a big community around replay parsing, that there's a built-in Lua API. It was actually, this API was meant for script, or for building mods rather than for building bots. And we were like, but we could probably use it to build bots. And one of the great things about Valve as a company is that they're very into having these open hackable games where people can go and do a bunch of custom things. And so kind of philosophically, it was very much the right kind of company to be working with. So we actually did this initial selection back in November. And we were working on some other projects at the time. And so it didn't really get started until late in December. November, and we were working on some other projects at the time, and so it didn't really get started until late in December. And one of the funny things is that by total coincidence, in mid-December, Valve released a new bot-focused API, and that they were saying, hey, our bots are famously bad, that maybe the community can solve this problem, so we'll actually build an API specific for people to do this. And that was just one of those coincidences of the universe that just worked out extremely well. So we were kind of in close contact with the developer of this API and kind of all throughout. So at the very beginning of the project, well, what are you going to do, right? And so the first thing was we had to become very familiar with this game API to make sure we understood all the little semantics and all of the different corner cases. And also to make sure that we could run this thing on a large scale and to turn it into a pleasant development environment. And so at the time, it was just two of us. One person was working with the bot API, building a scripted bot. And so basically, this is the learn all the game rules, think really hard about how it works. rules, think really hard about how it works. The particular person who wrote it, Rofl, has played about three or four games of Dota in his life, but he's watched over a thousand hours of Dota gameplay and has now written the best Dota scripted bot in the world. And so that, a lot of just writing this thing in Lua, getting very intimately familiar with all those details. In the meanwhile, what I was working on was trying to figure out how do you turn this thing into a Docker container? And so they had this whole build process. It turns out that Steam can only be in offline mode for two weeks at a time, that they push new patches all the time. So you needed to go from this sort of manually download the game and whatever to actually have an automated repeatable process. It turns out that the full game files are about 17 gigabytes and that our Docker registry can only support five gigabyte layers. And so I had to write a thing to chunk up things into five gigabyte tarballs and put those in S3 and slurp them back down. So a bunch of sort of like things there where it's really just about figuring out what the right workflow is, what the right abstractions are. And then the next step was, well, we know we want to be writing our bots in TensorFlow and Python. How do you get that? So why was that? Well, because so machine learning, you know, that it's actually quite interesting that a lot of the highest order bit on progress, just like having the game API is a higher order bit. having the game API is a higher bit. It's also, can you use tools that are familiar and sort of easy to iterate with before the world of kind of modern machine learning frameworks ever write their code in MATLAB? If you had a new idea, it'd take you two months to do it. Like good luck making progress. And so it was really all about iteration speed. And so if you can get into the Python world, well, we have these large code bases that we built up of high quality algorithms that there's just so much tooling built around it that that's like the optimal experience. And so the next step was to port the scripted bot into Python. And so the way I did that was I literally just renamed all of the .lua files to .py, commented out the code, and then started uncommenting function by function. And then you run the function, you get an exception you then like go and uncomment whatever code it depends on and as mechanically as possible i tried to be like a human transpiler and uh you know that lua is one index python zero index you have to do that that lua has uh doesn't distinguish between an array type and a dictionary type and so you kind of have to disambiguate those two but for the most part i did something that could have been like sort of totally mechanically done. And it's great because I didn't have to understand any of the game logic. I didn't have to understand anything that was going on under the hood. I could basically just port over and it just kind of came together. But then you end up with a small set of functions that you do not have implementations of, which are all of the actual API calls. And so I ended up with a file with a bunch of dummy calls, and I knew exactly which calls I needed, and then implemented on top of gRPC a protobuf-based protocol where on every tick the game would dump the full game state, send the diff over the wire, reassemble that into an in-memory state object in Python, and then all of these API methods would be implemented in Python. And so at the end of this, it sounds like a bit of a Frankenstein process, but it actually worked really well. And in the end, we had something that looked just like a typical OpenAI GIM environment. And so all you have to do is you say, GIM.make this Dota environment ID, and suddenly you're playing Dota, and your Python code just has to call into some object that implements the Lua API. And suddenly these characters are running around the screen doing what you want. And so this was like a lot of the kind of thing that I was working on in the engineering side. And actually, as time went on, so Shimon and Jakob and Jay and others joined the project. And most people were building on top of this API and really didn't have to dig into any of the underlying implementation details. So personally, my one machine learning contribution to the project, I'll tell you about that because my background is primarily startup engineering, building large infrastructure, not sort of machine learning, definitely not a machine learning PhD. I didn't even finish college. large infrastructure, not sort of machine learning, definitely not a machine learning PhD. I didn't even finish college. So I kind of reached a point where I'd gotten the infrastructure into a pretty stable point that I felt like, all right, I don't have to be fighting the fires there very constantly. I have some time to actually focus on digging some of the machine learning. One particular piece that we were interested in doing was behavioral cloning. So we had one of the systems that we had built was to go and download all of the replays that are published each day. And so the way this game works is that there's about 1.5 million replays that are available for public download. Valve clears them out after two weeks. And so you have to have some discovery process. You have to stick them in S3 somewhere. Originally, we were downloading all of them every day and realized that was about two terabytes worth of data a day. That adds up quite quickly. So we ended up filtering down to the sort of most expert players. But we wanted to actually take this data, parse it and use it to clone the behavior for a bot. And so I spent a lot of time with like sort of, you know, it's basically this, you need this whole pipeline to download the replays, to parse them, to, you know, kind of iterate on that, to then take it, train a model, and try to predict what the behavior would be. And, you know, at first, it's just like, like one thing I find very interesting is the sort of different workflow that you end up with when doing machine learning. Like there are a bunch of things where when software engineers join OpenAI that are just very surprising. For example, if you look at a typical research workflow, you'll see a lot of files named like, you know, experiment, whatever the name of the experiment is, one, two, three, four, and you look at them and they're just like slight forks of the same thing. And you're like, is this what version control is for? all this for? Why are you doing that? And after doing this cloning project, I learned exactly why. Because the thing is, if you have a new idea for, okay, well, I've kind of got this thing working and now I'm going to try something slightly different. As you're doing the new thing, well, machine learning is to some extent very binary. At the start, it just doesn't work at all and you don't know why. Or it kind of works, but it has some weird performance and you're not sure exactly is it a bug is it just how this data set works like you just don't know and so if you've gotten it working at all and then you make a change you're always going to want to go back and kind of compare to the previous thing you had running and so you actually do want the new thing running side by side with the old thing and if you're constantly stashing and unstashing and checking out and whatever then you're just going to be sad and and uh there are a lot of like kind of workflow issues like that that you just you got to bang your head against the wall and then you see like i've been enlightened so but before we progress further on the story um can you just explain the basics of training a bot in a game like how are you actually giving it the feedback oh um yeah so so on a high level we are using uh reinforcement learning with self-play. So what that means, I mean, it's not rocket science, even though reinforcement learning sounds all fancy. Essentially what's happening is we have a bot which observes some state in the environment and performs some actions based on that state. environment and perform some actions based on that state. And based on those actions that it executes, then it continues playing and eventually either does well or poorly. So that's something that we can quantify in a number and that's more of an engineering problem than research problem, how to quantify how good the bot is doing. You need to come up with some metric. And then the bot gets feedback on whether it's doing good or not and then tries to select the actions that yield to high that positive feedback to high reward and and to give us a sense for how well that works so the bot plays against itself to get better uh once you had everything working um how good would a bot from day n do against a bot from day n minus one? So I guess we have a story that kind of illustrates what to expect from those techniques. So when we started this project, our goal wasn't to really do research. I mean, at some high level it was, but we were very goal-oriented. All we wanted to do is we wanted to solve problem. We wanted to solve .fiv5 and we had our my-solve.onv1. And the way it started, it was like early days when there was just Greg and Rafał and Rafał was implementing scripted bots. So he just literally write the logic. I think this is what bots should do. When he sees a creep, he needs to attack it, yada yada. And he spent like three months of his time. And Raffo is actually a really good engineer, so we had a really good scripted bot. So what happened then, you know, we kind of like, he got it to the point of which he didn't think he could improve it much more, so we tried, okay, let's try some reinforcement learning. And, you know, I was actually on vacation at the time, but there was another engineer, Jakub, who, throughout my vacation, which I found super surprising, said, I leave, there is nothing. I come back, there is this reinforcement learning bot. And actually, it's beating our scripted bot after, like, a week worth of engineering effort. Possibly it was two weeks, but it was something very miniature compared to the development of scripted bots. So actually our bot, which didn't have any assumptions baked about the game, figured out the underlying game structure well enough to beat anything that we could code by hand, which was pretty amazing to see. And at what point do you decide to compete in the tournament? Well, so maybe I should finish up my story. Sorry if it's running a bit long, but it'll get good shortly so just finish up my machine learning contribution so I basically spent about a month really learning the workflow got something that like you know was able to do some signs of life where it like run to the middle and you're like oh it knows what it's doing it's so good and it's very clear like when you're just doing cloning that like these these algorithms like learn to imitate what it sees rather than the actual intent and so it'd get kind of confusing and like kind of run you know try to try to do some some like creep blocking or something but like the creeps wouldn't be around and so it'd just be like zigzagging back and forth um and anyway i got this to the point where it was actually creep blocking pretty reliably, pretty well. And then at that point, I turned it over to Jay, who's also working on the project, and he used reinforcement learning to fine tune that. And so suddenly it went from only understanding the actions rather than the intent to suddenly it really knew what it was doing and kind of has the best creep block that anyone has seen. And so that was my one machine learning contribution to the project. So time went on, and one of the most important parts of the project was having a scoreboard. So we had a metric on the wall, which was the true skill of our best bot. So true skill is basically like an evil rating that measures the win rate of your bot versus others. And you put that on the wall and each week people just try all the ideas and some of them work, some of them improve the performance. And that we actually ended up with this very smooth, almost linear curve. So we posted in a blog post and that that really means kind of like exponential increase in the strength of this bot over time. And that part of that is, you know, sometimes these data points where you just train the same experiment for longer, you know, typically our experiments are less for maybe up to two weeks. But also a lot of those were, well, we had a new idea. We tried something else. We made this tweak. We added this feature, removed this other component that wasn't necessary. And so we chose the goal of 1v1. I don't recall exactly when, but it must have been in the spring or maybe even early summer. But we really didn't know, are we actually going to be able to make it? And unlike normally when you're building an engineering system, you think really hard about all the components. It's like, well, you decompose it into this subsystem, that subsystem, that subsystem, and you can measure your progress as what percent of the components are built. Here, you really have ideas that you need to try out and that it's sort of unpredictable in some sense. And actually one of the most important changes to the project in terms of making progress was initially the way that the project management was happening was that each week, well, so we'd written down our milestones of let's beat this person by this date, let's beat this other person by this date, let's be able to do kind of these outcome-based milestones on kind of a weekly or biweekly basis. That those things would come and go and you wouldn't have them. And then what are you supposed to do? It's completely unactionable, right? It's not like there was anything else you could have done. It's just you have more ideas you need to try. And instead of shifting it to what are all the things we're going to try by next week? That's a good insight. And then you do that and then yeah if you didn't actually do everything you said you were going to do then you should feel bad and then you should do more of it and if you did all those and like it didn't work then you know fair enough but you achieved what you wanted to achieve and so um even going into uh the international so uh two weeks before the international was kind of our cutoff for uh at this point, there's not much more we can do. We're going to do our biggest experiment ever, put all of our compute into one basket, and see where it goes. And at that point, like at two weeks out, how good was the bot? Oh, it was barely sometimes winning with professionals that we had testing. But not even always? No, always no no no it sometimes happened So so yeah, so to be to be yet so to be specific I'm just pulling this back in so July 8th is when we had our first win against our semi protester and then a sequence of losses And then we were kind of more consistent with it. And then he went on vacation. And so he was on some laptop somewhere that was not very good. And then we were consistently beating him. But that was not very reliable data. This was the week before the international. And so we didn't really know how good we were getting. We knew that the true skill was going up. When was the last time that an OpenAI employee beat the bot? an OpenAI employee beat the bot? How far out was that from the process? I think like a month or two before TI, although, yeah. We're not very good at Dota. Okay, but so like a month or two out, it could beat all the OpenAI people. Two weeks out, it could one time beat a semi-pro. I'd say so four weeks was the first time that it beat the semi-pro. Okay. And then two weeks out, we don't know. We still can't really find out. I mean, I guess we could rerun that bot, but we really didn't know how good it was. At that time, we just knew, hey, we're able to beat our SemiPro occasionally. And we, going into the International, figured that, hey, there's a 50-50 shot shot and i think we were telling sam the whole way like uh you know the probability with these things you never really trust the probabilities you just trust the trend of the probabilities even that was just swinging wildly you guys would text me every night it would be oh we're gonna you know no chance we're not going oh we're definitely gonna win every game yep yeah yep and so it was very clear that our own estimates of of what was going to happen were were miscalibrated. And throughout the week of TI, actually, we still didn't know. And what was happening is we... You guys all went to Seattle for this week? Most of the team went there. Okay. So you were like holed up in a hotel or a conference center or something in Seattle? Well, actually, the reality of it was that we were holed up like near the stadium when it was happening. were holed up like uh near near the stadium when it was happening let me describe how we were holed up so we we were given a locker room in the basement of key arena uh so we all had production badges and so you feel very special as you walk in you're just like oh yeah you know like i just i just get to you know kind of go skip the line and uh and go to the backstage area uh but it was literally a locker room that converted into a filming area. And we all had our laptops in there and that they would also bring in pro players every so often. We had a whole filming setup and then we'd play against the pros. And we had a partition that we set up, which was just like a black cloth, basically, between like the whole team sitting there being like, are we going to be able to beat this pro? Maybe basically, between the whole team sitting there being like, are we going to be able to beat this pro, maybe? And trying to be as quiet as possible. And these pros who were playing. And on Monday, they brought three, or I think two pros and one very high-ranked analyst by. And we had our first game. And we really didn't know what was going to happen. And beat this person three three zero and uh you know this was actually a very exciting thing for everyone at opening eye where uh so at the time what i was i was kind of live uh uh live slacking uh the updates as the game is like this person you know just said this and like i'm with that and like you know now it's this many last hits and yeah were you winning by a large margin so yeah do you remember the details of that one uh which game specifically? This was Blitz. Oh, Blitz. I think we won every game. Yeah, we did 3-0. I don't know exactly what the margin was. We have all the data. But Valve brought in the second pro, this professional named Pycat. And he played the bot. And we beat him once. We beat him twice. And then he beat us. Oh, okay. And looking Oh, okay. And that looking at the game, we knew exactly what had happened. Yeah, essentially what happened is he accumulated a bunch of want charges, right? This item that accumulates charges. And he accumulated more charges than our bot has ever seen in the game. Because just our bots don't... It turns out that there was a small... I think it's safe to say kind of a bug in a setup. In Dota? In our setup. Oh, OK. So basically, it passed some threshold that your bot was not ready for. Well, I'd say very specifically, kind of the root cause here was that he had gone for an item, an early wand build. And we had just never done item early wand build. And so it's just like our bot had just never seen this particular item build before. And so it had never had a chance to really explore what does it mean? And so it had never learned to save up stick charges and to use them and whatever. And so that it would do is very good at calculating like who's going to win a fight. But because, and Pyke got kind of recognized that he he's like i wonder what happens if i push on this axis and sure enough it was an axis the bot hadn't seen um and so then we played a third we played a third match um against another pro went three zero on that and it's actually very interesting getting the pros reactions because we also didn't really know are they gonna have fun like it's gonna be cool i think you're gonna hate it yeah and we got we got a mix of reactions you know of the pros were like, this is the coolest thing ever. I want to learn more about it. One of the pros was like, this thing's stupid. I would never use it. But apparently, after the pros left that night, they spent four hours just talking about the bot and kind of what it meant. Yeah, and the players were highly emotional in their reactions to the bot. They were never beaten by the computer. So it's kind of unbelievable. So for example, one of the players actually managed to eventually beat the bot. He was like, okay, this bot is effing useless. Like, I never want to see it. And then he kind of called down and like after like five or ten minutes, he's like, okay, this is actually great. This is going to improve my practice a lot. And so after your bot uh it lost that first time did they start talking about counterintuitive strategies to beat it uh well so so i think at that point um that you know i think that well actually i don't know maybe you can you can answer that particular uh yeah so i i don't think pro players are interested in that the pro players are mostly interested in the aspect where they it lets them get better at the game, which means that... But there was a point after the event where we set up this big LAN party where we had like 50 computers running the bot, we kind of unleashed this swarm of humans at our bot, and they found all the exploits. And we kind of expected them to be there because the bot can only learn as well as the environment in which it plays allows it to. So there are some things that it just never seen and of course those ones will be exploitable. And we are kind of excited about our next step, which is 5v5, because 5v5 is one giant exploit like essentially it's it's about like exploiting the other team like being where they don't expect you to be kind of like doing other distribution things so so so naturally we know we will have to solve those problems head on for 5v5 right so one one thing i think was pretty interesting about the training process is that a lot of our job while we were doing this was seeing what the exploits were and then making a small tweak that fixes them and like the way that that i now think about machine learning systems is that they're they're really a way to make the leverage of human programmers go way up okay right because again normally like when you're building a system you build component one component two component three and that kind of your marginal return on like building component four is like, you know, similar to your marginal return on component one. Whereas here, a lot of the early stuff that we did is just like your thing goes from being like crappy to like slightly less crappy. But once we were at the international and we had this loss to PyCat, we knew, okay, well, the root cause here is just it's never seen this item build before. root cause here is just it's never seen this item build before well all we had to do was make it tweak to add that to our list of item builds and then it played out this scenario for the next you know however long can you walk me through actually how that tweak works on the technical side because my impression is uh kind of what you guys have been saying it's just been in a million games so it kind of has learned all this stuff and uh some people talk about you know these networks it's just very gray and they don't actually know how to manipulate what how are you guys getting in there and changing things yes so so it's kind of funny in some sense on a high level you can uh compare this person to teaching a human like you know you know like kind of you see a kid doing maths and it's confusing addition with subtraction, suppose, right? And you're like, look here, this symbol. This is what you're not seeing clearly, right? And the same with those tweaks to our bot. So clearly our bot has never seen this want build that Greg mentioned and all we had to do is we had to say that when the bot plays games and chooses what items to purchase, we just need to add some probability of sampling that specific build that it has never seen. And when it plays a couple of games against opponents that use that build, when it uses this build a couple of times itself, then it kind of becomes more comfortable with the idea of what happens, what are the in-game consequences of that build. Okay. And kind of at a, so I have kind of a couple different levels to that answer that I think are pretty interesting. So one is at a, you know, at a very like kind of object level. So the way that these models work is you basically do have a black box which takes in some list of numbers and outputs a list of numbers. And it's very smart in how it does that mapping, but that's what you get. And then you think of this as this is my primitive. Now, what do I build on top of that so that as little work as possible has to be done inside of the learning here? And a lot of your job is, well, one thing that we noticed that we'd forgotten as well on Monday was, well, it wasn't that we'd forgotten, we just hadn't gotten around to it, was the passing in data that corresponds to the person was passing in the visibility of a teleport. So as a human, you can see when someone's teleporting out, our bot just did not have that feature. The list of numbers passed in did not have that feature. And so, well, one of the things you need to do is you need to add it. And so that kind of goes from your feature is, you know, your feature vectors, however long it was, and now it's got one more, one more feature on it. And the bot wasn't recognizing that as an onscreen thing. So it doesn't see the screen it's passed data from the bot api oh okay and so it really is given whatever data we give it okay and so it's kind of on us to do some of this feature engineering and you want to do as much as you can to make it as easy as possible so that it has to do as little work inside as possible so it can spend you know you think of it as you've got some fixed capacity do you want to spend that on learning the strategy do you want to spend it on learning how to like, you know, map, you know, choose which creep you want to hit? Like, do you want to spend that on trying to parse pixels? Like, you know, that, that at the end of the day that I think a lot of our job as the, the system designers here is to push as much of that model capacity, as much of the learning towards the like interesting parts of the problem that you can't script, that you can't possibly do any processing for. And so that's kind of one level is that a lot of the work ends up being identifying which features aren't there or kind of engineering the observation and action spaces in an appropriate way. Another thing is, I think is like another level where you zoom out is like the way that this actually happened was, so you know we're there on monday and people got dinner and then shimon and yakup and raffle and uh and saho and uh i think you know maybe one or two others uh stayed up uh all night uh to do surgery on our running experiment okay um and so it was very much like a you've got your production outage and like everyone's there like all hands on deck trying to go and and to go and make the improvements. Specifically, to zoom in and give you a bit of a feel, what it felt like working on the bot. This was a very tiring week. Every day, the day was just meeting with the pros and watching our bot getting excited. And the nights were coding up the next version of experiment because actually it's a little known fact but from day to day like each version of the experiment was not good enough to beat the next player next next day's professional so so just that morning we would download the new parameters of the network and it would be good enough to beat it but the day before it wasn't how are you discerning that well this was this was again something of almost a coincidence i mean yeah there might be something a little bit deeper but so the you know kind of the full full story of the week was we did the monday play okay and that you know there we lost to piecat um and so just to just to clarify are you guys in the competition or not in the competition so the the thing that we did was we did a special event to play against Dendi, who's one of the best players of all time. And while we were there, we were also like, well, let's test this out against all these other pros. Because they're physically here right now. And let's see how we do. Got it. All right. So Monday happens. You start training it. Yep. And so actually, yeah, so this experiment we've kicked off maybe sometime the prior week. we've kicked off uh you know maybe sometime the prior week yep and weeks before i think yes something like that and uh we've been running this experiment for a while and our infrastructure is really meant for you run an experiment from scratch you know you start from complete randomness and then and then you run it and then you you two weeks later go and see how it does we didn't have two weeks anymore and so we had to do this surgery and it's very careful like you know read every single character of your commit to make sure that you're not going to have any bugs because if you mess it up we're out of time there's nothing you can do and it's not one of those things like if you're just a little bit more clever that you can you know go and do a hot patch and have everything be good it's just literally the case that you gotta let this thing sit here it's got to bake yeah and uh so monday came and went we were running this this this experiment uh that we perform surgery on and uh uh the next day we got a little bit of reprieve where we just played against some some kind of lower ranked players who you know are kind of commentators and popular in the community but you know we're not pushing the limit of our bot um on wednesday uh at 1 p.m our contact from from valve came by and said hey i'm gonna get you rtz and sumail um who are basically you know the top players in the world um and i i was like could we push them off to thursday maybe and uh he was like their schedule is booked uh you're gonna get them when you get them and uh that we're gonna we're scheduled to get them at 4 p.m okay so we looked at our bot to see how it was doing and we kind of been along the way gauging it we tested it against our semi-pro player and he said this brought bot is completely broken oh no and you know kind of pictures of maybe we had a bug during the surgery like went through our head and he showed us the issue and he said look first wave this bot takes a bunch of damage it doesn't have to take there's no advantage to that i'm gonna run and i'm gonna go kill it i'll show you how easy it is he ran in to kill it and he lost okay and then again and but don't jump ahead explain what happened so so he played it five times and he lost each time until he finally did figure out how to exploit it and we realized what was going on was that this bot had learned a strategy of baiting. You pretend to be a really dumb bot. You don't know what you're doing. And then when the person comes in to kill you, you just turn around and you go super bot. It was legitimately a bad strategy, you know, if you're really, really good. But I guess it was good against the whole population of bots that it was playing against. And you had never seen it until that day. So, yeah, we had not seen that behavior. And we did not at all expect that it was like one of the major examples of the things that we kind of didn't have explicit incentive for and yet the bot actually learned them. And, yeah, essentially, I mean, it was kind of funny because of course when the bot played against its other versions, it was just like good baiting strategy that was kind of out of distribution, but it had very interesting psychological effect on humans because optimal strategy was not to fall for the bait, it was kind of to wait it out a little bit because the bot already is at a disadvantage. But he's like, okay, look how stupid this bot is, I'm gonna go for a kill. So it kind of had interesting psychological effect on humans, which I thought was like, it's kind of funny. It almost knows it's a bot. Yeah, it knows how it's attacked. Yeah, it's funny to see a bot which kind of seems like it's playing with emotions of of the you have like of course it was not what actually happened but it seemed this way so now so now we were faced with a dilemma it's 1 p.m on wednesday that these best players are going to be showing up at 4 p.m we have a broken bot what are we going to do and we know that our monday bot is not going to be good enough we know it's not going to cut it. And so the first thing we do is we're like, well, Monday bot, it is pretty good at the first wave. This new bot is a super bot thereafter. So can we stitch the two together? So we had some code for doing something similar. So we kind of revived that. And then in the three hours, Jay spent his time doing a very careful stitch where you run the first bot and then you cut over at the right time to the second bot and and this is literally just like bot one plays the first x amount of time literally just that and he finished it 20 minutes before the course before the thing we ran it by our semi-pro semi pros like this is great so we at least we got that done in the nick of time but the other question was how do we actually fix this bot? I mean, actually, just to finish your story, there's one aspect. Because we are also kind of uncertain what happens when you switch over from one bot to the other. So I was actually standing by the pro who was playing it. And I was looking at the timer at the moment when it was switching. I was like, . Distract the guy in case something's just trying to distract him and of course it was probably completely unnecessary but uh but we weren't sure what happened there so i didn't know about that part of the story so the the question of how do you actually fix it so there was a little bit of debate of like maybe we should abandon ship on this switch back to our old experiment run that one for longer um and i forget i forget who suggested it but um someone was like i think we just have to let it run for longer because you learn a strategy of baiting well the counter strategy for that is just don't bait play well the whole time and so we got that run for the additional three hours and so we first played our tz uh who showed up on on our our switch bot you know kind of the the frankenbot and uh you know that beat him three times all right all right let's try out this other bot and just see uh what happened with the additional three hours of training um because you know our semi-protester at least validated that like it looks like it's fixed and so in that three hours of training how many games is it actually playing simultaneously it's a good question quite a bit yeah okay and uh uh and so we uh we played this new bot against rtz didn't know how it was going to do and sure enough it beats him and uh and he he loved it he was having a lot of fun he ended up playing 11 games that day uh and uh or maybe it was 10 but i i think that he was just like oh this is so cool um we were supposed to have sumail that day as well but due to a scheduling snafu he had to be at some panel and so like timing didn't work out um but artesi and his coach who also coaches uh sumail both said yeah both said sumail is going to beat this bot like it's going to happen you know maybe he'll have a little bit trouble to figure it out for the first game but like after that you're in trouble. And so you're like, all right, we've got one more day to figure out what to do. And so what did we do? I don't know. I just kind of like some nice dinner. Kind of what did you say? Kind of went for some nice dinner. We kind of rested a lot. We kind of chatted, slacked with some people at home. And, in the morning we download the new parameters of the network and just let it play. So we literally just hung out and just let it go. Just let it play. It's the exact opposite of how I'm used to engineering deadlines happening. Yeah. Normally it's your work right up until the minute. So you guys weren't like, you guys were getting like full nights of sleep, nice and relaxed. Oh no, no, absolutely not. Okay. So, so to make this clear, the night before the night, before the, like two nights before the day where we got the rest and relaxation, the night looked something like the following. We had full day of dealing with the problems and kind of like emotional highs and so on. So it's absolutely knackered. Come midnight, we start working. Okay. We need to make all those changes. Like the one thing that we talked about around midnight, we start with four people. And we are also tired that like, you know, we look through all the commits that we are going to add to the experiments. There were actually two people looking at them because we didn't trust a single person given how tired we all were. So they were like looking at those coins till 6am. I was updating the model, which is a lot of nasty off by one indexing things. So even though it was a short call, it took me like six hours to do. Somewhere around 3am we had a phone call with Azure. Because it turns out that with certain number of machines, you start exceeding some limits, so we tried to make them raise the limits. And around 6am we were okay, we were ready to deploy this. And then there was... Deploying is just like a one-man job, so Jakub was just like, you know, hitting, like, you know, clicking the deploy and kind of fixing all the issues that came up. I was staying around just exclusively to make sure that Jakub doesn't fall asleep. And eventually at 11 a.m. the experiment was running and we kind of went to sleep, like woke up at 4 p.m. or something. And then it was like all so it had it had over 24 hours to train uh i think i think i ended up being like one and a half day until the game of samaya sorry yeah just just to repeat the timeline so this was monday it was when we played first set of games had the loss did the surgery that night you know played it i guess starting at 11 on tuesday then yeah wednesday 4 p.m is when we played our tz and then trained for longer i don't think we made any changes after that maybe we made some small ones uh but um then on thursday is when we played sumail okay and so i think tuesday to wednesday was the night where we made last changes yeah and uh there was there was like quite quite a bit like it's it changes. And there was quite a bit of different work going on that all kind of came together at once. Like one thing I think is really important was, so one of our team members, his handle is Sihow, who's a very well known program competition competitor, was spending a lot of time just watching the bot play and seeing why does it do this weird thing in this case? You know, what are all the like, you know, weird tweaks and really getting intuitions for, oh, because we're representing this feature in this way. And so if we change it to this other thing, then it's going to work in a different way. And I think that like this really trying to, it's almost this very human-like process of watching this like expert playing the game and try to figure out what are all the little micro decisions that are going into this macro choice. And it's kind of interesting starting to have this very different relationship to the system you build. Because normally the way that you do it is, well, your goal is to have everything be very observable. And so, yeah, you want to put metrics on everything. And like, you know, if something's not understandable add more logging like you know that that's that's how you design the systems whereas here you have this you know you do have that for the surrounding bits but for this machine learning core that there you really do have to understand it at more of a of a sort of behavioral level um was it ever stumping you where you're just like it's being creative in a way that we didn't expect it to and it may be even working but you don't know why or how it decided to make that choice yeah i think the debating story that we're debating is the main one is the main story like this we kind of few small ones like where you know there's like some early days of the project where where like professionals playing the next edition of the bot and he's like, hmm, your bot is really good at crippling. And we are like, oh, what is crippling? I'd say that there is also one other part of the story that I think is interesting, and then I think probably we can wrap up this part. So to see how well, you know because our semi-pro tester had played hundreds of games against this bot over the past you know a couple months and so we wanted to see just how does he benchmark relative to artesi um and so we had him play against artesi and you know artesi was up the whole game it was just like beating him to the last hit by like 500 milliseconds every single time and so our semi-pro was like all right i've got one last ditch effort to go try this strategy that the bot always does to me and it's like some some you know strategy where you like do something complicated and then you like triple wave the the the your opponent you get them out of the tower you have regen you go and you go in for the kill um and he did it and it worked oh and this was the the the bot had like taught him the strategy that you could use against a human uh and i think that was like very interesting and a good example of the kinds of things you can get out of these systems that they can discover these very sort of you know non-obvious strategies that can actually be taught to humans and how did it go with smell uh so with smell we we we went undefeated i i and i think it was-0 that day. One thing that's actually interesting, so we'll probably blog about this in upcoming weeks, but we've actually been playing against a bunch of pros since then. So our bot has been in very high demand and some of these pros have been live streaming it. So we've gotten a better sense of watching as humans go from just being completely unable to beat it to if you play against it for long enough, you can actually get pretty good. And so there's actually a very interesting set of stats there that we'll be kind of pulling and analyzing in a bit. Are there humans that consistently beat the bot today? Yeah, so I think there's one who has like a 20% win rate or something. I think it might be actually 30. And that player played hundreds of games. And just finds strategies to exploit? No, actually. He becomes essentially as good as the bot at what the bot is doing. Which I find extremely surprising, but it turns out that he played hundreds of games with it. And is he a top player? Like, does he beat most humans? These are all professionals. Okay. It's not just some random kid who's good at beating the bot. That's right. The way to think about this is that, like, yeah, I mean, being a professional video game player is a pretty high bar. I think everyone wants to be a professional video game player who play these games and the number of pros is very small. And there are some who have really like, when you're playing hundreds of games against it, you're going to get very, very good at the things that it does. And so talking to Arteezy, I was asking him, has it changed your play style at all? And he said, he thinks that the thing that it's done for him is it's helped him focus more. Because while you're just there in lane last hitting, now suddenly like that's just so rote right because you just have been doing it so much you've gotten so good at it and i think that one really interesting thing to see is going to be how can you improve can you improve human play style can you change human play style and uh i think that we're starting to see some positive answers in that direction so i know we we're almost out of time. I could do a little lightning round, just quickly go through some of these questions, guys. Actually, to the question of what kind of skills you need to work at OpenAI, could we have a very small... That was going to be the first lightning round question. So a specific list of things that we found very useful, at least in the Dota team, is some knowledge of distributed systems, because we build a lot of those and those are easy to not do properly. And another thing that we found very important is actually writing bug-free code. Essentially, I know it's kind of taken for granted in computer science community that like everybody makes bugs and so on, but here it's even more important than other projects that you minimize it because they're very hard to debug. Specifically, many bugs manifest in kind of lower training performance where to get that number it takes a day and in like a spree of hundreds of comments it's really easy to miss uh so and primary way of debugging this is actually reading the code so so every bug has a very high cost associated with it so so actually writing like this correct bug free code is quite important to us. And we sometimes actually kind of sacrifice good engineering habits, good good kind of code modularity to make our code shorter and simpler and kind of having less, essentially less lines where you can make bugs. essentially less lines where you can make bugs. And I guess lastly, as we mentioned, primary skill is good engineering, but if somebody really feels like, gosh, I really need to brush up my maths, I really need to kind of go in there and feel comfortable, like not have somebody ask me question about maths and then I think mostly getting good basics in linear algebra and in basic statistics stats, especially when doing experiments, it's easy to make elementary statistics mistakes. And linear algebra is just kind of most of what you need to know to, like basic optimization as well, to follow what's happening in those models. But this is kind of, compared to being a good engineer, quite easy to pick up, at least in a project like the one we are doing. Yeah, so I wanted to talk about some non-technical skills that i think are are really important um so one is uh that i think that there's like a real humility that's required if you're coming from an engineering background like i am i and working working in these these projects where you're you're no longer the technical expert in the way that you're used to right and i think that you know if you go and you build you talk to you like, you know, let's say you want to build a product for doctors. Right. I think you can talk to 10 doctors and honestly, whatever thing you're going to build is probably going to be a pretty valuable addition to their workflow because doctors can't really build their own software tools. You know, some can, but, you know, as a general rule, no. Whereas with machine learning research, you know, everyone that you're working with is very very technical, can build their own tools. But if you inject engineering discipline in the right place, if you build the right tool at the right time, if you kind of look at the workflow and think about, oh, we could do it in this other way, that's where you can really add a bunch of value. And so I think it's about knowing when to inject the engineering discipline, but also knowing when not to. And being, you know, to Shimon's point, sometimes, well, we really just want the really short code because we're really terrified of bugs and uh and so that can yield different choices than you might expect for something that's just a pure production system who writes the least bugs at all open ai oh that's a contentious question actually that's a good what's the question who writes the least bugs per line of code at all open ai i'm definitely not going to say me. Possibly Jakub. Yeah. It's hard to say. But it's, yeah, it's three hearts. It could be Greg. I read a lot of bugs. I caught Jakub the least amount of times on bugs, so. OK. It's the least. It's more OK to have bugs that are going to cause exceptions. Right. And my bugs usually cause exceptions. So that's fine. That's fine. Yeah. It's what you don't want is the things that cause correctness issues where it gets 10% worse. Yeah. So there was another question related to skills, but this is for non-technical people. Yep. So Tim Beko asks, how can non-technical people be helpful to AI startups? Well, I was going to say, I think one important thing is that for AI generally right now, I think there's a lot of noise and I think it can be hard to distinguish what is real from what's not. I think just like simply educating yourself, I think is like a pretty important thing. Like I think it's very clear that AI is going to have a pretty big impact. And, you know, that's just look at what's already been created and extrapolate that without any new technology development, any new research. And it's pretty clear that's going to be baked into lots of different systems. There are a lot of ethical issues to work through. And I think that being kind of a voice in those conversations and educating yourself, I think is like a really important thing. And then you look to, well, what are we going to be able to develop next and i think that that's where the really transformative stuff's going to come okay uh i once saw a post of greg's rescue time report and was pretty shocked do you have any advice for workers for working such long focused hours i think it's not a good goal i would not have a goal of trying to maximize the number of hours you sit at your computer. For me, I do it because I love it. And the activity that I love most in the world is when you're in the zone writing code, producing it for something that's meaningful and worthwhile. And so I think that as a second order effect, it can be good. But I wouldn't say that that is the way to have an impact. I will also say more specifically, the only way i've ever seen people be super productive is if they're doing something they love there is nothing else that will sustain you over a long enough period of time um okay is the term ai overused by many startups just to look good in the press yes indeed okay what is the last job that will remain as ai starts to do everything else the last human job like what is going to be the hardest thing for ai to do it's it's it is a hard question to answer in general um because i think it's actually not ai researcher the gay researcher will will kind of go before it's actually it's actually very interesting when you ask people this question i think that everyone tends to say whatever their job is as well as the hardest one um but i actually think that ai research is going to be one that you're going to want to make these systems very good at totally i think the last question maybe this is obvious is um can you just connect the dots between how playing video games is relevant to building agi yeah it's actually maybe one of the most surprising things to me, the degree to which games end up being used for AI research. And the real thing that you want, right, is you really want to have algorithms that are operating in complex environments where they can learn skills and that you want to increase the complexity of those skills that they learn. And that's either you push the environment, you push the complexity of the algorithms, you scale these things up, and that that's really the path that you want to take to building really powerful systems. So games are great because they are a pre-packaged environment that some other humans have spent time sort of making, first of all, putting in a lot of complexity, making sure that there's like actual intellectual things to solve there, or not even just intellectual, but you know, like interesting mechanical challenges that you kind of can get human level baselines on them. So you know exactly how hard they are, that they're very nice. Unlike, you know, something like robotics, where you can just run them entirely virtually. And that means you can scale them up and you can run many copies of them. And so they're a very convenient testbed. And I think what you're going to see is that there's a lot of work that's going to be done in games. But the goal is, of course, bring it out of the game and actually use it to solve problems in the real world and to actually be able to interact with humans and do useful things there. So I think they're a very good sort of starter and a very good place to like i think one thing that i really like about this dota project and bringing it to all these pros is that we're all going to be interacting with super advanced ai systems in the future and right now i think we don't really have good intuitions as to how they operate where they fail what it's like to interact with them and that this is a very low stakes way of having your first interaction with very advanced ai technology. Cool. If someone wants to get involved with OpenAI, what should they do? Well, we have a job posting at our website. I guess the tips that are given about how to get a job at OpenAI are very geared towards the specific job posting that we have there, which is a large-scale reinforcement learning engineer. there which is a large scattering force and learning engineer cool yeah and in general we look for people who are very good at whatever technical access they specialize in and we can use lots of different specialties great all right thanks guys just to echo that like everyone thinks they have to be an AI PhD not true neither of these guys are um all right thanks a lot cool thanks yeah thank you All right. Thanks a lot. Cool. Thanks. Thank you.