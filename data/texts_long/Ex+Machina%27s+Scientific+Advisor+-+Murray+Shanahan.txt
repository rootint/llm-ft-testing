 So I think the first question I wanted to ask you is that given the popularity of AI, or at least the interest in AI right now, what was it like when you were doing your PhD thesis in the 80s around AI? Yeah, well, very different. I mean, it's quite a surprise for me to find myself in this current position where everyone is interested in what I'm doing. The media are interested, you know, corporations are interested. Everyone is interested in what I'm doing. The media are interested. You know, corporations are interested. So certainly when I was a PhD student and when I was a young postdoc, it was a fairly niche area. So you could just kind of like beaver away in your little kind of corner doing things that you thought were intellectually interesting and being reasonably secure that you weren't going to be bothered by anybody. But it's not like that anymore. No. And so what exactly was the subject matter at the time? and being reasonably secure that you weren't going to be bothered by anybody, but not like, it's not like that anymore. No. And so what, what exactly was the subject matter at the time? What were you working on? Uh, at the time when I did my thesis? Yeah. Uh, uh, well, I worked on, um, uh, how you could use, oh, this is a, this use oh this is a this is a tricky question i know you're asking this is this is uh you're asking me to go back uh like let me think what is it like 30 something years yeah uh 30 something years yeah 30 years i finished 30 years ago i finished my thesis um okay so what did it look at so i was interested in logic programming uh and prologue type languages uh and i was interested in how you could speed up answering queries in prologue-like languages by keeping a kind of record of the thread of relationships between facts and theorems that you'd already established. So instead of having to redo all the computations from scratch, it kind of kept a little collection of the relationships between properties that you'd already worked out so that you didn't have to redo the same computations over again. So that was the main contribution of the thesis. I'm amazed I can remember anything about it. That's very impressive. I did my thesis like five years ago and I barely remember. And so did you pursue that further at Imperial? No, I didn't. I kind of, well, one other thing that I discussed in my thesis was I had a whole chapter on the frame problem. So the frame problem is, there are different ways of characterizing it, but the frame problem in its largest guise is all about how a thinking mechanism or thinking creature or a thinking machine, if you like, can work out what's relevant and what's not relevant to its ongoing cognitive processes and how it isn't overwhelmed by having to rule out just trivial things that aren't irrelevant. And so that comes up in a particular guise when you're using logic and when you're using logic to think about actions and their effects and and there you want to make sure that you don't have to spend a lot of time thinking about the non-effects of action so for example uh if i if i uh move around a bit of the equipment like your microphone here then the color of the walls doesn't change. And you don't want to have to explicitly kind of think about all those kinds of trivial things. So that's one aspect of the frame problem. But then more generally, it's all about sort of circumscribing what is relevant to your current situation and what you need to think about and what isn't. And so how did that translate to what folks are working on today? and what isn't. And so how did that translate to what folks are working on today? Well, so it's actually, so this thing, the frame problem has, has recurred throughout my career. So although there's been a lot of variation in what I've done, and I've, so I worked for a long time in classical artificial intelligence, which is there. It's all about, it was, and still is all about using logic-like or sentence-like representations of the world. And you have mechanisms for reasoning about those sentences and a rule-based approach. And so that approach of classical AI has fallen out of favor a little bit. And I sort of got a bit disillusioned with it back in, well, a long time ago. So by kind of the turn of the millennium, I'd more or less abandoned classical AI because I didn't think it was moving towards what we now call AGI, artificial general intelligence, the big vision of human level AI. And so I thought, well, I'm going to study the brain instead, because that's the example that we have of an intelligent thinking thing. It's the perfect example. So I want to try and understand the brain a bit more. So I started working on building computational neuroscience-style models of the brain and thinking about the brain from a larger kind of perspective and thinking about consciousness and the architecture of the brain and big questions and um and now i'm getting around to answering your question by the way eventually but now i'm interested in um in machine learning there's there's been this resurgence of interest in machine learning so i've kind of moved back to some of my interests in in artificial intelligence and i'm not thinking so much about the brain or neuroscience or that kind of empirical work right now. And I've gone back to some of the old themes that I was interested in in good old-fashioned AI, classical AI. So that's sort of an interesting trajectory. Actually, the frame problem, interestingly, has been a recurring theme throughout all of that stuff because it keeps on coming up in one guise or another. So in classical AI, there was the question of how can you write out a set of sentences that represent the world where you don't have to write out a load of sentences that encompass a lot of trivial things that are irrelevant. And somehow the brain seems to solve that as well. The brain seems to manage to focus and attend to only what's relevant to the current situation and ignore all of the rest. And in contemporary machine learning, there's also this kind of issue as well. There's also a challenge of being able to build systems, especially if you start to rehabilitate some of these ideas from symbolic AI. to rehabilitate some of these ideas from symbolic AI. You want to think about how you can build systems that focus on what's relevant in the current situation and ignore things that are not. For example, a lot of work here at DeepMind has been done with these Atari retro computer games. So if you think of a retro computer game like Space Invaders, then if you think about the little invader going across the screen, it doesn't really matter what colour it is. In fact, it doesn't really matter actually what shape it is either. What really matters is that it's kind of dropping bombs and you need to get out of the way of these things. So in a sense a really smart uh a really smart system would learn that it's not the color that matters it's not the shape that matters it's these little shapes that fall out of the thing that matter and and so that's all that's all about kind of working out what's relevant and what's not relevant to the to to to solving getting a good score in the game. Sorry for the interruption, everyone. We just got to see Garry Kasparov talk. It's pretty amazing. Yeah, that was fantastic, wasn't it? Yeah. So Garry Kasparov in conversation with Demis Hassabis. Yeah, he gave a great talk about the history of computer chess and his famous match with Deep Blue. So yeah, we just had to pop upstairs to watch that, right? So now it's part two. Yeah, kind of one of those once-in-a-lifetime things. It also seems like he got out at the exact right time. Yeah, maybe. Yes, he did. Yeah, yeah. So Demis, at the beginning of the interview, said that he thought that he was perhaps the greatest chess player of all time. And so he was there just at the right time to be knocked out by a computer in a way. Yeah, knocked off the top spot. Very cool. Yeah, and he also said that maybe accurately that any iPhone chess player now is probably better. Than Deep Blue was in 1997. Yeah, which is interesting yeah i also thought it was interesting that he was saying that just anybody in their living room now can sit and watch two grandmasters playing a uh playing a match and can use their computer to see as soon as they make a mistake and can analyze the match and can follow exactly what's going on yeah whereas in the past it took you know expert commentators sometimes days to figure out what was going on. Whereas in the past, it took expert commentators sometimes days to figure out what was going on when two great players were playing. So that was interesting. What struck me was how he was kind of analyzing the current players and how they relied so heavily on the computer, or at least he thinks they rely so heavily on the computer that they're kind of reshaping their mind. Right, yeah. And that certainly, I think, is going to be true with Go and with AlphaGo. So it's been interesting watching the reactions of the top Go players, like Lisa Doll and KJ, who are very positive, in a way, about the impact of computers on the game of Go. about the impact of computers on the game of Go. And they talk about how AlphaGo and programs like it can help them to explore parts of this universe of Go that they would never otherwise have been able to visit. And so it's really interesting to hear them speak that way. Yeah, it seems like they're going to open up just kind of new territories for new kinds of games to actually be created. indeed yeah well so we've already seen that with our with alpha go in the in the match with lisa doll um so uh as you probably know there was uh there was a famous move in the second match against lisa doll move 37 where all the commentators all these sort of nine dan masters were saying saying oh, this is a mistake. What's AlphaGo doing? And this is very strange. And then they sort of gradually came to realize that this was a sort of revolutionary kind of tactic to put the stone in that particular rank in that particular time in the game. And since then, the top Go players have been exploring this kind of play about moving into that sort of territory when the conventional wisdom was that you shouldn't. Yeah. I mean, the augmentation in general, I find fascinating across the board. Yeah. And I think he was hinting that as well. Yeah, he was. was very positive about the prospects of human-machine partnerships where humans provide maybe a creative element and machines can be more analytical and so on. What was that law that he mentioned? I forgot the name of it. I wrote it down. Oh, Moravec's law. Moravec's law. Yeah, named after Hans Moravec, the roboticist, who wrote some amazing books, including Mind Children. So he wrote this book called Mind Children. And this phrase, Mind Children, alludes to the possibility that we might create these artifacts that are like children of our mind and that they have sort of lives of their own and they are the children of our minds. You know, it's a challenging idea. This is an old book. I mean, from the late 80s okay do you buy it um uh maybe in the distant future okay well then maybe we ought to we ought to segue back into what we were talking about which is kind of related to your book your uh two books ago embodiment in the inner life yeah yeah which which came out in 2010 okay um because that was that was kind of an integral question to the movie ex machina right because you you didn't necessarily have to have a person like ai and more importantly you didn't have to have an ai that sort of looked like a person that sort of looked like an attractive female that also looked like a robot right they they teed up in the beginning Nathan tees it up in the beginning yeah yeah no I mean obviously um uh to a certain extent those are things that make for good film and so they're artistic choices and cinematographic choices and um and I mean in the film her we actually have of course of course a disembodied AI. And so it's possible to make a film out of disembodied artificial intelligence as well. But obviously a lot of the plot and what drives the plot forward in Ex Machina is to do with Ava's embodiment and the fact that Caleb is attracted to her and sympathizes and empathizes with her. But there's also kind of a philosophical side to it too, which is certainly I think that it, well, no doubt, when it comes to human intelligence and human consciousness, our physical embodiment is a huge part of that. It's where our intelligence originates from, because what our brains are really here to do is to help us to navigate and manipulate this complex world of objects in 3D space. And that is... So embodiment is an essential fact here. We've got these hands that we use to manipulate objects and we've got legs that enable us to move around in complicated spaces. So that's in a sense what our brains are originally for. The biological brain is there to make for smarter movement. And all of the rest of intelligence is a flowering out of that in a way and so did you buy the um the the gel that he showed caleb in the beginning oh yeah they so so that it's interesting because um the way the film is is constructed is that so alex garland you know the writer and director so he uh uh sometimes says that the film is set 10 minutes into the future it's just you know it's like a really meant to be like, really a lot like our world, just very slightly into the future. Yeah. And so when you see the, you know, Nathan's lair, his sort of retreat in the wilderness, there's nothing particularly science fiction-y about that. It's designer-ish. And of course it is in fact a real hotel, or a real you can actually go and stay in this place. Where is it? In Norway. And so it doesn't have a particularly futuristic feel. And almost everything you see is not very futuristic. It's not like Star Wars, but then there are a few things, a few carefully chosen things that look very futuristic and they're ava's body so the way you can see uh you know the sort of the insides of of her torso and um and her head and then when he shows um the the brain which is made of this uh this gel and so i i think that was a good choice because we don't at the moment know how to make things that are like ava that have that kind of level of artificial intelligence so that's the the point at which you have to go sci-fi really well i mean those like lifelike melding elements have you watched the new show at the hbo show is it westworld uh do you know i haven't no i mean i i yeah i really um it really is on my to watchwatch list, but I've heard a lot about it, yeah. Because they definitely... I remember the original with Yul Brynner, but I haven't watched the current series yet. Yeah, yeah. They definitely take cues. I mean, I guess it's probably like in the sci-fi canon that you have this basement layer where you create the robots and then they become lifelike through this whole process even if you just watch kind of a the opening title credits it's exactly that it's like the very the 3d printed sinews of the muscles it looks exactly like uh nathan's lair and so what i was wondering is um as you were consulting on the show how much of that were they asking you about and were they saying like, is this like remotely 10 minutes in the future or is this 50 years in the future? Well, it wasn't really like that, actually. I mean, I'll tell you the sort of whole story of how the kind of collaboration came about. So I got this email from Alex Garland, you know, unsolicited email out of the blue. It's the kind of unsolicited email you really want to get from, you know, famous writer, director who wants you to work on a science fiction film. And he basically said, oh, I read your book, Embodiment in the Inner Life. And it helped me to kind of crystallize some of the ideas around this script that I'm writing for a film about AI and consciousness. And, you know, do you want to get together and have a chat about it? So I didn't have to think very hard about that. And so we got together and had lunch and he sent me the script. And so I'd read through the script by the time I got to see him. And he really, he certainly wanted to know whether it sort of felt right from the standpoint of somebody working in the field um and um and i have to say it really did there was there was nothing i mean it was as a script it was a great page turner actually because it's it's interesting being in that position because now um Machina and the image of Ava has become kind of iconic. And, you know, you see it everywhere. But, of course, when I read the script, all of that imagery didn't exist. So I was reading it. I had to kind of conjure it up in my own head. So he didn't give you any kind of preview of what he was thinking aside from text? No, because nobody had been cast then, uh, at that point. And I think, uh, actually when we met up, uh, if I, if my memory serves me right, he did have a few, uh, he did have some images of some mock-ups from artists of, of, of what Ava might look like. Yeah. But I hadn't seen it when I'd read the script. So, so for me, it was just kind of the script and, and the characters really leapt off the page. The character of Nathan in particular was, was really very vivid and, and, you know, you really didn't like this guy, you know, just reading the script. Anyway, so, so then Alex really wanted to – so I sort of grabbed the title of scientific advisor. I'm not sure if I ever really was officially a scientific advisor. But Alex really wanted to meet up and talk about these ideas. He wanted to talk about consciousness and about AI. And so we met up several times during the course of the filming. And I think there's very little that I contributed to the film at that point. In a sense, perhaps I'd already done my main bit by writing the book. I mean, there were a few little phrases that I corrected, tiny, tiny things, but otherwise I just thought, you know, great. Really, really very good. And there's some lines in the film that I just thought, you know, great. You know, gosh, really, really very good. And there are some lines in the film that I just thought were so spot on. Anything you remember? Like what line in particular? Yeah, well, I mean, so a favorite one is where Caleb – so initially Caleb is told that he's there to be the human component in a cheering test. And, of course, it isn't the cheering test. And Caleb says that pretty quickly. He says, well, look, in the real Turing test, the judge doesn't see whether it's a human or a machine and so on. But of course, I can see that Ava's a robot. And Nathan says, oh, yeah, we're way past that. The whole point here is to show you that she's a robot and see if you still feel she has consciousness. And I thought that that was so spot on. I thought that was an excellent, really an excellent point, making a very important philosophical point in this one little line in the middle of a psychological thriller, which is pretty cool. So I call that the Garland test. I found it very, like, yeah, that was really astute. I was wondering, like, which texts influence him most when he was writing it. Um, and in particular, like where you found that your, your work had seeds like planted throughout the movie. Yeah. Um, where do you think it was most influential? Well, it's a good question. You know, you, you, you need to ask him. Yeah, maybe. Um certainly my book is very heavily influenced by Wittgenstein. And in a sense, Wittgenstein is all about when it comes to these deep philosophical questions. In a sense, it's very down to earth. It's always saying, well, what do we mean by consciousness and intentionality and all these kind of big, difficult words? And Wittgenstein is always taking a step back and saying, well, what are the role of these words in ordinary life? And the role of these words in ordinary life with something like consciousness is all to do with the actual behavior of the people we see in front of us. And so, you know, in a sense, I judge others. Well, I don't actually go around judging others as conscious. That's a point that he would make as well. It's just I just naturally treat them as conscious. And so why do I naturally treat them as conscious? Because their behavior is such that they're just like fellow creatures. And that's just what you do when you encounter a fellow creature. You don't think carefully about it. And so this is an important Wittgensteinian point that i bring out in the in the book very much and um and in a sense that's very much what happens to caleb so caleb doesn't you know isn't sort of sitting making notes saying therefore she is conscious yeah but rather through interacting with her he just gradually comes to feel that she is conscious just to and to start treating her as as conscious and so so that's a very there's something very wittgensteinian about that and then i think probably that comes from i'd like to think that comes from my book to an extent well i had never um i guess it seems very cinematic that it would be like over the course of a week the the turing test but i had never seen a Turing test framed that way. Yeah. I mean, I guess it's not, you know, it's a Garland test. But did you coach him in any way of, like, the natural steps that someone would take as the test elevates? No, not at all. No, this is all Alex Garland's stuff. I had no input on that side at all. So the script was already, and the plot was already, the whole script was already, you know, 95% done, you know, when I first saw it. So there are a few differences in the final film from what you see in the script that I saw and indeed in the publish. So that was actually, that was a question from Twitter. This is kind of a seemingly pseudonym on Twitter. Someone trench shovel. They asked, were there any parts of the script that were changed or left out because they weren't technically feasible or realistic? Ah, well, well, so there was a bit that was in the script that was that was left out in the final filming, which I think is very significant. Okay. And so spoilers ahead for the few people. I assume if you're listening to this, you've seen the film. So right at the end of the film, where Ava is climbing into the helicopter, having escaped from the compound, and she's about to fly off. And we see her have a few words with a helicopter pilot and uh you know i wonder what she says actually yeah you know that's interesting that uh just fly me away from here um anyway and then off the off the the off the helicopter goes now in the written script there's a there's an instruction uh there which says something along the lines, oh, we see, you know, we see waveforms and we see the facial recognition vectors fluttering across the screen. And we see this, that and the other. And it's utterly alien. This is how Ava sees the world. It's utterly alien. Ava sees the world. It's utterly alien. And now in the end, so the very first version of the film that I saw was long before all the VFX had been properly done and everything. So it was a first crude cut. And they put sort of this scene and they put a little bit of this kind of visual effects in. And then I think they decided this didn't really work terribly well to to do that at that point so they kind of cut it out so in the version that we see you don't actually see that that you just see her speaking to the helicopter pilot and she climbs into the helicopter but it's very it's a very significant direction because you know we're left uh i mean i think one of the great strengths of the film is that it leaves so many unanswered questions you know you don't really you're left thinking is she really conscious you know yeah is you know does she really is she really capable of suffering is is is she just a kind of machine that's gone horribly wrong or is she a a person who's who's understandably had to had to do commit this act of violence in order to save herself. Which of these is it? And you never really quite know. Although I think people are leaning more towards the kind of, oh, she's conscious in a straightforward kind of way. But that version of the ending just points to the fact that there's a real ambiguity there. Because if that had been shown, you might be leaning more the other way. You might be thinking, gosh, this is a very alien creature indeed. And she still might be really genuinely conscious and genuinely capable of suffering. But it would really throw open the kind of question, how alien is she? It would really throw open the kind of question, you know, how alien is she? To me, that would also – so just so I understand, it was a VFX over the actual image, right? Well, I mean, in the script, it doesn't specify exactly how it would be done. So it just says something like, we see, you know, facial recognition vectors fluttering. And I can't remember the exact words but but the it was um um you know the obviously the idea was to give an impression of what things look like and sounded like for ava in some sense which of course is very in a sense is impossible to convey but you just have to would we would i think maybe that's why they've they they thought how do we how would we do this and well i didn't know if they were also trying to avoid some kind of i guess it's not really like a fourth wall but it's also trying to um trying to avoid the situation where they the author or uh the auteur of the alex right of the movie is saying like we're in a simulation like this is what you're seeing as you are the mind of some an artificial intelligence yes well i think it was meant to be shown from her point of view so so right um so that wouldn't have been an interpretation of it if they got it right i would imagine so maybe but i know i don't know why exactly they decided not to put it in but but but it's just the fact that that that direction is there in this script and by the way that's in the published script so i'm not giving any i'm not giving anything away away here. But there is the published version of the script has this little direction in it. Yeah. I rewatched it. Yeah. I rewatched it last night and I remembered the ending. I was like, it's so vague. Yeah. So vague what happens. And yeah, I do. I do remember. Cause I, I quite liked You know, where you just, you don't really know, really. You know, is she conscious at all? Is she conscious just like we are? Is she conscious in some kind of weird alien way? You never really know. And this is a deep philosophical question. And there's also, there's a moment where, right at the end, where she's coming down the stairs, having escaped, basically. She's coming down the stairs at the top floor of Nathan's compound. And she smiles. She does that. She goes up the stairs, kind of looks back and surveys. Yes. And she smiles. And she smiles. And I remember saying to Alex, I said, I don't think you should have that. After I'd seen the first version, I said, I don't think you should have that smile there because it's too human. And he really thought it was important to have the smile there because I think he would say, so I think Alex would say, I don't want to put words in his mouth, so I apologize to Alex if he's listening to this. I think he would say that people, of course, can have their own interpretations. And that's, of course, that's, you know, but he would probably lean towards the interpretation that she is conscious in the way that we are. And the evidence for that is, well, why would anybody smile to themselves privately if they weren't conscious just like we are? And what else in those conversations, you know, you're watching edits of the movie. What else did you guys work through? Well, so there's the Easter eggter egg yeah that's a good one yeah we should tell the story of the easter egg yeah so um so the first um time i saw any uh you know any kind of clip of ex machina as alex sent me an email and um and said well do you want to come in and see a see a bit ofina? You know, it's in the can, as these film people say, though there are no cans anymore for the film to go in. So come and see a kind of like, you know, come to the cutting room. So I went along and he showed me some scenes. And at one point he kind of stopped the uh stopped the machine and um and he said and this is the moment where uh caleb is reprogramming the security system in order to release all the locks to try and get out and uh and so alex froze the frame there and said oh right now you see these computer screens where caleb is into these computer screens. And he said, you see this window here? Now this window is all full of kind of some junk code at the moment. And he says, but you can be sure there are going to be some geeky types out there who the moment this thing comes out on a DVD, they're going to freeze that frame and they're going to say, what does this do? So let's give them an Easter egg. Let's give them a little surprise. So he said, basically, that window is yours. Put something in there and some kind of hidden message. And he said, maybe make it an allusion to your book. So I thought, this is very cool. This is the best product placement ever, ever. I've probably sold one other copy thanks um uh so i went home that that evening yeah and i made the mistake that evening of buying a bottle of sake and i and i was drinking this sake i said what am i going to do for this and i got got down coding some coding something up in in python and i was having a good laugh at what i was going to do so i i thought okay it's got to be vaguely kind of to do with security and encryption. So let's have something that kind of has some primes in it. So I wrote this little piece of the Suvavera Tosthenes, a classic way of computing primes. I wrote this. So instead of kind of getting off Wikipedia or something, I sat there and coded it myself after four glasses of Sarké. And I was coding this thing up. And then basically it computes a big array of prime numbers. And then there's this thing that indexes into the prime numbers and then adds some random looking other numbers to the numbers that it's. And then those are ASCII characters. And then it prints out what those ASCII characters actually look like. So when you look at this code on the screen, it's just gobbledygook, but something to do with prime numbers. If you run it, it prints out ISBN equals and the ISBN of my book, Embodiment of the Inner Life. Anyway, so I was very pleased with this, and I handed it over to them, and they put it in the thing. But I have to say, Alex was wrong. It wasn't when the DVD came out that, oh no, it only had to be on BitTorrent for 24 hours, long before the DVD came out, before there was pages about this thing on the internet. There was a whole Reddit thread. There's a GitHub repository with my piece of code. And the Reddit thread includes a whole lot of criticism about my coding style. It's not PEP8 compliant. It's really funny. And it's true. It's really true. But what I really regret was that the loop, I put the wrong terminating condition on the loop. You can terminate the sieve of eratosthenes after n squared. You don't have to go all the way to n over 2. But for some reason, I wasn't paying attention. Four glasses of sake, and it terminates after n over 2. It's inefficient. We'll let it slide. Maybe that's the bug in her code. Well, it's not a bug. It's just, give me some credit. I mean, it's not actually a bug. It's just, give me some credit. You know, I mean, it's not actually a bug. I mean, it does meet the specification, but it's not efficient. Fair enough. We should ask some of these questions from Twitter. I know people are very excited to ask you questions. So we already asked one. So Patrick Atwater, let's get to his question. This is, okay, so this is, we should, so Craig, ask how much closer we are to the sort of general Hollywood style AI now than we were in the 50s. In the 50s? So I think what he's alluding to is the flying car, you know, pastel version. Like it's kind of the crazy futuristic version of the AI in the 50s. Yeah, yeah. And then the AI that they're portraying in the movie. Well, I can tell you that we're precisely 60 years closer than we are in the 50s. But I don't think that's the kind of answer that... We can tweet them, though. So, well, of course, you have to remember that in Ex Machina, as in all films, the way that AI is portrayed, really a lot of it is to do with making a good film and making a good story. And, you know, really a lot of it is to do with making a good film and making a good story. And I mean, in particular, people love stories where, you know, where the AI is some kind of enemy nemesis and so on. Actually, Garry Kasparov, who we just heard speak, made a very interesting point, didn't he, about this? He said that there's been a kind of, he pointed out, and I think he's right, that there's been a kind of change from very positive views in science fiction of utopian views where we're going to kind of get to the stars and to more dystopian views of things where it's like the Terminator and so on. But anyway, it certainly makes for a good story if your AI is bad. And it also makes for a good story if your AI is embodied and if your AI is very human-like. Whereas in reality, AI insofar as it's going to get more and more sophisticated and closer and closer to human level intelligence, it's not necessarily going to be human-like. So it's not necessarily going to be human-like. So it's not necessarily going to be embodied in robotic form. Or if it is embodied in robotic form, it might not be in humanoid form. So in a sense, a self-driving car is a kind of robot. So I think that things will be a bit different from the way Hollywood has portrayed them. Of course, if you go back to the 50s, and it's very interesting to look at retro science fiction. I love retro science fiction. You look at something like The Forbidden Planet, then Robbie, of course, in The Forbidden Planet, is this metal hunk thing, you know, which is completely impractical. And you're thinking, how would it get around at all? And how would it do anything with these kind of claw arms and hands that it's got? So clearly we've changed a lot in our view of what we think we can, the kinds of bodies we think that we might be able to make in the future. And I think it's also quite difficult because there's not really a clear benchmarking happening right now. Because it's not obvious. If it was just like, you know, energy and compute going into this, then the race would be, I mean, it wouldn't be over, but it would be very obvious as who's winning and what's going on. It seems to be that there are clear breakthroughs that have to happen. Yeah, that's certainly my view. So if we're thinking about now the question of when might we get to human level AI or artificial general intelligence, then I think we really don't know. And certainly some people can, you know, you can draw graphs that extrapolate computing power and the sort of how fast the world's fastest supercomputers are. supercomputers are and and and you know we're we're pretty close to well depending on how you calculate it we're pretty close to human brain scale computing already in the world's fastest supercomputers and we will get there within the next couple of years um but that doesn't mean to say we know how to build human level intelligence that's an altogether different thing and also there's controversy about how you make that calculation as well i mean do you do you, you know, do you, how, what do you count, do you count a neuron? How do you count the computational power of one neuron or one synapse? And some people, you know, it may be that some of the immense complexity in the synapse is functionally irrelevant. It's just, you know, it's chemically important and so on, but it might be functionally irrelevant to cognition. So we really, there's a lot of open questions there. But even if we allow a kind of conservative estimate, and we assume that we're going to have enough computing power that's equivalent, the computing power that's equivalent to that of the human brain by, say, 2022 or something, or 2020, yeah, we still would need to understand exactly how to use all of that computing power to realize intelligence and i think there are probably an unknown number of conceptual breakthroughs between here and there yeah i mean specific ai absolutely happening but this general yeah i think he's talking about yeah yeah exactly so um uh so so yeah so clearly there's lots of specialist artificial intelligence where where we're getting really good at things like image recognition and image understanding is and and speech so speech recognition is more or less being cracked um the act they're just the process of turning the way raw waveform into uh text um into so that that's been cracked but then again, real understanding of the words, that's a whole other story. And while today's personal assistants can be quite cool and they're going to get better and better, there's still a way of displaying any genuine understanding of the words that are being used. I think that will happen in due course, but we're not quite there yet yeah i mean fortunately or unfortunately um because that also that underlies one of the other questions that i that i did want to ask so this is from uh i think mecca and mecca floss on twitter um so their question is excellent movie but why is asthma as moff's law forgotten um that would be the absolute first thing they asked. So just for people who don't know what that is, there are three laws of robotics, right? So I wrote these down. So a robot may not injure a human being or through inaction, allow a human being to come to harm. That's the first one. Two, a robot must obey orders given to it by human beings, except where such orders would conflict with the first law. And then the third law is a robot must protect its own existence as long as such protection does not conflict with the first or second law. And so their point is basically like, you know, why is the first law broken in Ex Machina? Yeah. Well, of course, Asimov's laws are themselves the product of science fiction. They're not real laws. We should make that clear. So Asimov wrote those laws down in order to make for great science fiction stories. And all of the science fiction stories, Asimov's stories, are, you know, center on the ambiguities and the difficulties of interpreting those laws or realizing them in actual machines. And often the sort of moral dilemmas, as it were, that the robot is faced with in trying to uphold those laws. So even if we did suppose that we wanted to somehow put something like those laws into a... I mean, it's not relevant to robotics today. But if we do want to put them into a robot, it would be immensely difficult. So I should take a step back and say, why is it irrelevant to robotics today? So, of course, let me qualify that. Of course, there are people who want to build autonomous weapons and all kinds of things like that. who want to build autonomous weapons and all kinds of things like that. And you might say to yourself, well, I would very much like it if somebody was trying to pay attention to things a bit like Asimov's laws and say, well, you know, you shouldn't build a robot that is capable of killing people. But that's a law that the designers, or that would be a principle if we were to have it, that the designers, or that would be a principle if we were to have it, that the designers and engineers would be exercising, not one that the robot itself was exercising. So that's the sense in which it's not relevant today, because we don't know how to today make an AI that is capable of even comprehending those laws. So that's kind of the first point. But so that's kind of the first point. So why doesn't it? But OK, but when we're thinking about the future, of course, this is in Ex Machina. So why not? Well, it would obviously, again, make a very different story if Asimov's laws were put into Ava. But let's suppose that it was a world where we were minded to put Asimov's laws into Ava. But let's suppose that it was a world where we were minded to put Asimov's laws into Ava. Well, maybe Ava might reason that she is human. What is the difference between herself and a human? And maybe she would reason that she shouldn't allow herself to come to harm, and therefore she was justified in what she was doing. Who knows? I mean, it's just a story, right? I think we have to remember that it's just a story and, and, and it's actually very important. I, I think science fiction is really, really good at making us think about the issues, but at the same time, we always have to remember that it's just stories, that there's a difference between fantasy and reality. And I think it's also, it is also kind of covered in the movie when Nathan and Caleb are debating. I think Nathan's criticizing Caleb over going with his gut reaction and his ego. And if he were to think through every logical possibility for every action, he would never do anything. Right, right. Which is kind of directly against against all these laws you know like ava would never do anything if she could harm someone possibly down the road by you know uh burning fossil fuel by being in the helicopter well indeed yeah yeah yeah i mean i guess we're all um we're all we all have to confront those sorts of dilemmas all the time. And indeed, you know, moral philosophers have got plenty of examples of these kinds of dilemmas that make it obvious that there's no simple, single rule really, you know, is enough by itself. Trolley problems, if you know about them, where, you know, the trolley is heading down the track and there are points and for some unknown reason somebody is tied across the tracks on one fork. It's very cinematic. It's very cinematic. And on the other fork, three people are tied across the tracks. And the points are currently such that the trolley is going to go over the three people and kill them. And you are faced with the possibility of changing the points so that the trolley goes down the first track and kills only one person. So what do you do? And philosophers can spend entire conferences debating what the answer to this is and thinking of variations and so on. And that little problem problem that little thought experiment of philip philip of foot's thought experiment there is a is a distillation of of you know much more complex moral dilemmas that exist in the real world absolutely so so before we go um i do want to talk about your your thoughts on broader things you you know obviously you work here we haven't been broad enough yeah no i've got a lot of things that then x my gonna um so obviously you're here deep mind you're at imperial as well 20 of the time yeah um can can you talk a little bit about uh things you're excited about for the future as far as it relates to what you're working on yeah um well so i've so i've recently got very interested in uh in deep reinforcement learning so deep reinforcement learning is is one of those things that deep mind has made famous really so when they published uh this paper back in 2014 and then the nature version in 2015 um about so they published this paper about um a system that could learn to play these retro Atari games completely from scratch. So all the system sees is just the screen, just the pixels on the screen. It's got no idea what objects are present in the game or anything. It just sees raw pixels and it sees the score. And it has to learn by trial and error how to get a good score. by trial and error, how to get a good score. And they managed to produce this system, which is capable of learning a huge number of these Atari games completely from scratch and getting, in some cases, superhuman-level performance, in other cases, human-level performance. And in some cases, it wasn't too good at the games. And so I think that opened up a whole new field. And to my mind, so that system was called DQN. To my mind, DQN is in a sense, one of the very first general intelligences because it learns completely from scratch. You can throw a whole variety of problems at it and it doesn't always do that well. In many cases it does it does pretty well yeah um so so to answer your question so i've got very interested in this field of deep reinforcement learning but when i sort of first uh you know long before i joined deep mind i first started playing with their dqn system when they made the source code public and i pretty quickly realized that it's got quite a lot of shortcomings as well, as today's deep reinforcement learning systems all have, which is it's very, very slow at learning for a start. And when you watch it learning, you think, actually, this thing is really stupid because it might get to superhuman performance eventually. But my goodness, it takes a long time to do it. Even just playing Space Invaders. Yeah, or even Pong or something even simpler than that. So it takes an awful long time to do it, whereas a human very quickly is able to work out some general principles. What are the objects? What are the sort of rules? And you work it out very quickly. And so it made me think about my ancient past in classical artificial intelligence, symbolic AI. And it made me realize that there were various ideas from symbolic AI that could be rehabilitated and put into deep reinforcement learning systems in a more modern guise. And so that's the kind of thing that I'm most interested in right now. Very cool. Yeah, that was actually one of my favorite questions from the kasparov talk today someone who was working on go asked exactly that yeah like how can humans compute so quickly all like they compute what is not relevant to the game and they can just they execute the game uh i guess it was chess right in in 50 moves rather than 100 moves yeah Yeah, yeah. And it's very much that framing. Yeah, yeah. That was Torre. Torre, who's one of the people on the AlphaGo team. And yeah, that's a very deep question, I think, that he was asking there. Yeah. Yeah, it's fascinating. Cool. So if someone wants to learn more about you or more about the field in general, what would you recommend? If they want to learn more about me, I can't think why they would want to. But if they did, they can Google my name and find my website. If they want to learn more about the field in general, well, we're in a very fortunate position of having an awful lot of material out there on the internet these days that people can find and all kinds of lectures and TED Talks and TEDx Talks and so on. If people want to know a bit more technical detail there are some excellent tutorial tutorials and uh about deep learning and so on out there um that people can find um there are lots of uh massive moocs you know massive online open courses um so there's a huge amount of material on the internet out there. Do you have a budding career in technical advising or is there Ex Machina 2? So people often ask me about Ex Machina 2, which of course is none of my business. But whenever I've heard Alex Garland asked about that, he always says he's got no intention of producing an Ex Machina 2, that it was a one-off. As for scientific advising yeah so the so i have been involved in a few other kind of projects like there's a there was a theater project i was involved in that i uh i enjoyed with nick pain um at the donmar warehouse here what's that called uh so that was a so this play by nick pain was called Elegy, and it's about an elderly couple where one of them has got a dementia-like disease. And it's set also sort of 10 minutes into the future. One of them has got a dementia-like disease, but techniques have been developed whereby these diseases can be cured, but the cost that you have to pay is that you lose a lot of your memories. And so the play really centers on the difficulties for the partner, knowing that her partner's memories of their first meetings and so on and their love is going to actually sort of vanish. So it was about that, and it was more of a neuroscience kind of stuff. But I've also been involved with an artistic collective called Random International. And Random International do some amazingly cool stuff, so I highly recommend Googling them. And so they were famous for this thing called Rain Room. Okay, sure. At MoMA in New York. Yes, that's right. Exactly, yeah. So it's toured, but it was in MoMA. It was in MoMA in New York indeed, yeah. So the idea there is that as you – so all of their art is using technology in various kind of interesting ways and often about how we interact with technology to make kind of art. So in Rain Room, the idea is it's a room with sprinklers on the ceiling and you walk around in this room and it's raining everywhere. But there's some clever technology that senses where you are. And you worked on that? No. I should finish. So there's some clever technology that senses where you are. And you worked on that? No, I didn't. I should finish. So there's some clever technology that senses where you are and turns off the sprinklers immediately above your head. So you walk around in this room miraculously never getting wet. So that's one of the things. So they also worked on this amazing sculpture called 15 Points. And this is based on point light displays. So a point light display is one of these little displays where on the screen, you've just got, say, 15 dots. And these 15 dots move around, and suddenly you see that it's a person because the 15 dots are like the elbow joints and the neck and the head and the torso and the knees and so on. And you see these things moving around and you instantly interpret it as motion. In fact, you can even tell whether the person is running or walking or digging or often whether it's a man or a woman just by these 15 points moving. So they constructed this beautiful sculpture which um has these sort of rods that have little lights on the end rods and motors and so it's very much a piece of mechanics mechanical robotic like mechanical thing and uh and when you just see it stationary it's just like this weird kind of contraption but then it starts moving and all the lights on the end and and then you suddenly you see there's this person appears walking towards you and and i thought that was a wonderful um a wonderful example of how we we see uh you know we we we we see someone there when there isn't and of course that for me that was very interesting because it it it made me think about when we do that with machines. Often we think that there's someone at home when there isn't. So a lot of their art is all about those kinds of questions. That's so cool. I think that's a perfect place to end it. And we'll link up to all their work as well. Okay, great. All right, thanks, Mark. Yeah, sure. Thank you.